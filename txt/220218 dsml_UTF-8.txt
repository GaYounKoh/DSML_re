크롬 열려있는 탭 한 창으로 합치는 방법
https://comterman.tistory.com/2191

<lab, pro, pre로 나누는 이유>
lab이 좀 더 사후적인 처치이기 때문

<np.std로 나누는 건 표준화의 일환>

<Markdown 공백>
&nbsp;
또는
&#160;
또는
　　　　　 복사해서 사용 (2byte(전각) 공백)
https://lynmp.com/en/article/cx811c9dc50y


<LSH 11 PRE, PRO, LAB ITEMID 찾기>





<1번째 comment>
희소한 feat을 뽑게 되면 상대적으로 0인 애들이 많아짐.
RF가 뽑는게 맞는 방법 (보편적인 애들 뽑아서)
** 희소하다: 비율 (그래프의 y축)이 낮다. (비율==비중)

RF로 뽑힌 FI top10은 Decision Tree 가지치기 했을 때 위에있는 feature.


RF가 더 우수한 이유: 1의 비율 높은 애들을 뽑았기 때문




<1의 비중이 높을 수 있도록 FI_our를 수정>

- 가중치(1의 개수/전체) 곱하는 방법? (LSH)
- 1과 0이 골고루여야 더 좋지 않을까? ==> 계산해 주는게 entropy (Prof.)

	entropy input : 확률 (feat마다 0, 1개수 조사해서 비율? 집어넣는 것)


	치우치면
	최소 0 (0에 가까울 수록)

	둘이 확률 똑같으면 (균등하면)
	1 나옴.



sign 뒤에 entropy 곱해주면
균등한 애들이 가중치 높아짐



inverse의 문제점
(균등하지 않을 때는) 다 1로 바꾸는게 inverse랑 비슷




<<<<문제점: 0값 너무 많이 나와서 판단력 떨어짐. 따라서 RF가 하듯이 0과 1 귵등한 애들 가지고 FI 다시 구할 필요가 있다.>>>>>

-----------------------------------------------------------------------------------------------------------------------------


<scaled에 대한 comment> - 역전(Method2)부분
좁아지다가 넓어지는 식으로 줄긴했다??????????????????
hmm....


Method2의 식을 -가 아닌 *로 한다면...

부호의 차이
음수일 때 완전 하위애들만 보면...
방향은 모르지만 일단 역전이 됐음을 알 수 있음.
/ sigma가 좋은지는 모르겠음.

--> 오,,,,,,


all_fit과 train test 나눠서한 것과 비교가 되려나? 현재로서는 무의미 (1이 희소하다는 문제가 너무 크기 때문)




<<수치만 볼게 아니라 시각화를 해봐야함.>>


<<<<<수치적으로 RF만 이기면 됨>>>>> --> RF에서 뽑힌 FI의 정확도만 이기면 된다는 뜻.






앙상블 --> FI (정확도 높이기) : 제일 어려운 작업................









gpu close()처럼 쓰고 바로 꺼지는거

제한 안걸면 바로 다 잡아먹음.
default가 gpu로 바뀐듯.
import 이후부터 다 잡아먹음.












{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance\n",
    "### 1) data load 및 train ~ test로 split\n",
    "### 2) Model\n",
    "### 3) (Feature 별로 reverse해서) 예측해서 loss값 구하기. -- 괄호는 이번 프로젝트를 위한 과정.\n",
    "### 4) Display LSTM Feature Importance\n",
    "### 5) Visualization (Graph)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings(action = 'ignore')\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((560, 10, 4068), (560,), (140, 10, 4068), (140,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) data load 및 split\n",
    "import random\n",
    "\n",
    "# data seed random 설정\n",
    "seed_num = 42\n",
    "random.seed(seed_num)\n",
    "\n",
    "x = np.load('C:/Users/kelly/Downloads/DSML_data/x_(7727,10,4068).npy')\n",
    "y = np.load('C:/Users/kelly/Downloads/DSML_data/y_(7727,1).npy')\n",
    "\n",
    "# 랜덤으로 700개 추출\n",
    "# random seed에 따른 shuffle\n",
    "\n",
    "idx = list(range(len(x)))\n",
    "random.shuffle(idx)\n",
    "idx = idx[:700]\n",
    "\n",
    "i = round(len(idx)*0.8)\n",
    "\n",
    "X_train, y_train = x[idx[:i],:,:], y[idx[:i]]\n",
    "X_test, y_test = x[idx[i:],:,:], y[idx[i:]]\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "4/4 [==============================] - 88s 2s/step - loss: 0.6831 - acc: 0.5927 - val_loss: 0.6637 - val_acc: 0.6214\n",
      "Epoch 2/100\n",
      "4/4 [==============================] - 1s 325ms/step - loss: 0.6677 - acc: 0.5963 - val_loss: 0.6681 - val_acc: 0.6214\n",
      "Epoch 3/100\n",
      "4/4 [==============================] - 1s 291ms/step - loss: 0.6931 - acc: 0.5565 - val_loss: 0.6706 - val_acc: 0.6214\n",
      "Epoch 4/100\n",
      "4/4 [==============================] - 1s 328ms/step - loss: 0.6910 - acc: 0.5236 - val_loss: 0.6683 - val_acc: 0.6214\n",
      "Epoch 5/100\n",
      "4/4 [==============================] - 2s 411ms/step - loss: 0.6840 - acc: 0.5601 - val_loss: 0.6655 - val_acc: 0.6214\n",
      "Epoch 6/100\n",
      "4/4 [==============================] - 2s 436ms/step - loss: 0.6887 - acc: 0.5877 - val_loss: 0.6636 - val_acc: 0.6214\n",
      "Epoch 7/100\n",
      "4/4 [==============================] - 2s 442ms/step - loss: 0.6815 - acc: 0.5900 - val_loss: 0.6632 - val_acc: 0.6214\n",
      "Epoch 8/100\n",
      "4/4 [==============================] - 1s 342ms/step - loss: 0.6814 - acc: 0.5858 - val_loss: 0.6637 - val_acc: 0.6214\n",
      "Epoch 9/100\n",
      "4/4 [==============================] - 1s 317ms/step - loss: 0.6874 - acc: 0.5712 - val_loss: 0.6650 - val_acc: 0.6214\n",
      "Epoch 10/100\n",
      "4/4 [==============================] - 2s 418ms/step - loss: 0.6839 - acc: 0.5862 - val_loss: 0.6647 - val_acc: 0.6214\n",
      "Epoch 11/100\n",
      "4/4 [==============================] - 1s 347ms/step - loss: 0.6762 - acc: 0.5848 - val_loss: 0.6639 - val_acc: 0.6214\n",
      "Epoch 12/100\n",
      "4/4 [==============================] - 1s 349ms/step - loss: 0.6688 - acc: 0.5977 - val_loss: 0.6637 - val_acc: 0.6214\n",
      "Epoch 13/100\n",
      "4/4 [==============================] - 1s 318ms/step - loss: 0.6772 - acc: 0.5933 - val_loss: 0.6634 - val_acc: 0.6214\n",
      "Epoch 14/100\n",
      "4/4 [==============================] - 1s 325ms/step - loss: 0.6697 - acc: 0.6177 - val_loss: 0.6634 - val_acc: 0.6214\n",
      "Epoch 15/100\n",
      "4/4 [==============================] - 1s 355ms/step - loss: 0.6688 - acc: 0.6108 - val_loss: 0.6630 - val_acc: 0.6214\n",
      "Epoch 16/100\n",
      "4/4 [==============================] - 1s 363ms/step - loss: 0.6851 - acc: 0.5591 - val_loss: 0.6642 - val_acc: 0.6214\n",
      "Epoch 17/100\n",
      "4/4 [==============================] - 1s 385ms/step - loss: 0.6739 - acc: 0.5765 - val_loss: 0.6598 - val_acc: 0.6214\n",
      "Epoch 18/100\n",
      "4/4 [==============================] - 1s 308ms/step - loss: 0.6701 - acc: 0.5781 - val_loss: 0.6524 - val_acc: 0.6214\n",
      "Epoch 19/100\n",
      "4/4 [==============================] - 2s 414ms/step - loss: 0.6585 - acc: 0.5792 - val_loss: 0.6419 - val_acc: 0.6214\n",
      "Epoch 20/100\n",
      "4/4 [==============================] - 1s 367ms/step - loss: 0.6361 - acc: 0.6024 - val_loss: 0.6295 - val_acc: 0.6214\n",
      "Epoch 21/100\n",
      "4/4 [==============================] - 1s 372ms/step - loss: 0.6062 - acc: 0.6374 - val_loss: 0.6175 - val_acc: 0.6214\n",
      "Epoch 22/100\n",
      "4/4 [==============================] - 1s 369ms/step - loss: 0.5731 - acc: 0.6763 - val_loss: 0.6013 - val_acc: 0.6929\n",
      "Epoch 23/100\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5137 - acc: 0.780 - 1s 321ms/step - loss: 0.5143 - acc: 0.7785 - val_loss: 0.5919 - val_acc: 0.7214\n",
      "Epoch 24/100\n",
      "4/4 [==============================] - 1s 328ms/step - loss: 0.4354 - acc: 0.8795 - val_loss: 0.5947 - val_acc: 0.6857\n",
      "Epoch 25/100\n",
      "4/4 [==============================] - 2s 412ms/step - loss: 0.3609 - acc: 0.9057 - val_loss: 0.6223 - val_acc: 0.6786\n",
      "Epoch 26/100\n",
      "4/4 [==============================] - 2s 425ms/step - loss: 0.3077 - acc: 0.9129 - val_loss: 0.6728 - val_acc: 0.6571\n",
      "Epoch 27/100\n",
      "4/4 [==============================] - 2s 395ms/step - loss: 0.2390 - acc: 0.9300 - val_loss: 0.6846 - val_acc: 0.7143\n",
      "Epoch 28/100\n",
      "4/4 [==============================] - 1s 368ms/step - loss: 0.1886 - acc: 0.9490 - val_loss: 0.7615 - val_acc: 0.6500\n",
      "Epoch 29/100\n",
      "4/4 [==============================] - 2s 396ms/step - loss: 0.1557 - acc: 0.9643 - val_loss: 0.7424 - val_acc: 0.7143\n",
      "Epoch 30/100\n",
      "4/4 [==============================] - 2s 403ms/step - loss: 0.1376 - acc: 0.9692 - val_loss: 0.7949 - val_acc: 0.6714\n",
      "Epoch 31/100\n",
      "4/4 [==============================] - 2s 450ms/step - loss: 0.1229 - acc: 0.9704 - val_loss: 0.8066 - val_acc: 0.6571\n",
      "Epoch 32/100\n",
      "4/4 [==============================] - 2s 469ms/step - loss: 0.1028 - acc: 0.9705 - val_loss: 0.9100 - val_acc: 0.6429\n",
      "Epoch 33/100\n",
      "4/4 [==============================] - 1s 345ms/step - loss: 0.0861 - acc: 0.9840 - val_loss: 0.8669 - val_acc: 0.6714\n",
      "Epoch 34/100\n",
      "4/4 [==============================] - 2s 366ms/step - loss: 0.0747 - acc: 0.9825 - val_loss: 0.9069 - val_acc: 0.6643\n",
      "Epoch 35/100\n",
      "4/4 [==============================] - 1s 361ms/step - loss: 0.0584 - acc: 0.9900 - val_loss: 0.9292 - val_acc: 0.6857\n",
      "Epoch 36/100\n",
      "4/4 [==============================] - 1s 331ms/step - loss: 0.0560 - acc: 0.9870 - val_loss: 0.9659 - val_acc: 0.6571\n",
      "Epoch 37/100\n",
      "4/4 [==============================] - 1s 293ms/step - loss: 0.0611 - acc: 0.9902 - val_loss: 0.9919 - val_acc: 0.6786\n",
      "Epoch 38/100\n",
      "4/4 [==============================] - 1s 316ms/step - loss: 0.0654 - acc: 0.9848 - val_loss: 1.0179 - val_acc: 0.6786\n",
      "Epoch 39/100\n",
      "4/4 [==============================] - 1s 345ms/step - loss: 0.0595 - acc: 0.9885 - val_loss: 1.0429 - val_acc: 0.6500\n",
      "Epoch 40/100\n",
      "4/4 [==============================] - 1s 293ms/step - loss: 0.0511 - acc: 0.9925 - val_loss: 1.0761 - val_acc: 0.6786\n",
      "Epoch 41/100\n",
      "4/4 [==============================] - 1s 287ms/step - loss: 0.0502 - acc: 0.9853 - val_loss: 1.1278 - val_acc: 0.6714\n",
      "Epoch 42/100\n",
      "4/4 [==============================] - 1s 282ms/step - loss: 0.1018 - acc: 0.9702 - val_loss: 1.0718 - val_acc: 0.6786\n",
      "Epoch 43/100\n",
      "4/4 [==============================] - 1s 286ms/step - loss: 0.0672 - acc: 0.9809 - val_loss: 1.0573 - val_acc: 0.6857\n",
      "Epoch 44/100\n",
      "4/4 [==============================] - 1s 378ms/step - loss: 0.0501 - acc: 0.9873 - val_loss: 1.0862 - val_acc: 0.6786\n",
      "Epoch 45/100\n",
      "4/4 [==============================] - 1s 290ms/step - loss: 0.0517 - acc: 0.9881 - val_loss: 1.0442 - val_acc: 0.6714\n",
      "Epoch 46/100\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.0516 - acc: 0.989 - 1s 335ms/step - loss: 0.0531 - acc: 0.9888 - val_loss: 1.0619 - val_acc: 0.6786\n",
      "Epoch 47/100\n",
      "4/4 [==============================] - 1s 336ms/step - loss: 0.0391 - acc: 0.9910 - val_loss: 1.0079 - val_acc: 0.6929\n",
      "Epoch 48/100\n",
      "4/4 [==============================] - 2s 423ms/step - loss: 0.0389 - acc: 0.9947 - val_loss: 1.0274 - val_acc: 0.6643\n",
      "Epoch 49/100\n",
      "4/4 [==============================] - 1s 333ms/step - loss: 0.0307 - acc: 0.9990 - val_loss: 1.0231 - val_acc: 0.6643\n",
      "Epoch 50/100\n",
      "4/4 [==============================] - 1s 367ms/step - loss: 0.0284 - acc: 0.9963 - val_loss: 1.0288 - val_acc: 0.6643\n",
      "Epoch 51/100\n",
      "4/4 [==============================] - 1s 381ms/step - loss: 0.0255 - acc: 0.9985 - val_loss: 1.0456 - val_acc: 0.6786\n",
      "Epoch 52/100\n",
      "4/4 [==============================] - 1s 337ms/step - loss: 0.0233 - acc: 0.9985 - val_loss: 1.0895 - val_acc: 0.6714\n",
      "Epoch 53/100\n",
      "4/4 [==============================] - 1s 315ms/step - loss: 0.0260 - acc: 0.9948 - val_loss: 1.1019 - val_acc: 0.6857\n",
      "Epoch 54/100\n",
      "4/4 [==============================] - 1s 322ms/step - loss: 0.0243 - acc: 0.9962 - val_loss: 1.1086 - val_acc: 0.6786\n",
      "Epoch 55/100\n",
      "4/4 [==============================] - 1s 361ms/step - loss: 0.0202 - acc: 1.0000 - val_loss: 1.1375 - val_acc: 0.6714\n",
      "Epoch 56/100\n",
      "4/4 [==============================] - 2s 412ms/step - loss: 0.0236 - acc: 0.9939 - val_loss: 1.1350 - val_acc: 0.6643\n",
      "Epoch 57/100\n",
      "4/4 [==============================] - 2s 375ms/step - loss: 0.0234 - acc: 0.9962 - val_loss: 1.1649 - val_acc: 0.6786\n",
      "Epoch 58/100\n",
      "4/4 [==============================] - 1s 366ms/step - loss: 0.0221 - acc: 0.9976 - val_loss: 1.1745 - val_acc: 0.6857\n",
      "Epoch 59/100\n",
      "4/4 [==============================] - 1s 376ms/step - loss: 0.0217 - acc: 0.9985 - val_loss: 1.2080 - val_acc: 0.6857\n",
      "Epoch 60/100\n",
      "4/4 [==============================] - 1s 351ms/step - loss: 0.0508 - acc: 0.9840 - val_loss: 1.2017 - val_acc: 0.6786\n",
      "Epoch 61/100\n",
      "4/4 [==============================] - 2s 395ms/step - loss: 0.0249 - acc: 0.9947 - val_loss: 1.2135 - val_acc: 0.6786\n",
      "Epoch 62/100\n",
      "4/4 [==============================] - 2s 525ms/step - loss: 0.0168 - acc: 0.9977 - val_loss: 1.2001 - val_acc: 0.6643\n",
      "Epoch 63/100\n",
      "4/4 [==============================] - 2s 512ms/step - loss: 0.0199 - acc: 0.9971 - val_loss: 1.2100 - val_acc: 0.6929\n",
      "Epoch 64/100\n",
      "4/4 [==============================] - 2s 569ms/step - loss: 0.0165 - acc: 1.0000 - val_loss: 1.2280 - val_acc: 0.6786\n",
      "Epoch 65/100\n",
      "4/4 [==============================] - 2s 587ms/step - loss: 0.0162 - acc: 0.9985 - val_loss: 1.2262 - val_acc: 0.6929\n",
      "Epoch 66/100\n",
      "4/4 [==============================] - 2s 529ms/step - loss: 0.0162 - acc: 0.9985 - val_loss: 1.2324 - val_acc: 0.6643\n",
      "Epoch 67/100\n",
      "4/4 [==============================] - 2s 520ms/step - loss: 0.0145 - acc: 0.9990 - val_loss: 1.2438 - val_acc: 0.6857\n",
      "Epoch 68/100\n",
      "4/4 [==============================] - 2s 462ms/step - loss: 0.0174 - acc: 1.0000 - val_loss: 1.2534 - val_acc: 0.6643\n",
      "Epoch 69/100\n",
      "4/4 [==============================] - 2s 656ms/step - loss: 0.0144 - acc: 1.0000 - val_loss: 1.2741 - val_acc: 0.6786\n",
      "Epoch 70/100\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 0.0144 - acc: 0.9977 - val_loss: 1.2814 - val_acc: 0.6857\n",
      "Epoch 71/100\n",
      "4/4 [==============================] - 2s 479ms/step - loss: 0.0125 - acc: 1.0000 - val_loss: 1.2871 - val_acc: 0.6643\n",
      "Epoch 72/100\n",
      "4/4 [==============================] - 2s 380ms/step - loss: 0.0163 - acc: 0.9962 - val_loss: 1.3014 - val_acc: 0.6786\n",
      "Epoch 73/100\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0125 - acc: 1.0000 - val_loss: 1.3069 - val_acc: 0.6714\n",
      "Epoch 74/100\n",
      "4/4 [==============================] - 2s 664ms/step - loss: 0.0126 - acc: 1.0000 - val_loss: 1.3252 - val_acc: 0.6786\n",
      "Epoch 75/100\n",
      "4/4 [==============================] - 1s 347ms/step - loss: 0.0112 - acc: 1.0000 - val_loss: 1.3373 - val_acc: 0.6714\n",
      "Epoch 76/100\n",
      "4/4 [==============================] - 1s 339ms/step - loss: 0.0107 - acc: 1.0000 - val_loss: 1.3471 - val_acc: 0.6571\n",
      "Epoch 77/100\n",
      "4/4 [==============================] - 1s 368ms/step - loss: 0.0100 - acc: 1.0000 - val_loss: 1.3542 - val_acc: 0.6643\n",
      "Epoch 78/100\n",
      "4/4 [==============================] - 1s 346ms/step - loss: 0.0108 - acc: 1.0000 - val_loss: 1.3650 - val_acc: 0.6643\n",
      "Epoch 79/100\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 0.0106 - acc: 1.0000 - val_loss: 1.3763 - val_acc: 0.6643\n",
      "Epoch 80/100\n",
      "4/4 [==============================] - 1s 320ms/step - loss: 0.0103 - acc: 1.0000 - val_loss: 1.3843 - val_acc: 0.6643\n",
      "Epoch 81/100\n",
      "4/4 [==============================] - 1s 312ms/step - loss: 0.0088 - acc: 1.0000 - val_loss: 1.3920 - val_acc: 0.6643\n",
      "Epoch 82/100\n",
      "4/4 [==============================] - 1s 289ms/step - loss: 0.0104 - acc: 1.0000 - val_loss: 1.4016 - val_acc: 0.6571\n",
      "Epoch 83/100\n",
      "4/4 [==============================] - 1s 313ms/step - loss: 0.0091 - acc: 1.0000 - val_loss: 1.4087 - val_acc: 0.6643\n",
      "Epoch 84/100\n",
      "4/4 [==============================] - 1s 328ms/step - loss: 0.0102 - acc: 1.0000 - val_loss: 1.4140 - val_acc: 0.6714\n",
      "Epoch 85/100\n",
      "4/4 [==============================] - 1s 331ms/step - loss: 0.0091 - acc: 1.0000 - val_loss: 1.4233 - val_acc: 0.6643\n",
      "Epoch 86/100\n",
      "4/4 [==============================] - 1s 321ms/step - loss: 0.0089 - acc: 1.0000 - val_loss: 1.4275 - val_acc: 0.6714\n",
      "Epoch 87/100\n",
      "4/4 [==============================] - 1s 377ms/step - loss: 0.0085 - acc: 1.0000 - val_loss: 1.4382 - val_acc: 0.6571\n",
      "Epoch 88/100\n",
      "4/4 [==============================] - 1s 327ms/step - loss: 0.0084 - acc: 1.0000 - val_loss: 1.4451 - val_acc: 0.6643\n",
      "Epoch 89/100\n",
      "4/4 [==============================] - 1s 376ms/step - loss: 0.0076 - acc: 1.0000 - val_loss: 1.4501 - val_acc: 0.6643\n",
      "Epoch 90/100\n",
      "4/4 [==============================] - 1s 390ms/step - loss: 0.0115 - acc: 0.9962 - val_loss: 1.4565 - val_acc: 0.6643\n",
      "Epoch 91/100\n",
      "4/4 [==============================] - 1s 367ms/step - loss: 0.0071 - acc: 1.0000 - val_loss: 1.4600 - val_acc: 0.6643\n",
      "Epoch 92/100\n",
      "4/4 [==============================] - 1s 323ms/step - loss: 0.0089 - acc: 1.0000 - val_loss: 1.4631 - val_acc: 0.6714\n",
      "Epoch 93/100\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 0.0075 - acc: 1.0000 - val_loss: 1.4662 - val_acc: 0.6643\n",
      "Epoch 94/100\n",
      "4/4 [==============================] - 1s 318ms/step - loss: 0.0074 - acc: 1.0000 - val_loss: 1.4682 - val_acc: 0.6786\n",
      "Epoch 95/100\n",
      "4/4 [==============================] - 1s 313ms/step - loss: 0.0093 - acc: 1.0000 - val_loss: 1.4750 - val_acc: 0.6714\n",
      "Epoch 96/100\n",
      "4/4 [==============================] - 1s 293ms/step - loss: 0.0078 - acc: 1.0000 - val_loss: 1.4842 - val_acc: 0.6786\n",
      "Epoch 97/100\n",
      "4/4 [==============================] - 1s 390ms/step - loss: 0.0067 - acc: 1.0000 - val_loss: 1.4824 - val_acc: 0.6643\n",
      "Epoch 98/100\n",
      "4/4 [==============================] - 1s 318ms/step - loss: 0.0073 - acc: 1.0000 - val_loss: 1.4935 - val_acc: 0.6571\n",
      "Epoch 99/100\n",
      "4/4 [==============================] - 1s 298ms/step - loss: 0.0067 - acc: 1.0000 - val_loss: 1.4997 - val_acc: 0.6786\n",
      "Epoch 100/100\n",
      "4/4 [==============================] - 1s 336ms/step - loss: 0.0075 - acc: 1.0000 - val_loss: 1.5037 - val_acc: 0.6500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f35d88ea60>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2) Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import *\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# ----------------------\n",
    "seed_num = 42\n",
    "# ----------------------\n",
    "tf.random.set_seed(seed_num)\n",
    "\n",
    "lstm2 = Sequential()\n",
    "lstm2.add(InputLayer(input_shape = (X_train.shape[1], X_train.shape[2])))\n",
    "lstm2.add(LSTM(units=128, activation = 'hard_sigmoid', return_sequences = True))\n",
    "lstm2.add(LSTM(units=64, activation = 'hard_sigmoid', return_sequences = True))\n",
    "lstm2.add(Dropout(0.2))\n",
    "\n",
    "lstm2.add(LSTM(units=64, activation = 'hard_sigmoid', return_sequences = True))\n",
    "lstm2.add(LSTM(units=32, activation = 'hard_sigmoid', return_sequences = False))\n",
    "lstm2.add(Dropout(0.2))\n",
    "\n",
    "lstm2.add(Dense(units=1, activation = 'sigmoid'))\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_acc', patience=30, verbose=1, restore_best_weights = True)\n",
    "# patience : 최고 성능 나온 이후에 학습 얼마나 더 해볼지 (뒤에 또 나올 수 있으니까.)\n",
    "# verbose : 0,1,2 중 선택, 훈련 진행과정 얼마나 자세히 표시할지 선택\n",
    "# restore_best_weights : 학습 중에 가장 좋았던 weights 저장할 지 여부\n",
    "# https://tykimos.github.io/2017/07/09/Early_Stopping/\n",
    "\n",
    "\n",
    "lstm2.compile(optimizer = 'adam', loss = tf.keras.losses.BinaryCrossentropy(), metrics = ['acc'])\n",
    "lstm2.fit(X_train, y_train, validation_split = 0.25, batch_size = 128, epochs = 100)\n",
    "# validation_split : x,y train에서 일정 비율 분리해서 검증으로 사용. validation_data param 대신 사용.\n",
    "# batch_size : 훈련 모델에 넣을 덩어리 크기\n",
    "# epochs : 훈련 횟수\n",
    "# https://roboreport.co.kr/%EB%94%A5%EB%9F%AC%EB%8B%9Dlstm%EC%9C%BC%EB%A1%9C-%EC%95%84%ED%8C%8C%ED%8A%B8-%EC%A7%80%EC%88%98-%EC%98%88%EC%B8%A1%ED%95%98%EA%B8%B0-2-lstm-%EC%8B%A4%ED%97%98%ED%95%98%EA%B8%B0/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 1., 0., ..., 1., 1., 1.],\n",
       "        [0., 1., 1., ..., 1., 1., 1.],\n",
       "        [0., 1., 1., ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [0., 1., 1., ..., 1., 1., 1.],\n",
       "        [0., 1., 1., ..., 1., 1., 1.],\n",
       "        [0., 1., 1., ..., 1., 1., 1.]],\n",
       "\n",
       "       [[1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.]],\n",
       "\n",
       "       [[0., 1., 0., ..., 1., 1., 1.],\n",
       "        [0., 1., 0., ..., 1., 1., 1.],\n",
       "        [0., 1., 0., ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [0., 1., 1., ..., 1., 1., 1.],\n",
       "        [0., 1., 1., ..., 1., 1., 1.],\n",
       "        [0., 1., 1., ..., 1., 1., 1.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0., 1., 0., ..., 1., 1., 1.],\n",
       "        [0., 1., 0., ..., 1., 1., 1.],\n",
       "        [0., 1., 0., ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [0., 1., 1., ..., 1., 1., 1.],\n",
       "        [0., 1., 1., ..., 1., 1., 1.],\n",
       "        [0., 1., 1., ..., 1., 1., 1.]],\n",
       "\n",
       "       [[1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [0., 1., 1., ..., 1., 1., 1.],\n",
       "        [0., 1., 1., ..., 1., 1., 1.],\n",
       "        [0., 1., 1., ..., 1., 1., 1.]],\n",
       "\n",
       "       [[1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(X_test-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4068/4068 [23:33<00:00,  2.88it/s]\n"
     ]
    }
   ],
   "source": [
    "# 3) feature 별로 reverse해서 예측해서 loss값 구하기.\n",
    "# cross_entropy의 방법으로는 binary_cross_entropy사용\n",
    "\n",
    "# feaures (itemid가 피쳐임.)\n",
    "from tqdm import tqdm\n",
    "\n",
    "a = pd.read_csv('C:/Users/kelly/Downloads/DSML_data/total_data_7727.csv')\n",
    "features = list(a['ITEMID'].sort_values().unique()) #4028개의 itemid존재.\n",
    "\n",
    "results = []\n",
    "preds = lstm2.predict(X_test) # baseline 설정하기 위해 먼저 preds\n",
    "\n",
    "\n",
    "# COMPUTE BASELINE\n",
    "# loss 값 구하기\n",
    "from tensorflow.keras.losses import BinaryCrossentropy \n",
    "# Binary Cross Entropy: loss값의 한 종류\n",
    "bce = BinaryCrossentropy()\n",
    "baseline_bce = bce(y_test, preds).numpy() # true, pred를 이용해 loss값 구한게 baseline\n",
    "results.append({'feature':'BASELINE', 'baseline_bce':baseline_bce})\n",
    "\n",
    "for k in tqdm(range(len(features))): # feature별 for문. (feature는 shuffle을 해도 되고, reverse(반대로 하는거)를 해도 됨.)\n",
    "\n",
    "    save_col = X_test[:,:,k].copy() # 초기화\n",
    "\n",
    "    # Reverse All Feature k\n",
    "    ## 방법 1 간단, 코드도 짧...\n",
    "    X_test[:,:,k] = np.abs(X_test[:,:,k]-1)\n",
    "\n",
    "    ## 방법 2 코드 긺..\n",
    "    # X_test[:,:,k] = np.where(X_test[:,:,k] == 1, 2, X_test[:,:,k]) # 1이면 2로 바꾸고 아니면 그대로 냅두라는 의미\n",
    "    # X_test[:,:,k] = np.where(X_test[:,:,k] == 0, 1, X_test[:,:,k])\n",
    "    # X_test[:,:,k] = np.where(X_test[:,:,k] == 2, 1, X_test[:,:,k])\n",
    "\n",
    "    \n",
    "    # Compute BCE with feature k reversed\n",
    "    pred2 = lstm2.predict(X_test) # diff (baseline_bce와 bce간 차이)위해서 각각의 bce 또 구하는 것.\n",
    "    loss_bce = bce(y_test, pred2).numpy() # 넘파이로 안하면 그냥 객체<tf.Tensor: shape=(), dtype=float32, numpy=1.4845252>로 나옴.\n",
    "\n",
    "    results.append({'feature':features[k], 'bce':loss_bce})\n",
    "    X_test[:,:,k] = save_col #리셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>baseline_bce</th>\n",
       "      <th>bce</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2196</th>\n",
       "      <td>409606211</td>\n",
       "      <td>2.353497</td>\n",
       "      <td>2.196926</td>\n",
       "      <td>-0.156570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>54858224</td>\n",
       "      <td>2.353497</td>\n",
       "      <td>2.198983</td>\n",
       "      <td>-0.154514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>51002</td>\n",
       "      <td>2.353497</td>\n",
       "      <td>2.201319</td>\n",
       "      <td>-0.152178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3724</th>\n",
       "      <td>61553015311</td>\n",
       "      <td>2.353497</td>\n",
       "      <td>2.202256</td>\n",
       "      <td>-0.151240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2736</th>\n",
       "      <td>10019016312</td>\n",
       "      <td>2.353497</td>\n",
       "      <td>2.202829</td>\n",
       "      <td>-0.150668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3968</th>\n",
       "      <td>66591018442</td>\n",
       "      <td>2.353497</td>\n",
       "      <td>2.656166</td>\n",
       "      <td>0.302669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50806</td>\n",
       "      <td>2.353497</td>\n",
       "      <td>2.658753</td>\n",
       "      <td>0.305256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>792</th>\n",
       "      <td>29321121</td>\n",
       "      <td>2.353497</td>\n",
       "      <td>2.663199</td>\n",
       "      <td>0.309703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2217</th>\n",
       "      <td>409739172</td>\n",
       "      <td>2.353497</td>\n",
       "      <td>2.675035</td>\n",
       "      <td>0.321538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BASELINE</td>\n",
       "      <td>2.353497</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4069 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          feature  baseline_bce       bce      diff\n",
       "2196    409606211      2.353497  2.196926 -0.156570\n",
       "1010     54858224      2.353497  2.198983 -0.154514\n",
       "119         51002      2.353497  2.201319 -0.152178\n",
       "3724  61553015311      2.353497  2.202256 -0.151240\n",
       "2736  10019016312      2.353497  2.202829 -0.150668\n",
       "...           ...           ...       ...       ...\n",
       "3968  66591018442      2.353497  2.656166  0.302669\n",
       "5           50806      2.353497  2.658753  0.305256\n",
       "792      29321121      2.353497  2.663199  0.309703\n",
       "2217    409739172      2.353497  2.675035  0.321538\n",
       "0        BASELINE      2.353497       NaN       NaN\n",
       "\n",
       "[4069 rows x 4 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4) Display LSTM feature importance\n",
    "df = pd.DataFrame(results)\n",
    "df1 = df.copy()\n",
    "df1['baseline_bce'] = float(df['baseline_bce'].dropna().unique())\n",
    "df1['diff'] = df1['bce'] - df1['baseline_bce']\n",
    "df1 = df1.sort_values(by = 'diff')\n",
    "df1"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e4cfef919f511c0e3a05919ed86dab4fa959f970f2723dae87eeb918be14f5f0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import warnings \n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, GRU, Dropout, LSTM, InputLayer\n",
    "from sklearn.ensemble import VotingClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import metrics \n",
    "from tensorflow import keras\n",
    "import random  \n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "seed_num = 42\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "if gpus:\n",
    "    try:\n",
    "        for i in range(len(gpus)):\n",
    "            tf.config.experimental.set_memory_growth(gpus[i], True)\n",
    "    except RuntimeError as e:\n",
    "        # 프로그램 시작시에 메모리 증가가 설정되어야만 합니다\n",
    "        print(e)\n",
    "\n",
    "seed_num = 42\n",
    "random.seed(seed_num)\n",
    "\n",
    "x = np.load('/project/LSH/x_(7727,10,4068).npy')\n",
    "y = np.load('/project/LSH/y_(7727,1).npy')\n",
    "\n",
    "idx = list(range(len(x)))\n",
    "random.shuffle(idx)\n",
    "\n",
    "i = round(x.shape[0]*0.8)\n",
    "X_train, y_train = x[idx[:i],:,:], y[idx[:i]]\n",
    "X_test, y_test = x[idx[i:],:,:], y[idx[i:]]\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    lstm = Sequential()\n",
    "    lstm.add(InputLayer(input_shape=(x.shape[1],x.shape[2])))\n",
    "    lstm.add(LSTM(units=128, activation='hard_sigmoid', return_sequences=True))\n",
    "    lstm.add(LSTM(units=64, activation='hard_sigmoid', return_sequences=True))\n",
    "    lstm.add(Dropout(0.2))\n",
    "    lstm.add(LSTM(units=64, activation='hard_sigmoid', return_sequences=True))\n",
    "    lstm.add(LSTM(units=32, activation='hard_sigmoid', return_sequences=False))\n",
    "    lstm.add(Dropout(0.2))\n",
    "    lstm.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "    lstm.compile(optimizer= keras.optimizers.Adam(learning_rate = 0.001), \n",
    "                          loss = \"binary_crossentropy\", metrics=['acc'])\n",
    "    return lstm\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "class MyKerasClassifier(KerasClassifier):\n",
    "    def fit(self, x, y, sample_weight=None, **kwargs):\n",
    "        y = np.array(y)\n",
    "        if len(y.shape) == 2 and y.shape[1] > 1:\n",
    "            self.classes_ = np.arange(y.shape[1])\n",
    "        elif (len(y.shape) == 2 and y.shape[1] == 1) or len(y.shape) == 1:\n",
    "            self.classes_ = np.unique(y)\n",
    "            y = np.searchsorted(self.classes_, y)\n",
    "        else:\n",
    "            raise ValueError('Invalid shape for y: ' + str(y.shape))\n",
    "        self.n_classes_ = len(self.classes_)\n",
    "        #---------------수정---------------\n",
    "        if sample_weight is not None:\n",
    "            print('sample weight : ', sample_weight)\n",
    "            if sample_weight[0] == 0.00016175994823681658:\n",
    "                print('x, y', x.shape, x.sum().sum())\n",
    "                return super(MyKerasClassifier, self).fit(x, y, **kwargs)\n",
    "            weights = sample_weight / sum(sample_weight)\n",
    "            random_range = [(sum(weights[:i]), sum(weights[:i])+weights[i]) if i!=0 else (0, weights[i]) for i in range(len(weights))]\n",
    "            random_nums = [random.random() for _ in range(len(weights))]\n",
    "            idx_list = []\n",
    "            for i in random_nums:\n",
    "                for j in random_range:\n",
    "                    if j[0] < i <= j[1]:\n",
    "                        idx_list.append(random_range.index(j))\n",
    "                        break\n",
    "            new_x = x[idx_list, :, :]\n",
    "            new_y = y[idx_list]\n",
    "            print(new_x.sum().sum())\n",
    "            print('new_x, new_y', new_x.shape, new_y.shape)\n",
    "            return super(MyKerasClassifier, self).fit(new_x, new_y, **kwargs)\n",
    "        \n",
    "    def predict(self, x, **kwargs):\n",
    "        return super(MyKerasClassifier, self).predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaboost-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost LSTM Start\n",
      "sample weight :  [0.00016176 0.00016176 0.00016176 ... 0.00016176 0.00016176 0.00016176]\n",
      "x, y (6182, 10, 4068) 1644947.0\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/70\n",
      "2/2 [==============================] - 7s 779ms/step - loss: 0.6868 - acc: 0.5621 - val_loss: 0.6661 - val_acc: 0.6177\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 159ms/step - loss: 0.6768 - acc: 0.5973 - val_loss: 0.6657 - val_acc: 0.6177\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 162ms/step - loss: 0.6746 - acc: 0.6059 - val_loss: 0.6667 - val_acc: 0.6177\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 0s 162ms/step - loss: 0.6765 - acc: 0.6083 - val_loss: 0.6660 - val_acc: 0.6177\n",
      "Epoch 5/70\n",
      "2/2 [==============================] - 0s 182ms/step - loss: 0.6810 - acc: 0.6048 - val_loss: 0.6651 - val_acc: 0.6177\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 167ms/step - loss: 0.6777 - acc: 0.6046 - val_loss: 0.6653 - val_acc: 0.6177\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 161ms/step - loss: 0.6737 - acc: 0.6012 - val_loss: 0.6657 - val_acc: 0.6177\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 158ms/step - loss: 0.6733 - acc: 0.6022 - val_loss: 0.6658 - val_acc: 0.6177\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 154ms/step - loss: 0.6743 - acc: 0.6009 - val_loss: 0.6654 - val_acc: 0.6177\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 154ms/step - loss: 0.6749 - acc: 0.6025 - val_loss: 0.6651 - val_acc: 0.6177\n",
      "Epoch 11/70\n",
      "2/2 [==============================] - 0s 170ms/step - loss: 0.6737 - acc: 0.6083 - val_loss: 0.6650 - val_acc: 0.6177\n",
      "Epoch 12/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6732 - acc: 0.6068 - val_loss: 0.6650 - val_acc: 0.6177\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 160ms/step - loss: 0.6716 - acc: 0.6059 - val_loss: 0.6650 - val_acc: 0.6177\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 165ms/step - loss: 0.6764 - acc: 0.6070 - val_loss: 0.6649 - val_acc: 0.6177\n",
      "Epoch 15/70\n",
      "2/2 [==============================] - 0s 167ms/step - loss: 0.6744 - acc: 0.6083 - val_loss: 0.6649 - val_acc: 0.6177\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 152ms/step - loss: 0.6731 - acc: 0.6079 - val_loss: 0.6648 - val_acc: 0.6177\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 169ms/step - loss: 0.6744 - acc: 0.6079 - val_loss: 0.6647 - val_acc: 0.6177\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 165ms/step - loss: 0.6737 - acc: 0.6040 - val_loss: 0.6646 - val_acc: 0.6177\n",
      "Epoch 19/70\n",
      "2/2 [==============================] - 0s 176ms/step - loss: 0.6730 - acc: 0.6072 - val_loss: 0.6645 - val_acc: 0.6177\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 160ms/step - loss: 0.6736 - acc: 0.6053 - val_loss: 0.6644 - val_acc: 0.6177\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 163ms/step - loss: 0.6696 - acc: 0.6109 - val_loss: 0.6643 - val_acc: 0.6177\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 156ms/step - loss: 0.6710 - acc: 0.6079 - val_loss: 0.6641 - val_acc: 0.6177\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 159ms/step - loss: 0.6720 - acc: 0.6074 - val_loss: 0.6635 - val_acc: 0.6177\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6710 - acc: 0.6083 - val_loss: 0.6623 - val_acc: 0.6177\n",
      "Epoch 25/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6665 - acc: 0.6143 - val_loss: 0.6607 - val_acc: 0.6177\n",
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6678 - acc: 0.6122 - val_loss: 0.6590 - val_acc: 0.6177\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6668 - acc: 0.6128 - val_loss: 0.6563 - val_acc: 0.6177\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6621 - acc: 0.6124 - val_loss: 0.6529 - val_acc: 0.6177\n",
      "Epoch 29/70\n",
      "2/2 [==============================] - 0s 152ms/step - loss: 0.6573 - acc: 0.6113 - val_loss: 0.6484 - val_acc: 0.6177\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6564 - acc: 0.6126 - val_loss: 0.6419 - val_acc: 0.6177\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6476 - acc: 0.6148 - val_loss: 0.6339 - val_acc: 0.6177\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6378 - acc: 0.6176 - val_loss: 0.6236 - val_acc: 0.6177\n",
      "Epoch 33/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6276 - acc: 0.6296 - val_loss: 0.6125 - val_acc: 0.6177\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6167 - acc: 0.6473 - val_loss: 0.6001 - val_acc: 0.6928\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6010 - acc: 0.6764 - val_loss: 0.5865 - val_acc: 0.7109\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.5832 - acc: 0.7034 - val_loss: 0.5719 - val_acc: 0.7270\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.5680 - acc: 0.7261 - val_loss: 0.5567 - val_acc: 0.7329\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.5484 - acc: 0.7420 - val_loss: 0.5510 - val_acc: 0.7348\n",
      "Epoch 39/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.5373 - acc: 0.7470 - val_loss: 0.5314 - val_acc: 0.7484\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.5139 - acc: 0.7660 - val_loss: 0.5201 - val_acc: 0.7503\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.4982 - acc: 0.7808 - val_loss: 0.5159 - val_acc: 0.7484\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.4830 - acc: 0.7880 - val_loss: 0.5051 - val_acc: 0.7613\n",
      "Epoch 43/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.4667 - acc: 0.8011 - val_loss: 0.5103 - val_acc: 0.7568\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.4554 - acc: 0.8044 - val_loss: 0.4982 - val_acc: 0.7600\n",
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.4459 - acc: 0.8132 - val_loss: 0.5003 - val_acc: 0.7639\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.4271 - acc: 0.8208 - val_loss: 0.4998 - val_acc: 0.7665\n",
      "Epoch 47/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.4170 - acc: 0.8305 - val_loss: 0.5040 - val_acc: 0.7652\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.4143 - acc: 0.8313 - val_loss: 0.5062 - val_acc: 0.7717\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.4015 - acc: 0.8365 - val_loss: 0.5129 - val_acc: 0.7697\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.3915 - acc: 0.8419 - val_loss: 0.5064 - val_acc: 0.7671\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.3869 - acc: 0.8484 - val_loss: 0.5083 - val_acc: 0.7717\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.3722 - acc: 0.8572 - val_loss: 0.5088 - val_acc: 0.7723\n",
      "Epoch 53/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.3615 - acc: 0.8628 - val_loss: 0.5124 - val_acc: 0.7743\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.3550 - acc: 0.8684 - val_loss: 0.5163 - val_acc: 0.7723\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.3470 - acc: 0.8749 - val_loss: 0.5339 - val_acc: 0.7613\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 0s 151ms/step - loss: 0.3531 - acc: 0.8632 - val_loss: 0.5345 - val_acc: 0.7730\n",
      "Epoch 57/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 137ms/step - loss: 0.3351 - acc: 0.8770 - val_loss: 0.5336 - val_acc: 0.7626\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.3237 - acc: 0.8861 - val_loss: 0.5391 - val_acc: 0.7626\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.3121 - acc: 0.8904 - val_loss: 0.5450 - val_acc: 0.7594\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.3113 - acc: 0.8913 - val_loss: 0.5521 - val_acc: 0.7633\n",
      "Epoch 61/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.3021 - acc: 0.8975 - val_loss: 0.5582 - val_acc: 0.7620\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.2978 - acc: 0.9001 - val_loss: 0.5625 - val_acc: 0.7639\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.2901 - acc: 0.8999 - val_loss: 0.5666 - val_acc: 0.7665\n",
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.2829 - acc: 0.9021 - val_loss: 0.5711 - val_acc: 0.7646\n",
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.2798 - acc: 0.9081 - val_loss: 0.5813 - val_acc: 0.7658\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.2814 - acc: 0.9036 - val_loss: 0.5875 - val_acc: 0.7646\n",
      "Epoch 67/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.2680 - acc: 0.9094 - val_loss: 0.6067 - val_acc: 0.7581\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.2813 - acc: 0.9023 - val_loss: 0.5976 - val_acc: 0.7613\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.2606 - acc: 0.9165 - val_loss: 0.6046 - val_acc: 0.7600\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.2586 - acc: 0.9174 - val_loss: 0.6114 - val_acc: 0.7607\n",
      "sample weight :  [1.23566246e-04 5.91051960e-05 4.54982532e-05 ... 5.33510117e-05\n",
      " 4.44828578e-05 6.41236526e-05]\n",
      "1585017.0\n",
      "new_x, new_y (6182, 10, 4068) (6182,)\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/70\n",
      "2/2 [==============================] - 6s 756ms/step - loss: 0.7009 - acc: 0.5373 - val_loss: 0.6898 - val_acc: 0.5427\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6935 - acc: 0.5313 - val_loss: 0.6899 - val_acc: 0.5427\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6963 - acc: 0.5134 - val_loss: 0.6910 - val_acc: 0.5427\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6963 - acc: 0.5075 - val_loss: 0.6903 - val_acc: 0.5427\n",
      "Epoch 5/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6951 - acc: 0.5205 - val_loss: 0.6895 - val_acc: 0.5427\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6950 - acc: 0.5203 - val_loss: 0.6898 - val_acc: 0.5427\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6945 - acc: 0.5365 - val_loss: 0.6903 - val_acc: 0.5427\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6925 - acc: 0.5410 - val_loss: 0.6904 - val_acc: 0.5427\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6958 - acc: 0.5298 - val_loss: 0.6899 - val_acc: 0.5427\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6921 - acc: 0.5464 - val_loss: 0.6896 - val_acc: 0.5427\n",
      "Epoch 11/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6931 - acc: 0.5267 - val_loss: 0.6895 - val_acc: 0.5427\n",
      "Epoch 12/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6943 - acc: 0.5242 - val_loss: 0.6896 - val_acc: 0.5427\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6924 - acc: 0.5263 - val_loss: 0.6896 - val_acc: 0.5427\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6934 - acc: 0.5229 - val_loss: 0.6896 - val_acc: 0.5427\n",
      "Epoch 15/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6939 - acc: 0.5160 - val_loss: 0.6895 - val_acc: 0.5427\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6946 - acc: 0.5186 - val_loss: 0.6895 - val_acc: 0.5427\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6936 - acc: 0.5205 - val_loss: 0.6894 - val_acc: 0.5427\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6926 - acc: 0.5339 - val_loss: 0.6894 - val_acc: 0.5427\n",
      "Epoch 19/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6924 - acc: 0.5311 - val_loss: 0.6894 - val_acc: 0.5427\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6901 - acc: 0.5371 - val_loss: 0.6894 - val_acc: 0.5427\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 0.6911 - acc: 0.5317 - val_loss: 0.6894 - val_acc: 0.5427\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6920 - acc: 0.5300 - val_loss: 0.6895 - val_acc: 0.5427\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6913 - acc: 0.5339 - val_loss: 0.6895 - val_acc: 0.5427\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6916 - acc: 0.5371 - val_loss: 0.6895 - val_acc: 0.5427\n",
      "Epoch 25/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6913 - acc: 0.5360 - val_loss: 0.6894 - val_acc: 0.5427\n",
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6906 - acc: 0.5390 - val_loss: 0.6893 - val_acc: 0.5427\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6905 - acc: 0.5352 - val_loss: 0.6892 - val_acc: 0.5427\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6898 - acc: 0.5414 - val_loss: 0.6893 - val_acc: 0.5427\n",
      "Epoch 29/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6920 - acc: 0.5270 - val_loss: 0.6893 - val_acc: 0.5427\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6912 - acc: 0.5326 - val_loss: 0.6892 - val_acc: 0.5427\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6893 - acc: 0.5382 - val_loss: 0.6889 - val_acc: 0.5427\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6909 - acc: 0.5317 - val_loss: 0.6886 - val_acc: 0.5427\n",
      "Epoch 33/70\n",
      "2/2 [==============================] - 0s 345ms/step - loss: 0.6909 - acc: 0.5336 - val_loss: 0.6884 - val_acc: 0.5427\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6881 - acc: 0.5399 - val_loss: 0.6881 - val_acc: 0.5427\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6884 - acc: 0.5429 - val_loss: 0.6877 - val_acc: 0.5427\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6875 - acc: 0.5483 - val_loss: 0.6870 - val_acc: 0.5427\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6883 - acc: 0.5421 - val_loss: 0.6861 - val_acc: 0.5427\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6880 - acc: 0.5375 - val_loss: 0.6848 - val_acc: 0.5427\n",
      "Epoch 39/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6873 - acc: 0.5440 - val_loss: 0.6829 - val_acc: 0.5427\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 0.6833 - acc: 0.5552 - val_loss: 0.6803 - val_acc: 0.5427\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6818 - acc: 0.5619 - val_loss: 0.6765 - val_acc: 0.5427\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6755 - acc: 0.5764 - val_loss: 0.6715 - val_acc: 0.5427\n",
      "Epoch 43/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6690 - acc: 0.5981 - val_loss: 0.6648 - val_acc: 0.6300\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6629 - acc: 0.6130 - val_loss: 0.6579 - val_acc: 0.6688\n",
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6540 - acc: 0.6521 - val_loss: 0.6481 - val_acc: 0.6662\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6436 - acc: 0.6739 - val_loss: 0.6396 - val_acc: 0.6772\n",
      "Epoch 47/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6275 - acc: 0.7062 - val_loss: 0.6251 - val_acc: 0.6934\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6116 - acc: 0.7187 - val_loss: 0.6102 - val_acc: 0.7050\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.5904 - acc: 0.7396 - val_loss: 0.6169 - val_acc: 0.6643\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.5803 - acc: 0.7299 - val_loss: 0.5969 - val_acc: 0.7038\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.5675 - acc: 0.7355 - val_loss: 0.6072 - val_acc: 0.6630\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.5485 - acc: 0.7351 - val_loss: 0.5826 - val_acc: 0.7257\n",
      "Epoch 53/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.5402 - acc: 0.7601 - val_loss: 0.5667 - val_acc: 0.7238\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.5102 - acc: 0.7750 - val_loss: 0.6058 - val_acc: 0.6895\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.5177 - acc: 0.7638 - val_loss: 0.5698 - val_acc: 0.7400\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.5056 - acc: 0.7830 - val_loss: 0.5509 - val_acc: 0.7445\n",
      "Epoch 57/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.4733 - acc: 0.7979 - val_loss: 0.5868 - val_acc: 0.7096\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.4823 - acc: 0.7817 - val_loss: 0.5402 - val_acc: 0.7529\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.4495 - acc: 0.8164 - val_loss: 0.5354 - val_acc: 0.7549\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.4422 - acc: 0.8167 - val_loss: 0.5378 - val_acc: 0.7503\n",
      "Epoch 61/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.4322 - acc: 0.8274 - val_loss: 0.5270 - val_acc: 0.7620\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.4214 - acc: 0.8324 - val_loss: 0.5248 - val_acc: 0.7658\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.4128 - acc: 0.8397 - val_loss: 0.5294 - val_acc: 0.7697\n",
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.4102 - acc: 0.8371 - val_loss: 0.5222 - val_acc: 0.7743\n",
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.4018 - acc: 0.8492 - val_loss: 0.5383 - val_acc: 0.7555\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.3985 - acc: 0.8399 - val_loss: 0.5243 - val_acc: 0.7755\n",
      "Epoch 67/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.3832 - acc: 0.8466 - val_loss: 0.5220 - val_acc: 0.7762\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.3736 - acc: 0.8542 - val_loss: 0.5233 - val_acc: 0.7788\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.3653 - acc: 0.8624 - val_loss: 0.5233 - val_acc: 0.7827\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.3586 - acc: 0.8660 - val_loss: 0.5333 - val_acc: 0.7781\n",
      "sample weight :  [1.36528475e-04 1.94546977e-04 1.72327575e-04 ... 3.03877552e-05\n",
      " 1.37388787e-04 8.21824425e-05]\n",
      "1489287.0\n",
      "new_x, new_y (6182, 10, 4068) (6182,)\n",
      "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/70\n",
      "2/2 [==============================] - 6s 552ms/step - loss: 0.7115 - acc: 0.5078 - val_loss: 0.6935 - val_acc: 0.5213\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.7029 - acc: 0.5097 - val_loss: 0.6926 - val_acc: 0.5213\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.7021 - acc: 0.4961 - val_loss: 0.6945 - val_acc: 0.4787\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.7018 - acc: 0.4884 - val_loss: 0.6946 - val_acc: 0.4787\n",
      "Epoch 5/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6999 - acc: 0.5011 - val_loss: 0.6932 - val_acc: 0.4787\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.7013 - acc: 0.4879 - val_loss: 0.6923 - val_acc: 0.5213\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.7015 - acc: 0.4937 - val_loss: 0.6923 - val_acc: 0.5213\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.7000 - acc: 0.4946 - val_loss: 0.6926 - val_acc: 0.5213\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6999 - acc: 0.5041 - val_loss: 0.6927 - val_acc: 0.5213\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6976 - acc: 0.5160 - val_loss: 0.6924 - val_acc: 0.5213\n",
      "Epoch 11/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6962 - acc: 0.5097 - val_loss: 0.6922 - val_acc: 0.5213\n",
      "Epoch 12/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6983 - acc: 0.5028 - val_loss: 0.6924 - val_acc: 0.5213\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 151ms/step - loss: 0.6981 - acc: 0.5004 - val_loss: 0.6930 - val_acc: 0.5220\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6975 - acc: 0.4991 - val_loss: 0.6936 - val_acc: 0.4787\n",
      "Epoch 15/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6976 - acc: 0.5082 - val_loss: 0.6937 - val_acc: 0.4787\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6972 - acc: 0.4957 - val_loss: 0.6935 - val_acc: 0.4787\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6968 - acc: 0.5006 - val_loss: 0.6929 - val_acc: 0.5213\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6957 - acc: 0.5041 - val_loss: 0.6924 - val_acc: 0.5213\n",
      "Epoch 19/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6971 - acc: 0.5080 - val_loss: 0.6921 - val_acc: 0.5213\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6962 - acc: 0.5091 - val_loss: 0.6921 - val_acc: 0.5213\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6981 - acc: 0.5069 - val_loss: 0.6921 - val_acc: 0.5213\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6973 - acc: 0.5000 - val_loss: 0.6921 - val_acc: 0.5213\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6968 - acc: 0.4991 - val_loss: 0.6920 - val_acc: 0.5213\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6987 - acc: 0.5050 - val_loss: 0.6920 - val_acc: 0.5213\n",
      "Epoch 25/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6947 - acc: 0.5043 - val_loss: 0.6920 - val_acc: 0.5213\n",
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6940 - acc: 0.5114 - val_loss: 0.6921 - val_acc: 0.5213\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6962 - acc: 0.4946 - val_loss: 0.6923 - val_acc: 0.5213\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6955 - acc: 0.5039 - val_loss: 0.6924 - val_acc: 0.5213\n",
      "Epoch 29/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6959 - acc: 0.4981 - val_loss: 0.6923 - val_acc: 0.5213\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6933 - acc: 0.5220 - val_loss: 0.6920 - val_acc: 0.5213\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6928 - acc: 0.5201 - val_loss: 0.6917 - val_acc: 0.5213\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6937 - acc: 0.5183 - val_loss: 0.6914 - val_acc: 0.5213\n",
      "Epoch 33/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6953 - acc: 0.5060 - val_loss: 0.6911 - val_acc: 0.5213\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6932 - acc: 0.5166 - val_loss: 0.6908 - val_acc: 0.5213\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6928 - acc: 0.5224 - val_loss: 0.6905 - val_acc: 0.5213\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6937 - acc: 0.5170 - val_loss: 0.6901 - val_acc: 0.5213\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6920 - acc: 0.5181 - val_loss: 0.6895 - val_acc: 0.5213\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6915 - acc: 0.5278 - val_loss: 0.6886 - val_acc: 0.5356\n",
      "Epoch 39/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6906 - acc: 0.5261 - val_loss: 0.6873 - val_acc: 0.5834\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6911 - acc: 0.5250 - val_loss: 0.6858 - val_acc: 0.6151\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6872 - acc: 0.5431 - val_loss: 0.6840 - val_acc: 0.6145\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6853 - acc: 0.5597 - val_loss: 0.6814 - val_acc: 0.6171\n",
      "Epoch 43/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6859 - acc: 0.5578 - val_loss: 0.6779 - val_acc: 0.6248\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6795 - acc: 0.5837 - val_loss: 0.6734 - val_acc: 0.6320\n",
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6738 - acc: 0.6111 - val_loss: 0.6680 - val_acc: 0.6417\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6680 - acc: 0.6314 - val_loss: 0.6610 - val_acc: 0.6481\n",
      "Epoch 47/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6600 - acc: 0.6465 - val_loss: 0.6523 - val_acc: 0.6527\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6501 - acc: 0.6620 - val_loss: 0.6416 - val_acc: 0.6611\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6355 - acc: 0.6857 - val_loss: 0.6292 - val_acc: 0.6766\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6206 - acc: 0.7002 - val_loss: 0.6164 - val_acc: 0.6966\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6063 - acc: 0.7153 - val_loss: 0.6047 - val_acc: 0.7018\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.5901 - acc: 0.7237 - val_loss: 0.5942 - val_acc: 0.7025\n",
      "Epoch 53/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.5705 - acc: 0.7394 - val_loss: 0.5843 - val_acc: 0.7005\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.5554 - acc: 0.7468 - val_loss: 0.5726 - val_acc: 0.7212\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.5404 - acc: 0.7487 - val_loss: 0.5662 - val_acc: 0.7251\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.5268 - acc: 0.7632 - val_loss: 0.5560 - val_acc: 0.7277\n",
      "Epoch 57/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.5114 - acc: 0.7634 - val_loss: 0.5478 - val_acc: 0.7490\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.4976 - acc: 0.7836 - val_loss: 0.5435 - val_acc: 0.7471\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.4876 - acc: 0.7834 - val_loss: 0.5345 - val_acc: 0.7581\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.4732 - acc: 0.7966 - val_loss: 0.5297 - val_acc: 0.7633\n",
      "Epoch 61/70\n",
      "2/2 [==============================] - 0s 152ms/step - loss: 0.4632 - acc: 0.8041 - val_loss: 0.5246 - val_acc: 0.7704\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.4516 - acc: 0.8145 - val_loss: 0.5218 - val_acc: 0.7710\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.4428 - acc: 0.8182 - val_loss: 0.5237 - val_acc: 0.7658\n",
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.4357 - acc: 0.8201 - val_loss: 0.5206 - val_acc: 0.7730\n",
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.4257 - acc: 0.8283 - val_loss: 0.5201 - val_acc: 0.7730\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.4200 - acc: 0.8287 - val_loss: 0.5202 - val_acc: 0.7736\n",
      "Epoch 67/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.4116 - acc: 0.8387 - val_loss: 0.5180 - val_acc: 0.7704\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 151ms/step - loss: 0.4053 - acc: 0.8397 - val_loss: 0.5127 - val_acc: 0.7846\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.3916 - acc: 0.8525 - val_loss: 0.5150 - val_acc: 0.7846\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.3902 - acc: 0.8514 - val_loss: 0.5113 - val_acc: 0.7859\n",
      "sample weight :  [5.45858744e-05 7.78861927e-05 7.27571750e-05 ... 1.96044626e-05\n",
      " 5.55710154e-05 6.59238008e-05]\n",
      "1499443.0\n",
      "new_x, new_y (6182, 10, 4068) (6182,)\n",
      "WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_15 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/70\n",
      "2/2 [==============================] - 6s 1s/step - loss: 0.7155 - acc: 0.5149 - val_loss: 0.6990 - val_acc: 0.5071\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.7018 - acc: 0.5170 - val_loss: 0.6933 - val_acc: 0.5071\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6986 - acc: 0.5039 - val_loss: 0.6935 - val_acc: 0.4929\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.7019 - acc: 0.4927 - val_loss: 0.6939 - val_acc: 0.4929\n",
      "Epoch 5/70\n",
      "2/2 [==============================] - 0s 299ms/step - loss: 0.6992 - acc: 0.5050 - val_loss: 0.6935 - val_acc: 0.4929\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 154ms/step - loss: 0.6996 - acc: 0.4970 - val_loss: 0.6931 - val_acc: 0.5071\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6988 - acc: 0.4991 - val_loss: 0.6931 - val_acc: 0.5071\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6973 - acc: 0.5035 - val_loss: 0.6936 - val_acc: 0.5071\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6985 - acc: 0.5002 - val_loss: 0.6940 - val_acc: 0.5071\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6998 - acc: 0.5063 - val_loss: 0.6946 - val_acc: 0.5071\n",
      "Epoch 11/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6985 - acc: 0.5086 - val_loss: 0.6946 - val_acc: 0.5071\n",
      "Epoch 12/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6985 - acc: 0.5030 - val_loss: 0.6942 - val_acc: 0.5071\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6953 - acc: 0.5196 - val_loss: 0.6937 - val_acc: 0.5071\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6979 - acc: 0.5071 - val_loss: 0.6932 - val_acc: 0.5071\n",
      "Epoch 15/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6953 - acc: 0.5101 - val_loss: 0.6930 - val_acc: 0.5071\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 0.6964 - acc: 0.5093 - val_loss: 0.6930 - val_acc: 0.5071\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6984 - acc: 0.5017 - val_loss: 0.6931 - val_acc: 0.5071\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6972 - acc: 0.5056 - val_loss: 0.6932 - val_acc: 0.5071\n",
      "Epoch 19/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6971 - acc: 0.5080 - val_loss: 0.6933 - val_acc: 0.5071\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6966 - acc: 0.5006 - val_loss: 0.6934 - val_acc: 0.5071\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6969 - acc: 0.5056 - val_loss: 0.6935 - val_acc: 0.5071\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 154ms/step - loss: 0.6982 - acc: 0.5002 - val_loss: 0.6934 - val_acc: 0.5071\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6967 - acc: 0.5099 - val_loss: 0.6933 - val_acc: 0.5071\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6969 - acc: 0.5000 - val_loss: 0.6932 - val_acc: 0.5071\n",
      "Epoch 25/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6970 - acc: 0.5047 - val_loss: 0.6931 - val_acc: 0.5071\n",
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6959 - acc: 0.5080 - val_loss: 0.6930 - val_acc: 0.5071\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6965 - acc: 0.5091 - val_loss: 0.6930 - val_acc: 0.5071\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6943 - acc: 0.5136 - val_loss: 0.6930 - val_acc: 0.5071\n",
      "Epoch 29/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6942 - acc: 0.5151 - val_loss: 0.6932 - val_acc: 0.5071\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.6959 - acc: 0.5132 - val_loss: 0.6937 - val_acc: 0.5071\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6936 - acc: 0.5259 - val_loss: 0.6938 - val_acc: 0.5071\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6969 - acc: 0.5071 - val_loss: 0.6936 - val_acc: 0.5071\n",
      "Epoch 33/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6964 - acc: 0.5043 - val_loss: 0.6934 - val_acc: 0.5071\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6976 - acc: 0.4972 - val_loss: 0.6932 - val_acc: 0.5071\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6943 - acc: 0.5134 - val_loss: 0.6930 - val_acc: 0.5071\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6948 - acc: 0.5039 - val_loss: 0.6929 - val_acc: 0.5071\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6940 - acc: 0.5082 - val_loss: 0.6928 - val_acc: 0.5071\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 157ms/step - loss: 0.6923 - acc: 0.5114 - val_loss: 0.6925 - val_acc: 0.5071\n",
      "Epoch 39/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6925 - acc: 0.5203 - val_loss: 0.6922 - val_acc: 0.5071\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6941 - acc: 0.5054 - val_loss: 0.6919 - val_acc: 0.5071\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6909 - acc: 0.5242 - val_loss: 0.6915 - val_acc: 0.5071\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6922 - acc: 0.5265 - val_loss: 0.6910 - val_acc: 0.5071\n",
      "Epoch 43/70\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 0.6936 - acc: 0.5110 - val_loss: 0.6904 - val_acc: 0.5071\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6912 - acc: 0.5207 - val_loss: 0.6895 - val_acc: 0.5071\n",
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6921 - acc: 0.5177 - val_loss: 0.6888 - val_acc: 0.5071\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6889 - acc: 0.5406 - val_loss: 0.6877 - val_acc: 0.5071\n",
      "Epoch 47/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6874 - acc: 0.5503 - val_loss: 0.6859 - val_acc: 0.5614\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 151ms/step - loss: 0.6859 - acc: 0.5565 - val_loss: 0.6837 - val_acc: 0.5906\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6827 - acc: 0.5673 - val_loss: 0.6811 - val_acc: 0.5957\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6772 - acc: 0.5813 - val_loss: 0.6782 - val_acc: 0.5977\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6764 - acc: 0.5863 - val_loss: 0.6729 - val_acc: 0.6442\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6703 - acc: 0.6119 - val_loss: 0.6675 - val_acc: 0.6436\n",
      "Epoch 53/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6618 - acc: 0.6376 - val_loss: 0.6626 - val_acc: 0.6397\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6535 - acc: 0.6480 - val_loss: 0.6526 - val_acc: 0.6617\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6420 - acc: 0.6752 - val_loss: 0.6425 - val_acc: 0.6643\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6267 - acc: 0.6959 - val_loss: 0.6304 - val_acc: 0.6856\n",
      "Epoch 57/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6104 - acc: 0.7110 - val_loss: 0.6235 - val_acc: 0.6721\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.5899 - acc: 0.7343 - val_loss: 0.6142 - val_acc: 0.6831\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.5755 - acc: 0.7336 - val_loss: 0.6200 - val_acc: 0.6714\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.5650 - acc: 0.7349 - val_loss: 0.6042 - val_acc: 0.6908\n",
      "Epoch 61/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.5420 - acc: 0.7539 - val_loss: 0.6016 - val_acc: 0.6947\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.5243 - acc: 0.7673 - val_loss: 0.5970 - val_acc: 0.7089\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.5039 - acc: 0.7785 - val_loss: 0.5975 - val_acc: 0.7115\n",
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.4910 - acc: 0.7899 - val_loss: 0.6008 - val_acc: 0.7154\n",
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.4767 - acc: 0.7959 - val_loss: 0.6027 - val_acc: 0.7186\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.4666 - acc: 0.7998 - val_loss: 0.6040 - val_acc: 0.7245\n",
      "Epoch 67/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.4628 - acc: 0.8085 - val_loss: 0.6045 - val_acc: 0.7303\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.4497 - acc: 0.8227 - val_loss: 0.6143 - val_acc: 0.7193\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.4556 - acc: 0.8149 - val_loss: 0.6127 - val_acc: 0.7277\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.4443 - acc: 0.8229 - val_loss: 0.5984 - val_acc: 0.7342\n",
      "sample weight :  [1.65987521e-04 3.59219326e-05 6.93738115e-05 ... 6.12874941e-05\n",
      " 1.75402600e-04 6.35699121e-05]\n",
      "1419753.0\n",
      "new_x, new_y (6182, 10, 4068) (6182,)\n",
      "WARNING:tensorflow:Layer lstm_16 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_17 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_18 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_19 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 6s 549ms/step - loss: 0.7011 - acc: 0.4903 - val_loss: 0.6932 - val_acc: 0.4903\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6970 - acc: 0.4955 - val_loss: 0.6930 - val_acc: 0.5097\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6967 - acc: 0.5088 - val_loss: 0.6930 - val_acc: 0.5097\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6974 - acc: 0.5052 - val_loss: 0.6930 - val_acc: 0.5097\n",
      "Epoch 5/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6961 - acc: 0.5030 - val_loss: 0.6930 - val_acc: 0.5097\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6943 - acc: 0.5173 - val_loss: 0.6930 - val_acc: 0.5097\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 155ms/step - loss: 0.6954 - acc: 0.5054 - val_loss: 0.6930 - val_acc: 0.5097\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6967 - acc: 0.4994 - val_loss: 0.6929 - val_acc: 0.5097\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6989 - acc: 0.4849 - val_loss: 0.6929 - val_acc: 0.5097\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6981 - acc: 0.4875 - val_loss: 0.6930 - val_acc: 0.5097\n",
      "Epoch 11/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6957 - acc: 0.5047 - val_loss: 0.6929 - val_acc: 0.5097\n",
      "Epoch 12/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6954 - acc: 0.4994 - val_loss: 0.6929 - val_acc: 0.5097\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6943 - acc: 0.5123 - val_loss: 0.6929 - val_acc: 0.5097\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6966 - acc: 0.4996 - val_loss: 0.6929 - val_acc: 0.5097\n",
      "Epoch 15/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6949 - acc: 0.5011 - val_loss: 0.6929 - val_acc: 0.5097\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6962 - acc: 0.4981 - val_loss: 0.6929 - val_acc: 0.5097\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6935 - acc: 0.5214 - val_loss: 0.6930 - val_acc: 0.5097\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6934 - acc: 0.5075 - val_loss: 0.6930 - val_acc: 0.5097\n",
      "Epoch 19/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6949 - acc: 0.5086 - val_loss: 0.6930 - val_acc: 0.5097\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6957 - acc: 0.5032 - val_loss: 0.6929 - val_acc: 0.5097\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6955 - acc: 0.5004 - val_loss: 0.6929 - val_acc: 0.5097\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6951 - acc: 0.5002 - val_loss: 0.6929 - val_acc: 0.5097\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6949 - acc: 0.4965 - val_loss: 0.6929 - val_acc: 0.5097\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6944 - acc: 0.5075 - val_loss: 0.6929 - val_acc: 0.5097\n",
      "Epoch 25/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6949 - acc: 0.5017 - val_loss: 0.6929 - val_acc: 0.5097\n",
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6947 - acc: 0.4985 - val_loss: 0.6929 - val_acc: 0.5097\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6936 - acc: 0.5073 - val_loss: 0.6929 - val_acc: 0.5239\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6941 - acc: 0.5017 - val_loss: 0.6929 - val_acc: 0.4987\n",
      "Epoch 29/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6937 - acc: 0.5043 - val_loss: 0.6930 - val_acc: 0.4903\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6946 - acc: 0.5037 - val_loss: 0.6929 - val_acc: 0.4903\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6948 - acc: 0.4996 - val_loss: 0.6927 - val_acc: 0.5737\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6946 - acc: 0.4976 - val_loss: 0.6924 - val_acc: 0.5097\n",
      "Epoch 33/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6946 - acc: 0.4972 - val_loss: 0.6923 - val_acc: 0.5097\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6934 - acc: 0.5075 - val_loss: 0.6921 - val_acc: 0.5097\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6929 - acc: 0.5065 - val_loss: 0.6919 - val_acc: 0.5123\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6925 - acc: 0.5129 - val_loss: 0.6915 - val_acc: 0.5123\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6923 - acc: 0.5155 - val_loss: 0.6911 - val_acc: 0.5123\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6920 - acc: 0.5173 - val_loss: 0.6905 - val_acc: 0.5220\n",
      "Epoch 39/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6931 - acc: 0.5166 - val_loss: 0.6897 - val_acc: 0.5291\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6895 - acc: 0.5321 - val_loss: 0.6886 - val_acc: 0.5938\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6878 - acc: 0.5526 - val_loss: 0.6873 - val_acc: 0.5925\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6856 - acc: 0.5692 - val_loss: 0.6851 - val_acc: 0.5970\n",
      "Epoch 43/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6830 - acc: 0.5846 - val_loss: 0.6823 - val_acc: 0.5873\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6800 - acc: 0.6012 - val_loss: 0.6787 - val_acc: 0.5880\n",
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6734 - acc: 0.6214 - val_loss: 0.6733 - val_acc: 0.6184\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6656 - acc: 0.6368 - val_loss: 0.6676 - val_acc: 0.6113\n",
      "Epoch 47/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6539 - acc: 0.6596 - val_loss: 0.6596 - val_acc: 0.6326\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6425 - acc: 0.6855 - val_loss: 0.6534 - val_acc: 0.6268\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6296 - acc: 0.6879 - val_loss: 0.6437 - val_acc: 0.6449\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6099 - acc: 0.7066 - val_loss: 0.6410 - val_acc: 0.6462\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.5971 - acc: 0.7118 - val_loss: 0.6369 - val_acc: 0.6455\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.5795 - acc: 0.7142 - val_loss: 0.6273 - val_acc: 0.6682\n",
      "Epoch 53/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.5552 - acc: 0.7435 - val_loss: 0.6269 - val_acc: 0.6721\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 0.5427 - acc: 0.7489 - val_loss: 0.6271 - val_acc: 0.6837\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.5291 - acc: 0.7604 - val_loss: 0.6342 - val_acc: 0.6727\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.5179 - acc: 0.7651 - val_loss: 0.6313 - val_acc: 0.6889\n",
      "Epoch 57/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.5122 - acc: 0.7739 - val_loss: 0.6326 - val_acc: 0.6889\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.5054 - acc: 0.7772 - val_loss: 0.6259 - val_acc: 0.6902\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.4892 - acc: 0.7921 - val_loss: 0.6220 - val_acc: 0.7044\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.4832 - acc: 0.7996 - val_loss: 0.6250 - val_acc: 0.6915\n",
      "Epoch 61/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.4830 - acc: 0.7985 - val_loss: 0.6206 - val_acc: 0.7050\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.4762 - acc: 0.7996 - val_loss: 0.6129 - val_acc: 0.7083\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.4615 - acc: 0.8128 - val_loss: 0.6136 - val_acc: 0.7128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.4587 - acc: 0.8145 - val_loss: 0.6163 - val_acc: 0.7096\n",
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.4554 - acc: 0.8173 - val_loss: 0.6155 - val_acc: 0.7063\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.4453 - acc: 0.8255 - val_loss: 0.6126 - val_acc: 0.7251\n",
      "Epoch 67/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.4316 - acc: 0.8343 - val_loss: 0.6142 - val_acc: 0.7135\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.4272 - acc: 0.8399 - val_loss: 0.6124 - val_acc: 0.7264\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 170ms/step - loss: 0.4225 - acc: 0.8382 - val_loss: 0.6124 - val_acc: 0.7335\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.4121 - acc: 0.8477 - val_loss: 0.6147 - val_acc: 0.7309\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fe72c676ca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "sample weight :  [5.58768531e-04 1.68935902e-05 3.66322889e-05 ... 2.42766272e-05\n",
      " 6.18061465e-05 5.36604238e-05]\n",
      "1436860.0\n",
      "new_x, new_y (6182, 10, 4068) (6182,)\n",
      "WARNING:tensorflow:Layer lstm_20 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_22 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_23 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/70\n",
      "2/2 [==============================] - 6s 736ms/step - loss: 0.7397 - acc: 0.5235 - val_loss: 0.7005 - val_acc: 0.5382\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.7112 - acc: 0.5214 - val_loss: 0.6910 - val_acc: 0.5382\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.7027 - acc: 0.5153 - val_loss: 0.6917 - val_acc: 0.5382\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6985 - acc: 0.5125 - val_loss: 0.6965 - val_acc: 0.4618\n",
      "Epoch 5/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.7050 - acc: 0.4903 - val_loss: 0.6981 - val_acc: 0.4618\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 151ms/step - loss: 0.7053 - acc: 0.4940 - val_loss: 0.6967 - val_acc: 0.4618\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.7054 - acc: 0.4916 - val_loss: 0.6941 - val_acc: 0.4618\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.7023 - acc: 0.4972 - val_loss: 0.6920 - val_acc: 0.5382\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6982 - acc: 0.5136 - val_loss: 0.6907 - val_acc: 0.5382\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6955 - acc: 0.5108 - val_loss: 0.6902 - val_acc: 0.5382\n",
      "Epoch 11/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6996 - acc: 0.5147 - val_loss: 0.6902 - val_acc: 0.5382\n",
      "Epoch 12/70\n",
      "2/2 [==============================] - 0s 162ms/step - loss: 0.6979 - acc: 0.5097 - val_loss: 0.6902 - val_acc: 0.5382\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6955 - acc: 0.5151 - val_loss: 0.6902 - val_acc: 0.5382\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6985 - acc: 0.5136 - val_loss: 0.6902 - val_acc: 0.5382\n",
      "Epoch 15/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6992 - acc: 0.5082 - val_loss: 0.6902 - val_acc: 0.5382\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.7000 - acc: 0.5084 - val_loss: 0.6902 - val_acc: 0.5382\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6986 - acc: 0.5088 - val_loss: 0.6901 - val_acc: 0.5382\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6971 - acc: 0.5188 - val_loss: 0.6902 - val_acc: 0.5382\n",
      "Epoch 19/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6971 - acc: 0.5147 - val_loss: 0.6903 - val_acc: 0.5382\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6985 - acc: 0.5035 - val_loss: 0.6905 - val_acc: 0.5382\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6968 - acc: 0.5095 - val_loss: 0.6907 - val_acc: 0.5382\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6995 - acc: 0.4925 - val_loss: 0.6910 - val_acc: 0.5382\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 153ms/step - loss: 0.6965 - acc: 0.5067 - val_loss: 0.6913 - val_acc: 0.5382\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6977 - acc: 0.5149 - val_loss: 0.6912 - val_acc: 0.5382\n",
      "Epoch 25/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6951 - acc: 0.5233 - val_loss: 0.6908 - val_acc: 0.5382\n",
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6984 - acc: 0.4978 - val_loss: 0.6905 - val_acc: 0.5382\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 0.6969 - acc: 0.5032 - val_loss: 0.6903 - val_acc: 0.5382\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.6966 - acc: 0.5013 - val_loss: 0.6903 - val_acc: 0.5382\n",
      "Epoch 29/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6962 - acc: 0.5121 - val_loss: 0.6903 - val_acc: 0.5382\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6961 - acc: 0.5056 - val_loss: 0.6904 - val_acc: 0.5382\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6960 - acc: 0.5022 - val_loss: 0.6903 - val_acc: 0.5382\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6933 - acc: 0.5151 - val_loss: 0.6901 - val_acc: 0.5382\n",
      "Epoch 33/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6940 - acc: 0.5145 - val_loss: 0.6899 - val_acc: 0.5382\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6945 - acc: 0.5110 - val_loss: 0.6896 - val_acc: 0.5382\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6963 - acc: 0.5082 - val_loss: 0.6893 - val_acc: 0.5382\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6922 - acc: 0.5220 - val_loss: 0.6889 - val_acc: 0.5382\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6950 - acc: 0.5170 - val_loss: 0.6887 - val_acc: 0.5382\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6934 - acc: 0.5257 - val_loss: 0.6885 - val_acc: 0.5382\n",
      "Epoch 39/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6926 - acc: 0.5177 - val_loss: 0.6883 - val_acc: 0.5382\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6910 - acc: 0.5343 - val_loss: 0.6879 - val_acc: 0.5382\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6908 - acc: 0.5270 - val_loss: 0.6871 - val_acc: 0.5382\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.6895 - acc: 0.5371 - val_loss: 0.6862 - val_acc: 0.5388\n",
      "Epoch 43/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6897 - acc: 0.5296 - val_loss: 0.6850 - val_acc: 0.5556\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.6872 - acc: 0.5438 - val_loss: 0.6831 - val_acc: 0.5699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6871 - acc: 0.5418 - val_loss: 0.6806 - val_acc: 0.5834\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6853 - acc: 0.5494 - val_loss: 0.6774 - val_acc: 0.5964\n",
      "Epoch 47/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6787 - acc: 0.5828 - val_loss: 0.6734 - val_acc: 0.6119\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 156ms/step - loss: 0.6743 - acc: 0.5971 - val_loss: 0.6684 - val_acc: 0.6410\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6699 - acc: 0.6066 - val_loss: 0.6635 - val_acc: 0.6352\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 0.6634 - acc: 0.6240 - val_loss: 0.6558 - val_acc: 0.6695\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6545 - acc: 0.6568 - val_loss: 0.6488 - val_acc: 0.6630\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6432 - acc: 0.6743 - val_loss: 0.6390 - val_acc: 0.6953\n",
      "Epoch 53/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.6338 - acc: 0.6939 - val_loss: 0.6296 - val_acc: 0.6876\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6190 - acc: 0.7213 - val_loss: 0.6248 - val_acc: 0.6766\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6060 - acc: 0.7157 - val_loss: 0.6096 - val_acc: 0.6953\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.5841 - acc: 0.7437 - val_loss: 0.6020 - val_acc: 0.7005\n",
      "Epoch 57/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.5691 - acc: 0.7504 - val_loss: 0.6106 - val_acc: 0.6837\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.5714 - acc: 0.7321 - val_loss: 0.5923 - val_acc: 0.7044\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.5469 - acc: 0.7558 - val_loss: 0.5871 - val_acc: 0.7076\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.5335 - acc: 0.7683 - val_loss: 0.5936 - val_acc: 0.7018\n",
      "Epoch 61/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.5346 - acc: 0.7647 - val_loss: 0.6000 - val_acc: 0.6921\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.5341 - acc: 0.7571 - val_loss: 0.5929 - val_acc: 0.7096\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.5180 - acc: 0.7737 - val_loss: 0.5782 - val_acc: 0.7283\n",
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.5039 - acc: 0.7875 - val_loss: 0.5769 - val_acc: 0.7303\n",
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.4962 - acc: 0.7927 - val_loss: 0.5767 - val_acc: 0.7283\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.4928 - acc: 0.7947 - val_loss: 0.5765 - val_acc: 0.7367\n",
      "Epoch 67/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.4854 - acc: 0.7966 - val_loss: 0.5803 - val_acc: 0.7342\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.4826 - acc: 0.8024 - val_loss: 0.5829 - val_acc: 0.7257\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.4822 - acc: 0.7962 - val_loss: 0.5752 - val_acc: 0.7419\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.4710 - acc: 0.8162 - val_loss: 0.5755 - val_acc: 0.7477\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fe6ec595a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "sample weight :  [2.36143483e-04 1.44771155e-05 1.72804263e-05 ... 1.40240040e-05\n",
      " 1.08910429e-04 7.02678018e-05]\n",
      "1395948.0\n",
      "new_x, new_y (6182, 10, 4068) (6182,)\n",
      "WARNING:tensorflow:Layer lstm_24 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_25 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_26 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_27 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/70\n",
      "2/2 [==============================] - 6s 575ms/step - loss: 0.7573 - acc: 0.4845 - val_loss: 0.7191 - val_acc: 0.4890\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.7276 - acc: 0.4836 - val_loss: 0.7022 - val_acc: 0.4890\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.7098 - acc: 0.4907 - val_loss: 0.6940 - val_acc: 0.4890\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 1s 482ms/step - loss: 0.7042 - acc: 0.4879 - val_loss: 0.6935 - val_acc: 0.5110\n",
      "Epoch 5/70\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 0.6975 - acc: 0.5149 - val_loss: 0.6977 - val_acc: 0.5110\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.7037 - acc: 0.5108 - val_loss: 0.7003 - val_acc: 0.5110\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.7068 - acc: 0.5123 - val_loss: 0.6996 - val_acc: 0.5110\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.7028 - acc: 0.5175 - val_loss: 0.6973 - val_acc: 0.5110\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.7015 - acc: 0.5101 - val_loss: 0.6950 - val_acc: 0.5110\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.7015 - acc: 0.5006 - val_loss: 0.6935 - val_acc: 0.5110\n",
      "Epoch 11/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6999 - acc: 0.5058 - val_loss: 0.6929 - val_acc: 0.5110\n",
      "Epoch 12/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6988 - acc: 0.5011 - val_loss: 0.6929 - val_acc: 0.5110\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.7029 - acc: 0.4873 - val_loss: 0.6930 - val_acc: 0.5110\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6998 - acc: 0.5039 - val_loss: 0.6930 - val_acc: 0.5110\n",
      "Epoch 15/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6965 - acc: 0.5142 - val_loss: 0.6931 - val_acc: 0.5110\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6956 - acc: 0.5244 - val_loss: 0.6931 - val_acc: 0.5136\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6965 - acc: 0.5138 - val_loss: 0.6930 - val_acc: 0.5110\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6994 - acc: 0.4991 - val_loss: 0.6930 - val_acc: 0.5110\n",
      "Epoch 19/70\n",
      "2/2 [==============================] - 0s 160ms/step - loss: 0.6981 - acc: 0.5043 - val_loss: 0.6929 - val_acc: 0.5110\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6974 - acc: 0.5006 - val_loss: 0.6929 - val_acc: 0.5110\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6971 - acc: 0.5009 - val_loss: 0.6929 - val_acc: 0.5110\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6981 - acc: 0.5002 - val_loss: 0.6929 - val_acc: 0.5110\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 154ms/step - loss: 0.6994 - acc: 0.5002 - val_loss: 0.6930 - val_acc: 0.5110\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 155ms/step - loss: 0.6961 - acc: 0.5151 - val_loss: 0.6931 - val_acc: 0.5110\n",
      "Epoch 25/70\n",
      "2/2 [==============================] - 0s 151ms/step - loss: 0.6977 - acc: 0.5045 - val_loss: 0.6931 - val_acc: 0.5110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 154ms/step - loss: 0.6951 - acc: 0.5129 - val_loss: 0.6931 - val_acc: 0.5110\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6968 - acc: 0.4996 - val_loss: 0.6931 - val_acc: 0.5110\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6947 - acc: 0.5127 - val_loss: 0.6930 - val_acc: 0.5110\n",
      "Epoch 29/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6981 - acc: 0.4942 - val_loss: 0.6929 - val_acc: 0.5110\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6948 - acc: 0.5129 - val_loss: 0.6929 - val_acc: 0.5110\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6966 - acc: 0.5026 - val_loss: 0.6929 - val_acc: 0.5110\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6970 - acc: 0.5041 - val_loss: 0.6929 - val_acc: 0.5110\n",
      "Epoch 33/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.6948 - acc: 0.5149 - val_loss: 0.6929 - val_acc: 0.5110\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 151ms/step - loss: 0.6955 - acc: 0.5116 - val_loss: 0.6929 - val_acc: 0.5110\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 152ms/step - loss: 0.6963 - acc: 0.4983 - val_loss: 0.6929 - val_acc: 0.5110\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 153ms/step - loss: 0.6936 - acc: 0.5252 - val_loss: 0.6929 - val_acc: 0.5110\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6962 - acc: 0.5119 - val_loss: 0.6929 - val_acc: 0.5110\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.6954 - acc: 0.5043 - val_loss: 0.6928 - val_acc: 0.5110\n",
      "Epoch 39/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.6970 - acc: 0.5037 - val_loss: 0.6927 - val_acc: 0.5110\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6942 - acc: 0.5162 - val_loss: 0.6927 - val_acc: 0.5110\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 0.6984 - acc: 0.4976 - val_loss: 0.6926 - val_acc: 0.5110\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6957 - acc: 0.5065 - val_loss: 0.6926 - val_acc: 0.5110\n",
      "Epoch 43/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.6954 - acc: 0.5099 - val_loss: 0.6926 - val_acc: 0.5110\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.6952 - acc: 0.5075 - val_loss: 0.6925 - val_acc: 0.5110\n",
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6952 - acc: 0.5078 - val_loss: 0.6925 - val_acc: 0.5110\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6945 - acc: 0.5080 - val_loss: 0.6924 - val_acc: 0.5110\n",
      "Epoch 47/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6939 - acc: 0.5121 - val_loss: 0.6924 - val_acc: 0.5110\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6963 - acc: 0.4965 - val_loss: 0.6923 - val_acc: 0.5110\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6937 - acc: 0.5155 - val_loss: 0.6923 - val_acc: 0.5110\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6944 - acc: 0.5108 - val_loss: 0.6922 - val_acc: 0.5110\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6961 - acc: 0.5095 - val_loss: 0.6920 - val_acc: 0.5110\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.6928 - acc: 0.5188 - val_loss: 0.6918 - val_acc: 0.5110\n",
      "Epoch 53/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6918 - acc: 0.5291 - val_loss: 0.6915 - val_acc: 0.5110\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6910 - acc: 0.5248 - val_loss: 0.6913 - val_acc: 0.5110\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6935 - acc: 0.5181 - val_loss: 0.6911 - val_acc: 0.5110\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6925 - acc: 0.5166 - val_loss: 0.6909 - val_acc: 0.5110\n",
      "Epoch 57/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.6905 - acc: 0.5300 - val_loss: 0.6900 - val_acc: 0.5110\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6888 - acc: 0.5341 - val_loss: 0.6891 - val_acc: 0.5110\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6866 - acc: 0.5468 - val_loss: 0.6880 - val_acc: 0.5356\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6893 - acc: 0.5393 - val_loss: 0.6868 - val_acc: 0.5466\n",
      "Epoch 61/70\n",
      "2/2 [==============================] - 0s 156ms/step - loss: 0.6843 - acc: 0.5550 - val_loss: 0.6847 - val_acc: 0.5614\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6831 - acc: 0.5641 - val_loss: 0.6825 - val_acc: 0.5867\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 151ms/step - loss: 0.6804 - acc: 0.5718 - val_loss: 0.6806 - val_acc: 0.5744\n",
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.6760 - acc: 0.5895 - val_loss: 0.6766 - val_acc: 0.6061\n",
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6705 - acc: 0.6113 - val_loss: 0.6726 - val_acc: 0.6274\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6643 - acc: 0.6337 - val_loss: 0.6688 - val_acc: 0.6171\n",
      "Epoch 67/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6566 - acc: 0.6467 - val_loss: 0.6631 - val_acc: 0.6371\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6500 - acc: 0.6648 - val_loss: 0.6600 - val_acc: 0.6281\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6441 - acc: 0.6721 - val_loss: 0.6507 - val_acc: 0.6429\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6310 - acc: 0.6956 - val_loss: 0.6421 - val_acc: 0.6643\n",
      "sample weight :  [3.15329211e-04 1.91963838e-05 2.37641583e-05 ... 1.54240206e-05\n",
      " 1.49541413e-04 6.90226337e-05]\n",
      "1399781.0\n",
      "new_x, new_y (6182, 10, 4068) (6182,)\n",
      "WARNING:tensorflow:Layer lstm_28 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_29 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_30 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_31 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/70\n",
      "2/2 [==============================] - 6s 656ms/step - loss: 0.6976 - acc: 0.5114 - val_loss: 0.6932 - val_acc: 0.4929\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 154ms/step - loss: 0.7038 - acc: 0.4858 - val_loss: 0.6935 - val_acc: 0.4929\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.7011 - acc: 0.5015 - val_loss: 0.6950 - val_acc: 0.4929\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.7013 - acc: 0.4946 - val_loss: 0.6956 - val_acc: 0.4929\n",
      "Epoch 5/70\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 0.7010 - acc: 0.5041 - val_loss: 0.6947 - val_acc: 0.4929\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6988 - acc: 0.4987 - val_loss: 0.6941 - val_acc: 0.4929\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6993 - acc: 0.5075 - val_loss: 0.6940 - val_acc: 0.4929\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6980 - acc: 0.5011 - val_loss: 0.6942 - val_acc: 0.4929\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6998 - acc: 0.4940 - val_loss: 0.6944 - val_acc: 0.4929\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6974 - acc: 0.4948 - val_loss: 0.6943 - val_acc: 0.4929\n",
      "Epoch 11/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6936 - acc: 0.5188 - val_loss: 0.6939 - val_acc: 0.4929\n",
      "Epoch 12/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6956 - acc: 0.5123 - val_loss: 0.6938 - val_acc: 0.4929\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6982 - acc: 0.5013 - val_loss: 0.6941 - val_acc: 0.4929\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6943 - acc: 0.5216 - val_loss: 0.6942 - val_acc: 0.4929\n",
      "Epoch 15/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6970 - acc: 0.5041 - val_loss: 0.6939 - val_acc: 0.4929\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6967 - acc: 0.4953 - val_loss: 0.6938 - val_acc: 0.4929\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6934 - acc: 0.5224 - val_loss: 0.6940 - val_acc: 0.4929\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6965 - acc: 0.5017 - val_loss: 0.6943 - val_acc: 0.4929\n",
      "Epoch 19/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6948 - acc: 0.5084 - val_loss: 0.6944 - val_acc: 0.4929\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6949 - acc: 0.5075 - val_loss: 0.6944 - val_acc: 0.4929\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6959 - acc: 0.5026 - val_loss: 0.6942 - val_acc: 0.4929\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6958 - acc: 0.5039 - val_loss: 0.6939 - val_acc: 0.4929\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6953 - acc: 0.5050 - val_loss: 0.6939 - val_acc: 0.4929\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 0.6943 - acc: 0.5168 - val_loss: 0.6940 - val_acc: 0.4929\n",
      "Epoch 25/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6938 - acc: 0.5134 - val_loss: 0.6942 - val_acc: 0.4929\n",
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 0.6956 - acc: 0.5006 - val_loss: 0.6945 - val_acc: 0.4929\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6934 - acc: 0.5211 - val_loss: 0.6945 - val_acc: 0.4929\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6956 - acc: 0.5006 - val_loss: 0.6940 - val_acc: 0.4929\n",
      "Epoch 29/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6953 - acc: 0.5004 - val_loss: 0.6934 - val_acc: 0.4929\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.6948 - acc: 0.4948 - val_loss: 0.6932 - val_acc: 0.4929\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6936 - acc: 0.5082 - val_loss: 0.6934 - val_acc: 0.4929\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6935 - acc: 0.5151 - val_loss: 0.6937 - val_acc: 0.4929\n",
      "Epoch 33/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6935 - acc: 0.5132 - val_loss: 0.6941 - val_acc: 0.4929\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6933 - acc: 0.5194 - val_loss: 0.6944 - val_acc: 0.4929\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6945 - acc: 0.5069 - val_loss: 0.6944 - val_acc: 0.4929\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6927 - acc: 0.5155 - val_loss: 0.6939 - val_acc: 0.4929\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 152ms/step - loss: 0.6940 - acc: 0.5071 - val_loss: 0.6936 - val_acc: 0.4929\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 158ms/step - loss: 0.6952 - acc: 0.4903 - val_loss: 0.6935 - val_acc: 0.4929\n",
      "Epoch 39/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6937 - acc: 0.5013 - val_loss: 0.6935 - val_acc: 0.4929\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6956 - acc: 0.4942 - val_loss: 0.6935 - val_acc: 0.4929\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6932 - acc: 0.5177 - val_loss: 0.6935 - val_acc: 0.4929\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 161ms/step - loss: 0.6947 - acc: 0.4978 - val_loss: 0.6938 - val_acc: 0.4929\n",
      "Epoch 43/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6931 - acc: 0.5211 - val_loss: 0.6942 - val_acc: 0.4929\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6949 - acc: 0.5054 - val_loss: 0.6941 - val_acc: 0.4929\n",
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 158ms/step - loss: 0.6944 - acc: 0.5017 - val_loss: 0.6938 - val_acc: 0.4929\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6937 - acc: 0.5108 - val_loss: 0.6936 - val_acc: 0.4929\n",
      "Epoch 47/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6950 - acc: 0.4972 - val_loss: 0.6937 - val_acc: 0.4929\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6938 - acc: 0.5052 - val_loss: 0.6940 - val_acc: 0.4929\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6940 - acc: 0.5125 - val_loss: 0.6942 - val_acc: 0.4929\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6934 - acc: 0.5082 - val_loss: 0.6941 - val_acc: 0.4929\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6944 - acc: 0.5030 - val_loss: 0.6939 - val_acc: 0.4929\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6925 - acc: 0.5181 - val_loss: 0.6939 - val_acc: 0.4929\n",
      "Epoch 53/70\n",
      "2/2 [==============================] - 0s 160ms/step - loss: 0.6929 - acc: 0.5065 - val_loss: 0.6941 - val_acc: 0.4929\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 0s 176ms/step - loss: 0.6933 - acc: 0.5093 - val_loss: 0.6940 - val_acc: 0.4929\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 0s 159ms/step - loss: 0.6924 - acc: 0.5198 - val_loss: 0.6940 - val_acc: 0.4929\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 0s 164ms/step - loss: 0.6937 - acc: 0.5112 - val_loss: 0.6940 - val_acc: 0.4929\n",
      "Epoch 57/70\n",
      "2/2 [==============================] - 0s 170ms/step - loss: 0.6930 - acc: 0.5183 - val_loss: 0.6937 - val_acc: 0.4929\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 0s 169ms/step - loss: 0.6937 - acc: 0.5108 - val_loss: 0.6933 - val_acc: 0.4929\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 0s 169ms/step - loss: 0.6931 - acc: 0.5136 - val_loss: 0.6930 - val_acc: 0.4929\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 0s 169ms/step - loss: 0.6931 - acc: 0.5082 - val_loss: 0.6929 - val_acc: 0.4929\n",
      "Epoch 61/70\n",
      "2/2 [==============================] - 0s 170ms/step - loss: 0.6928 - acc: 0.5067 - val_loss: 0.6931 - val_acc: 0.4929\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 170ms/step - loss: 0.6926 - acc: 0.5121 - val_loss: 0.6938 - val_acc: 0.4929\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6933 - acc: 0.5054 - val_loss: 0.6947 - val_acc: 0.4929\n",
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6912 - acc: 0.5216 - val_loss: 0.6948 - val_acc: 0.4929\n",
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6929 - acc: 0.5145 - val_loss: 0.6938 - val_acc: 0.4929\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6920 - acc: 0.5265 - val_loss: 0.6931 - val_acc: 0.4929\n",
      "Epoch 67/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6923 - acc: 0.5209 - val_loss: 0.6926 - val_acc: 0.4929\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6894 - acc: 0.5397 - val_loss: 0.6925 - val_acc: 0.4929\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6913 - acc: 0.5291 - val_loss: 0.6928 - val_acc: 0.4929\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6909 - acc: 0.5229 - val_loss: 0.6920 - val_acc: 0.4929\n",
      "sample weight :  [3.05417056e-04 1.83302011e-05 2.52226977e-05 ... 1.60971778e-05\n",
      " 1.53505158e-04 7.08462594e-05]\n",
      "1393174.0\n",
      "new_x, new_y (6182, 10, 4068) (6182,)\n",
      "WARNING:tensorflow:Layer lstm_32 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_33 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_34 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_35 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/70\n",
      "2/2 [==============================] - 6s 718ms/step - loss: 0.6967 - acc: 0.5164 - val_loss: 0.6927 - val_acc: 0.5168\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6970 - acc: 0.5166 - val_loss: 0.6926 - val_acc: 0.5168\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 151ms/step - loss: 0.6965 - acc: 0.5063 - val_loss: 0.6926 - val_acc: 0.5168\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6984 - acc: 0.5013 - val_loss: 0.6926 - val_acc: 0.5168\n",
      "Epoch 5/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6977 - acc: 0.5019 - val_loss: 0.6926 - val_acc: 0.5168\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6946 - acc: 0.5181 - val_loss: 0.6930 - val_acc: 0.5168\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6946 - acc: 0.5091 - val_loss: 0.6937 - val_acc: 0.5168\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6973 - acc: 0.5209 - val_loss: 0.6942 - val_acc: 0.5168\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6961 - acc: 0.5170 - val_loss: 0.6939 - val_acc: 0.5168\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6958 - acc: 0.5095 - val_loss: 0.6933 - val_acc: 0.5168\n",
      "Epoch 11/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6976 - acc: 0.5069 - val_loss: 0.6929 - val_acc: 0.5168\n",
      "Epoch 12/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6947 - acc: 0.5175 - val_loss: 0.6927 - val_acc: 0.5168\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6953 - acc: 0.5142 - val_loss: 0.6927 - val_acc: 0.5168\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.6940 - acc: 0.5108 - val_loss: 0.6928 - val_acc: 0.5168\n",
      "Epoch 15/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6943 - acc: 0.5121 - val_loss: 0.6930 - val_acc: 0.5168\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6944 - acc: 0.5132 - val_loss: 0.6931 - val_acc: 0.5168\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6937 - acc: 0.5192 - val_loss: 0.6929 - val_acc: 0.5168\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6926 - acc: 0.5276 - val_loss: 0.6928 - val_acc: 0.5168\n",
      "Epoch 19/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6939 - acc: 0.5203 - val_loss: 0.6928 - val_acc: 0.5168\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6923 - acc: 0.5205 - val_loss: 0.6929 - val_acc: 0.5168\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6929 - acc: 0.5196 - val_loss: 0.6931 - val_acc: 0.5168\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6959 - acc: 0.5147 - val_loss: 0.6934 - val_acc: 0.5168\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6947 - acc: 0.5214 - val_loss: 0.6932 - val_acc: 0.5168\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 152ms/step - loss: 0.6927 - acc: 0.5267 - val_loss: 0.6929 - val_acc: 0.5168\n",
      "Epoch 25/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6945 - acc: 0.5175 - val_loss: 0.6928 - val_acc: 0.5168\n",
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6933 - acc: 0.5157 - val_loss: 0.6927 - val_acc: 0.5168\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6935 - acc: 0.5196 - val_loss: 0.6926 - val_acc: 0.5168\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6954 - acc: 0.5047 - val_loss: 0.6926 - val_acc: 0.5168\n",
      "Epoch 29/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6926 - acc: 0.5151 - val_loss: 0.6926 - val_acc: 0.5168\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6934 - acc: 0.5069 - val_loss: 0.6926 - val_acc: 0.5168\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6928 - acc: 0.5272 - val_loss: 0.6927 - val_acc: 0.5168\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6938 - acc: 0.5145 - val_loss: 0.6928 - val_acc: 0.5168\n",
      "Epoch 33/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6934 - acc: 0.5237 - val_loss: 0.6930 - val_acc: 0.5168\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6928 - acc: 0.5196 - val_loss: 0.6929 - val_acc: 0.5168\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6940 - acc: 0.5183 - val_loss: 0.6927 - val_acc: 0.5168\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6935 - acc: 0.5177 - val_loss: 0.6927 - val_acc: 0.5168\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 197ms/step - loss: 0.6941 - acc: 0.5067 - val_loss: 0.6927 - val_acc: 0.5168\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6916 - acc: 0.5267 - val_loss: 0.6927 - val_acc: 0.5168\n",
      "Epoch 39/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6933 - acc: 0.5194 - val_loss: 0.6929 - val_acc: 0.5168\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6923 - acc: 0.5272 - val_loss: 0.6931 - val_acc: 0.5168\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6931 - acc: 0.5283 - val_loss: 0.6931 - val_acc: 0.5168\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6926 - acc: 0.5231 - val_loss: 0.6931 - val_acc: 0.5168\n",
      "Epoch 43/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6923 - acc: 0.5220 - val_loss: 0.6929 - val_acc: 0.5168\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6929 - acc: 0.5222 - val_loss: 0.6927 - val_acc: 0.5168\n",
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6924 - acc: 0.5274 - val_loss: 0.6926 - val_acc: 0.5168\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6925 - acc: 0.5263 - val_loss: 0.6926 - val_acc: 0.5168\n",
      "Epoch 47/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6918 - acc: 0.5229 - val_loss: 0.6926 - val_acc: 0.5168\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6924 - acc: 0.5248 - val_loss: 0.6926 - val_acc: 0.5168\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6924 - acc: 0.5226 - val_loss: 0.6925 - val_acc: 0.5168\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6925 - acc: 0.5214 - val_loss: 0.6923 - val_acc: 0.5168\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6916 - acc: 0.5267 - val_loss: 0.6922 - val_acc: 0.5168\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6925 - acc: 0.5186 - val_loss: 0.6921 - val_acc: 0.5168\n",
      "Epoch 53/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6919 - acc: 0.5205 - val_loss: 0.6921 - val_acc: 0.5168\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6917 - acc: 0.5224 - val_loss: 0.6920 - val_acc: 0.5168\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6917 - acc: 0.5246 - val_loss: 0.6919 - val_acc: 0.5168\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6902 - acc: 0.5352 - val_loss: 0.6917 - val_acc: 0.5168\n",
      "Epoch 57/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6907 - acc: 0.5252 - val_loss: 0.6913 - val_acc: 0.5168\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6897 - acc: 0.5321 - val_loss: 0.6907 - val_acc: 0.5168\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6886 - acc: 0.5410 - val_loss: 0.6901 - val_acc: 0.5168\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6884 - acc: 0.5451 - val_loss: 0.6892 - val_acc: 0.5168\n",
      "Epoch 61/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6858 - acc: 0.5608 - val_loss: 0.6882 - val_acc: 0.5414\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6844 - acc: 0.5662 - val_loss: 0.6868 - val_acc: 0.5686\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6822 - acc: 0.5826 - val_loss: 0.6849 - val_acc: 0.5705\n",
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6771 - acc: 0.6020 - val_loss: 0.6826 - val_acc: 0.5737\n",
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6730 - acc: 0.6277 - val_loss: 0.6799 - val_acc: 0.5951\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6654 - acc: 0.6471 - val_loss: 0.6774 - val_acc: 0.5886\n",
      "Epoch 67/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6615 - acc: 0.6424 - val_loss: 0.6730 - val_acc: 0.6074\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6490 - acc: 0.6805 - val_loss: 0.6673 - val_acc: 0.6151\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6380 - acc: 0.6926 - val_loss: 0.6602 - val_acc: 0.6203\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6206 - acc: 0.7110 - val_loss: 0.6549 - val_acc: 0.6345\n",
      "sample weight :  [4.30561396e-04 1.70401549e-05 1.91877007e-05 ... 2.15938683e-05\n",
      " 2.06868258e-04 6.32390784e-05]\n",
      "1324282.0\n",
      "new_x, new_y (6182, 10, 4068) (6182,)\n",
      "WARNING:tensorflow:Layer lstm_36 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_37 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_38 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_39 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/70\n",
      "2/2 [==============================] - 6s 549ms/step - loss: 0.6988 - acc: 0.5024 - val_loss: 0.6933 - val_acc: 0.4586\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 156ms/step - loss: 0.6988 - acc: 0.4989 - val_loss: 0.6968 - val_acc: 0.4586\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6960 - acc: 0.5138 - val_loss: 0.6993 - val_acc: 0.4586\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6989 - acc: 0.4965 - val_loss: 0.6976 - val_acc: 0.4586\n",
      "Epoch 5/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6962 - acc: 0.5155 - val_loss: 0.6950 - val_acc: 0.4586\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6955 - acc: 0.5129 - val_loss: 0.6929 - val_acc: 0.5414\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6951 - acc: 0.5140 - val_loss: 0.6917 - val_acc: 0.5414\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6983 - acc: 0.4907 - val_loss: 0.6916 - val_acc: 0.5414\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6953 - acc: 0.5073 - val_loss: 0.6928 - val_acc: 0.5414\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6972 - acc: 0.4942 - val_loss: 0.6944 - val_acc: 0.4586\n",
      "Epoch 11/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6971 - acc: 0.4940 - val_loss: 0.6955 - val_acc: 0.4586\n",
      "Epoch 12/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6973 - acc: 0.4983 - val_loss: 0.6962 - val_acc: 0.4586\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 0.6966 - acc: 0.5019 - val_loss: 0.6961 - val_acc: 0.4586\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6960 - acc: 0.5065 - val_loss: 0.6950 - val_acc: 0.4586\n",
      "Epoch 15/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6974 - acc: 0.5024 - val_loss: 0.6937 - val_acc: 0.4586\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6980 - acc: 0.4881 - val_loss: 0.6927 - val_acc: 0.5414\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 152ms/step - loss: 0.6934 - acc: 0.5145 - val_loss: 0.6924 - val_acc: 0.5414\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6961 - acc: 0.5011 - val_loss: 0.6926 - val_acc: 0.5414\n",
      "Epoch 19/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6953 - acc: 0.4978 - val_loss: 0.6933 - val_acc: 0.4586\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6949 - acc: 0.5063 - val_loss: 0.6940 - val_acc: 0.4586\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6953 - acc: 0.5052 - val_loss: 0.6945 - val_acc: 0.4586\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6957 - acc: 0.4989 - val_loss: 0.6946 - val_acc: 0.4586\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6949 - acc: 0.5024 - val_loss: 0.6942 - val_acc: 0.4586\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6956 - acc: 0.4983 - val_loss: 0.6934 - val_acc: 0.4586\n",
      "Epoch 25/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6942 - acc: 0.5052 - val_loss: 0.6931 - val_acc: 0.5414\n",
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6927 - acc: 0.5112 - val_loss: 0.6931 - val_acc: 0.5388\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 0.6957 - acc: 0.5011 - val_loss: 0.6933 - val_acc: 0.4586\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6955 - acc: 0.4937 - val_loss: 0.6940 - val_acc: 0.4586\n",
      "Epoch 29/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6941 - acc: 0.4998 - val_loss: 0.6950 - val_acc: 0.4586\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6953 - acc: 0.5024 - val_loss: 0.6952 - val_acc: 0.4586\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6944 - acc: 0.5101 - val_loss: 0.6944 - val_acc: 0.4586\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6947 - acc: 0.5088 - val_loss: 0.6935 - val_acc: 0.4586\n",
      "Epoch 33/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6949 - acc: 0.5022 - val_loss: 0.6930 - val_acc: 0.5414\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6945 - acc: 0.4946 - val_loss: 0.6925 - val_acc: 0.5414\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6939 - acc: 0.5041 - val_loss: 0.6921 - val_acc: 0.5414\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6941 - acc: 0.5006 - val_loss: 0.6925 - val_acc: 0.5414\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6941 - acc: 0.5013 - val_loss: 0.6940 - val_acc: 0.4586\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.6945 - acc: 0.5011 - val_loss: 0.6962 - val_acc: 0.4586\n",
      "Epoch 39/70\n",
      "2/2 [==============================] - 0s 153ms/step - loss: 0.6944 - acc: 0.5054 - val_loss: 0.6981 - val_acc: 0.4586\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6953 - acc: 0.5086 - val_loss: 0.6993 - val_acc: 0.4586\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6946 - acc: 0.5069 - val_loss: 0.6992 - val_acc: 0.4586\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6949 - acc: 0.5024 - val_loss: 0.6975 - val_acc: 0.4586\n",
      "Epoch 43/70\n",
      "2/2 [==============================] - 0s 152ms/step - loss: 0.6934 - acc: 0.5132 - val_loss: 0.6956 - val_acc: 0.4586\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6950 - acc: 0.4901 - val_loss: 0.6940 - val_acc: 0.4586\n",
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6944 - acc: 0.4974 - val_loss: 0.6928 - val_acc: 0.5414\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6945 - acc: 0.5000 - val_loss: 0.6926 - val_acc: 0.5414\n",
      "Epoch 47/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6932 - acc: 0.5060 - val_loss: 0.6930 - val_acc: 0.5414\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 153ms/step - loss: 0.6943 - acc: 0.4927 - val_loss: 0.6935 - val_acc: 0.4586\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6946 - acc: 0.4974 - val_loss: 0.6943 - val_acc: 0.4586\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6934 - acc: 0.5041 - val_loss: 0.6952 - val_acc: 0.4586\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6942 - acc: 0.5032 - val_loss: 0.6956 - val_acc: 0.4586\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6941 - acc: 0.5035 - val_loss: 0.6952 - val_acc: 0.4586\n",
      "Epoch 53/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6944 - acc: 0.4968 - val_loss: 0.6943 - val_acc: 0.4586\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6939 - acc: 0.5022 - val_loss: 0.6937 - val_acc: 0.4586\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6937 - acc: 0.5039 - val_loss: 0.6935 - val_acc: 0.4586\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6942 - acc: 0.4931 - val_loss: 0.6932 - val_acc: 0.4605\n",
      "Epoch 57/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6929 - acc: 0.5050 - val_loss: 0.6929 - val_acc: 0.5414\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6941 - acc: 0.4981 - val_loss: 0.6927 - val_acc: 0.5414\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.6934 - acc: 0.5095 - val_loss: 0.6926 - val_acc: 0.5414\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6937 - acc: 0.4953 - val_loss: 0.6927 - val_acc: 0.5414\n",
      "Epoch 61/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6927 - acc: 0.5050 - val_loss: 0.6930 - val_acc: 0.5317\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.6935 - acc: 0.5041 - val_loss: 0.6935 - val_acc: 0.4586\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6941 - acc: 0.5050 - val_loss: 0.6943 - val_acc: 0.4586\n",
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6934 - acc: 0.5006 - val_loss: 0.6955 - val_acc: 0.4586\n",
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6937 - acc: 0.5052 - val_loss: 0.6967 - val_acc: 0.4586\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6947 - acc: 0.4959 - val_loss: 0.6973 - val_acc: 0.4586\n",
      "Epoch 67/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6941 - acc: 0.5006 - val_loss: 0.6967 - val_acc: 0.4586\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6937 - acc: 0.5110 - val_loss: 0.6955 - val_acc: 0.4586\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6938 - acc: 0.5052 - val_loss: 0.6944 - val_acc: 0.4586\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.6924 - acc: 0.5136 - val_loss: 0.6939 - val_acc: 0.4586\n",
      "sample weight :  [4.23521401e-04 1.67853807e-05 1.95906984e-05 ... 2.18498313e-05\n",
      " 2.10052604e-04 6.39036257e-05]\n",
      "1328446.0\n",
      "new_x, new_y (6182, 10, 4068) (6182,)\n",
      "WARNING:tensorflow:Layer lstm_40 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_41 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_42 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_43 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/70\n",
      "2/2 [==============================] - 6s 555ms/step - loss: 0.7121 - acc: 0.4944 - val_loss: 0.6935 - val_acc: 0.5207\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.7050 - acc: 0.4965 - val_loss: 0.6928 - val_acc: 0.5207\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6980 - acc: 0.5093 - val_loss: 0.6970 - val_acc: 0.4793\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.7004 - acc: 0.5052 - val_loss: 0.7002 - val_acc: 0.4793\n",
      "Epoch 5/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.7023 - acc: 0.5019 - val_loss: 0.6997 - val_acc: 0.4793\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.7019 - acc: 0.5063 - val_loss: 0.6971 - val_acc: 0.4793\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.7010 - acc: 0.4968 - val_loss: 0.6948 - val_acc: 0.4793\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6980 - acc: 0.5037 - val_loss: 0.6932 - val_acc: 0.4793\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6994 - acc: 0.5006 - val_loss: 0.6925 - val_acc: 0.5207\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6980 - acc: 0.5091 - val_loss: 0.6923 - val_acc: 0.5207\n",
      "Epoch 11/70\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 0.6988 - acc: 0.4963 - val_loss: 0.6923 - val_acc: 0.5207\n",
      "Epoch 12/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6975 - acc: 0.5086 - val_loss: 0.6923 - val_acc: 0.5207\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.7009 - acc: 0.4866 - val_loss: 0.6923 - val_acc: 0.5207\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.7000 - acc: 0.4922 - val_loss: 0.6925 - val_acc: 0.5207\n",
      "Epoch 15/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6987 - acc: 0.5063 - val_loss: 0.6929 - val_acc: 0.5207\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6987 - acc: 0.4963 - val_loss: 0.6936 - val_acc: 0.4793\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6975 - acc: 0.5024 - val_loss: 0.6943 - val_acc: 0.4793\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6970 - acc: 0.5045 - val_loss: 0.6948 - val_acc: 0.4793\n",
      "Epoch 19/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6963 - acc: 0.4961 - val_loss: 0.6946 - val_acc: 0.4793\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6981 - acc: 0.4907 - val_loss: 0.6940 - val_acc: 0.4793\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6969 - acc: 0.4933 - val_loss: 0.6934 - val_acc: 0.4793\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6974 - acc: 0.4987 - val_loss: 0.6930 - val_acc: 0.5207\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6946 - acc: 0.5022 - val_loss: 0.6927 - val_acc: 0.5207\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6956 - acc: 0.5026 - val_loss: 0.6926 - val_acc: 0.5207\n",
      "Epoch 25/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6954 - acc: 0.5000 - val_loss: 0.6926 - val_acc: 0.5207\n",
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6972 - acc: 0.4935 - val_loss: 0.6927 - val_acc: 0.5207\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6965 - acc: 0.5009 - val_loss: 0.6928 - val_acc: 0.5207\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6956 - acc: 0.5097 - val_loss: 0.6929 - val_acc: 0.5207\n",
      "Epoch 29/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6955 - acc: 0.5082 - val_loss: 0.6928 - val_acc: 0.5207\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6962 - acc: 0.4955 - val_loss: 0.6929 - val_acc: 0.5207\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6959 - acc: 0.4981 - val_loss: 0.6931 - val_acc: 0.5207\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6947 - acc: 0.4996 - val_loss: 0.6933 - val_acc: 0.4793\n",
      "Epoch 33/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6950 - acc: 0.5035 - val_loss: 0.6935 - val_acc: 0.4793\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6979 - acc: 0.4825 - val_loss: 0.6938 - val_acc: 0.4793\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6959 - acc: 0.5000 - val_loss: 0.6942 - val_acc: 0.4793\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6963 - acc: 0.4905 - val_loss: 0.6945 - val_acc: 0.4793\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6947 - acc: 0.5030 - val_loss: 0.6944 - val_acc: 0.4793\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6941 - acc: 0.5086 - val_loss: 0.6943 - val_acc: 0.4793\n",
      "Epoch 39/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6964 - acc: 0.4970 - val_loss: 0.6941 - val_acc: 0.4793\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6959 - acc: 0.4989 - val_loss: 0.6938 - val_acc: 0.4793\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6951 - acc: 0.5082 - val_loss: 0.6934 - val_acc: 0.4793\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6953 - acc: 0.5000 - val_loss: 0.6931 - val_acc: 0.5149\n",
      "Epoch 43/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6956 - acc: 0.4950 - val_loss: 0.6929 - val_acc: 0.5207\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6942 - acc: 0.5041 - val_loss: 0.6929 - val_acc: 0.5207\n",
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6951 - acc: 0.4946 - val_loss: 0.6929 - val_acc: 0.5207\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 0.6940 - acc: 0.5104 - val_loss: 0.6932 - val_acc: 0.4793\n",
      "Epoch 47/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.6944 - acc: 0.5030 - val_loss: 0.6936 - val_acc: 0.4793\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6942 - acc: 0.4991 - val_loss: 0.6942 - val_acc: 0.4793\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6933 - acc: 0.5157 - val_loss: 0.6944 - val_acc: 0.4793\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6942 - acc: 0.5082 - val_loss: 0.6943 - val_acc: 0.4793\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6942 - acc: 0.4985 - val_loss: 0.6940 - val_acc: 0.4793\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6920 - acc: 0.5183 - val_loss: 0.6936 - val_acc: 0.4793\n",
      "Epoch 53/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6939 - acc: 0.5050 - val_loss: 0.6933 - val_acc: 0.4793\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 0.6933 - acc: 0.5108 - val_loss: 0.6930 - val_acc: 0.5569\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6936 - acc: 0.5030 - val_loss: 0.6928 - val_acc: 0.5207\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 0.6933 - acc: 0.5028 - val_loss: 0.6926 - val_acc: 0.5207\n",
      "Epoch 57/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6935 - acc: 0.5035 - val_loss: 0.6926 - val_acc: 0.5207\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6943 - acc: 0.5028 - val_loss: 0.6928 - val_acc: 0.5207\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6925 - acc: 0.5186 - val_loss: 0.6931 - val_acc: 0.4890\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6954 - acc: 0.4998 - val_loss: 0.6935 - val_acc: 0.4793\n",
      "Epoch 61/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6938 - acc: 0.5101 - val_loss: 0.6938 - val_acc: 0.4793\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6946 - acc: 0.5058 - val_loss: 0.6939 - val_acc: 0.4793\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 153ms/step - loss: 0.6946 - acc: 0.4931 - val_loss: 0.6937 - val_acc: 0.4793\n",
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.6932 - acc: 0.5075 - val_loss: 0.6934 - val_acc: 0.4793\n",
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6931 - acc: 0.5112 - val_loss: 0.6928 - val_acc: 0.5563\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6934 - acc: 0.5069 - val_loss: 0.6925 - val_acc: 0.5207\n",
      "Epoch 67/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6924 - acc: 0.5063 - val_loss: 0.6923 - val_acc: 0.5207\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.6944 - acc: 0.5028 - val_loss: 0.6923 - val_acc: 0.5207\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6927 - acc: 0.5162 - val_loss: 0.6923 - val_acc: 0.5207\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6939 - acc: 0.5017 - val_loss: 0.6923 - val_acc: 0.5246\n",
      "sample weight :  [4.26270743e-04 1.68975719e-05 1.95613765e-05 ... 2.16735237e-05\n",
      " 2.09191809e-04 6.31634696e-05]\n",
      "1346135.0\n",
      "new_x, new_y (6182, 10, 4068) (6182,)\n",
      "WARNING:tensorflow:Layer lstm_44 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_45 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_46 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_47 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/70\n",
      "2/2 [==============================] - 6s 2s/step - loss: 0.7212 - acc: 0.4858 - val_loss: 0.6996 - val_acc: 0.4812\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.7048 - acc: 0.5004 - val_loss: 0.6929 - val_acc: 0.5188\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 0.7020 - acc: 0.4991 - val_loss: 0.6933 - val_acc: 0.5188\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.7011 - acc: 0.5160 - val_loss: 0.6951 - val_acc: 0.5188\n",
      "Epoch 5/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.7003 - acc: 0.5123 - val_loss: 0.6949 - val_acc: 0.5188\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.7041 - acc: 0.5095 - val_loss: 0.6934 - val_acc: 0.5188\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6980 - acc: 0.5116 - val_loss: 0.6925 - val_acc: 0.5188\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.6986 - acc: 0.5121 - val_loss: 0.6925 - val_acc: 0.5188\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6944 - acc: 0.5220 - val_loss: 0.6927 - val_acc: 0.5188\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6989 - acc: 0.5054 - val_loss: 0.6928 - val_acc: 0.5188\n",
      "Epoch 11/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6981 - acc: 0.5054 - val_loss: 0.6927 - val_acc: 0.5188\n",
      "Epoch 12/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6996 - acc: 0.5108 - val_loss: 0.6925 - val_acc: 0.5188\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6995 - acc: 0.5022 - val_loss: 0.6924 - val_acc: 0.5188\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6986 - acc: 0.5069 - val_loss: 0.6925 - val_acc: 0.5188\n",
      "Epoch 15/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6971 - acc: 0.5214 - val_loss: 0.6927 - val_acc: 0.5188\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.7000 - acc: 0.4994 - val_loss: 0.6929 - val_acc: 0.5188\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6969 - acc: 0.5173 - val_loss: 0.6928 - val_acc: 0.5188\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6928 - acc: 0.5237 - val_loss: 0.6925 - val_acc: 0.5188\n",
      "Epoch 19/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6954 - acc: 0.5214 - val_loss: 0.6924 - val_acc: 0.5188\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6985 - acc: 0.4972 - val_loss: 0.6925 - val_acc: 0.5188\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6991 - acc: 0.5013 - val_loss: 0.6926 - val_acc: 0.5188\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6984 - acc: 0.4957 - val_loss: 0.6926 - val_acc: 0.5188\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6996 - acc: 0.4925 - val_loss: 0.6925 - val_acc: 0.5188\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6985 - acc: 0.5050 - val_loss: 0.6924 - val_acc: 0.5188\n",
      "Epoch 25/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6975 - acc: 0.5067 - val_loss: 0.6924 - val_acc: 0.5188\n",
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6941 - acc: 0.5194 - val_loss: 0.6925 - val_acc: 0.5188\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6970 - acc: 0.5035 - val_loss: 0.6926 - val_acc: 0.5188\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6972 - acc: 0.5183 - val_loss: 0.6926 - val_acc: 0.5188\n",
      "Epoch 29/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.6990 - acc: 0.4912 - val_loss: 0.6925 - val_acc: 0.5188\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6944 - acc: 0.5255 - val_loss: 0.6924 - val_acc: 0.5188\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6952 - acc: 0.5205 - val_loss: 0.6924 - val_acc: 0.5188\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6976 - acc: 0.4989 - val_loss: 0.6925 - val_acc: 0.5188\n",
      "Epoch 33/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6990 - acc: 0.4927 - val_loss: 0.6925 - val_acc: 0.5188\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6964 - acc: 0.5041 - val_loss: 0.6924 - val_acc: 0.5188\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6966 - acc: 0.5024 - val_loss: 0.6924 - val_acc: 0.5188\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6933 - acc: 0.5192 - val_loss: 0.6924 - val_acc: 0.5188\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6970 - acc: 0.5056 - val_loss: 0.6926 - val_acc: 0.5188\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6960 - acc: 0.5125 - val_loss: 0.6928 - val_acc: 0.5188\n",
      "Epoch 39/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6962 - acc: 0.5134 - val_loss: 0.6927 - val_acc: 0.5188\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6946 - acc: 0.5145 - val_loss: 0.6925 - val_acc: 0.5188\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6952 - acc: 0.5138 - val_loss: 0.6924 - val_acc: 0.5188\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6951 - acc: 0.5132 - val_loss: 0.6923 - val_acc: 0.5188\n",
      "Epoch 43/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6937 - acc: 0.5155 - val_loss: 0.6924 - val_acc: 0.5188\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6954 - acc: 0.5063 - val_loss: 0.6924 - val_acc: 0.5188\n",
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6951 - acc: 0.5009 - val_loss: 0.6924 - val_acc: 0.5188\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6954 - acc: 0.5129 - val_loss: 0.6923 - val_acc: 0.5188\n",
      "Epoch 47/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6946 - acc: 0.4961 - val_loss: 0.6923 - val_acc: 0.5188\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6953 - acc: 0.5095 - val_loss: 0.6923 - val_acc: 0.5188\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 333ms/step - loss: 0.6942 - acc: 0.5151 - val_loss: 0.6925 - val_acc: 0.5188\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6948 - acc: 0.5026 - val_loss: 0.6925 - val_acc: 0.5188\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6969 - acc: 0.5015 - val_loss: 0.6923 - val_acc: 0.5188\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6955 - acc: 0.5071 - val_loss: 0.6922 - val_acc: 0.5188\n",
      "Epoch 53/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6921 - acc: 0.5192 - val_loss: 0.6922 - val_acc: 0.5188\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6939 - acc: 0.5104 - val_loss: 0.6922 - val_acc: 0.5188\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6941 - acc: 0.5095 - val_loss: 0.6921 - val_acc: 0.5188\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6923 - acc: 0.5168 - val_loss: 0.6920 - val_acc: 0.5188\n",
      "Epoch 57/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6938 - acc: 0.5114 - val_loss: 0.6920 - val_acc: 0.5188\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6937 - acc: 0.5125 - val_loss: 0.6919 - val_acc: 0.5188\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6924 - acc: 0.5209 - val_loss: 0.6918 - val_acc: 0.5188\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6925 - acc: 0.5205 - val_loss: 0.6917 - val_acc: 0.5188\n",
      "Epoch 61/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6924 - acc: 0.5188 - val_loss: 0.6916 - val_acc: 0.5188\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 0.6933 - acc: 0.5155 - val_loss: 0.6914 - val_acc: 0.5188\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6923 - acc: 0.5080 - val_loss: 0.6913 - val_acc: 0.5188\n",
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6909 - acc: 0.5211 - val_loss: 0.6912 - val_acc: 0.5188\n",
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6897 - acc: 0.5365 - val_loss: 0.6908 - val_acc: 0.5188\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6906 - acc: 0.5239 - val_loss: 0.6904 - val_acc: 0.5188\n",
      "Epoch 67/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6890 - acc: 0.5328 - val_loss: 0.6900 - val_acc: 0.5188\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6876 - acc: 0.5505 - val_loss: 0.6895 - val_acc: 0.5233\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6875 - acc: 0.5367 - val_loss: 0.6888 - val_acc: 0.5278\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6874 - acc: 0.5397 - val_loss: 0.6880 - val_acc: 0.5388\n",
      "sample weight :  [4.44227842e-04 1.85265566e-05 1.99377982e-05 ... 2.02442684e-05\n",
      " 2.09142077e-04 5.77076896e-05]\n",
      "1314081.0\n",
      "new_x, new_y (6182, 10, 4068) (6182,)\n",
      "WARNING:tensorflow:Layer lstm_48 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_49 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_50 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_51 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/70\n",
      "2/2 [==============================] - 6s 559ms/step - loss: 0.6974 - acc: 0.5032 - val_loss: 0.6938 - val_acc: 0.5019\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 163ms/step - loss: 0.6990 - acc: 0.5080 - val_loss: 0.6932 - val_acc: 0.5019\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 161ms/step - loss: 0.6955 - acc: 0.5075 - val_loss: 0.6933 - val_acc: 0.5019\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 0s 166ms/step - loss: 0.6980 - acc: 0.5052 - val_loss: 0.6944 - val_acc: 0.5019\n",
      "Epoch 5/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 163ms/step - loss: 0.6958 - acc: 0.5114 - val_loss: 0.6957 - val_acc: 0.5019\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 172ms/step - loss: 0.6952 - acc: 0.5226 - val_loss: 0.6953 - val_acc: 0.5019\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6970 - acc: 0.5037 - val_loss: 0.6942 - val_acc: 0.5019\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6944 - acc: 0.5097 - val_loss: 0.6939 - val_acc: 0.5019\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6941 - acc: 0.5214 - val_loss: 0.6939 - val_acc: 0.5019\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6960 - acc: 0.5078 - val_loss: 0.6939 - val_acc: 0.5019\n",
      "Epoch 11/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6939 - acc: 0.5194 - val_loss: 0.6942 - val_acc: 0.5019\n",
      "Epoch 12/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6948 - acc: 0.5095 - val_loss: 0.6945 - val_acc: 0.5019\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6944 - acc: 0.5112 - val_loss: 0.6944 - val_acc: 0.5019\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6931 - acc: 0.5173 - val_loss: 0.6944 - val_acc: 0.5019\n",
      "Epoch 15/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6963 - acc: 0.5153 - val_loss: 0.6947 - val_acc: 0.5019\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6957 - acc: 0.5157 - val_loss: 0.6950 - val_acc: 0.5019\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6929 - acc: 0.5306 - val_loss: 0.6955 - val_acc: 0.5019\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6962 - acc: 0.5101 - val_loss: 0.6954 - val_acc: 0.5019\n",
      "Epoch 19/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6958 - acc: 0.5175 - val_loss: 0.6950 - val_acc: 0.5019\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6944 - acc: 0.5183 - val_loss: 0.6949 - val_acc: 0.5019\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6958 - acc: 0.5168 - val_loss: 0.6946 - val_acc: 0.5019\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6922 - acc: 0.5242 - val_loss: 0.6942 - val_acc: 0.5019\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6947 - acc: 0.5119 - val_loss: 0.6940 - val_acc: 0.5019\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6941 - acc: 0.5095 - val_loss: 0.6939 - val_acc: 0.5019\n",
      "Epoch 25/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6956 - acc: 0.5082 - val_loss: 0.6943 - val_acc: 0.5019\n",
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6942 - acc: 0.5125 - val_loss: 0.6947 - val_acc: 0.5019\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.6945 - acc: 0.5127 - val_loss: 0.6950 - val_acc: 0.5019\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6940 - acc: 0.5097 - val_loss: 0.6953 - val_acc: 0.5019\n",
      "Epoch 29/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6930 - acc: 0.5181 - val_loss: 0.6952 - val_acc: 0.5019\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6916 - acc: 0.5267 - val_loss: 0.6947 - val_acc: 0.5019\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.6936 - acc: 0.5188 - val_loss: 0.6945 - val_acc: 0.5019\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6929 - acc: 0.5201 - val_loss: 0.6943 - val_acc: 0.5019\n",
      "Epoch 33/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6932 - acc: 0.5261 - val_loss: 0.6940 - val_acc: 0.5019\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6942 - acc: 0.5108 - val_loss: 0.6937 - val_acc: 0.5019\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6927 - acc: 0.5201 - val_loss: 0.6935 - val_acc: 0.5019\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6949 - acc: 0.5000 - val_loss: 0.6937 - val_acc: 0.5019\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6921 - acc: 0.5231 - val_loss: 0.6941 - val_acc: 0.5019\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6933 - acc: 0.5244 - val_loss: 0.6952 - val_acc: 0.5019\n",
      "Epoch 39/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6931 - acc: 0.5194 - val_loss: 0.6963 - val_acc: 0.5019\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6930 - acc: 0.5283 - val_loss: 0.6966 - val_acc: 0.5019\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6926 - acc: 0.5244 - val_loss: 0.6961 - val_acc: 0.5019\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6933 - acc: 0.5216 - val_loss: 0.6953 - val_acc: 0.5019\n",
      "Epoch 43/70\n",
      "2/2 [==============================] - 0s 152ms/step - loss: 0.6933 - acc: 0.5220 - val_loss: 0.6948 - val_acc: 0.5019\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6936 - acc: 0.5164 - val_loss: 0.6944 - val_acc: 0.5019\n",
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6921 - acc: 0.5149 - val_loss: 0.6942 - val_acc: 0.5019\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6929 - acc: 0.5242 - val_loss: 0.6942 - val_acc: 0.5019\n",
      "Epoch 47/70\n",
      "2/2 [==============================] - 0s 161ms/step - loss: 0.6925 - acc: 0.5179 - val_loss: 0.6942 - val_acc: 0.5019\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.6936 - acc: 0.5207 - val_loss: 0.6941 - val_acc: 0.5019\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6926 - acc: 0.5211 - val_loss: 0.6940 - val_acc: 0.5019\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6925 - acc: 0.5209 - val_loss: 0.6939 - val_acc: 0.5019\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6936 - acc: 0.5214 - val_loss: 0.6938 - val_acc: 0.5019\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.6919 - acc: 0.5220 - val_loss: 0.6938 - val_acc: 0.5019\n",
      "Epoch 53/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6916 - acc: 0.5252 - val_loss: 0.6940 - val_acc: 0.5019\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6924 - acc: 0.5209 - val_loss: 0.6943 - val_acc: 0.5019\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6921 - acc: 0.5261 - val_loss: 0.6945 - val_acc: 0.5019\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6917 - acc: 0.5276 - val_loss: 0.6948 - val_acc: 0.5019\n",
      "Epoch 57/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6915 - acc: 0.5296 - val_loss: 0.6948 - val_acc: 0.5019\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6918 - acc: 0.5278 - val_loss: 0.6942 - val_acc: 0.5019\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6914 - acc: 0.5272 - val_loss: 0.6937 - val_acc: 0.5019\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6912 - acc: 0.5207 - val_loss: 0.6937 - val_acc: 0.5019\n",
      "Epoch 61/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6914 - acc: 0.5242 - val_loss: 0.6941 - val_acc: 0.5019\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6912 - acc: 0.5270 - val_loss: 0.6945 - val_acc: 0.5019\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6907 - acc: 0.5300 - val_loss: 0.6944 - val_acc: 0.5019\n",
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6903 - acc: 0.5246 - val_loss: 0.6939 - val_acc: 0.5019\n",
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6899 - acc: 0.5306 - val_loss: 0.6936 - val_acc: 0.5019\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 152ms/step - loss: 0.6895 - acc: 0.5332 - val_loss: 0.6931 - val_acc: 0.5019\n",
      "Epoch 67/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6879 - acc: 0.5382 - val_loss: 0.6920 - val_acc: 0.5019\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 152ms/step - loss: 0.6867 - acc: 0.5429 - val_loss: 0.6908 - val_acc: 0.5019\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6851 - acc: 0.5462 - val_loss: 0.6904 - val_acc: 0.5026\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6820 - acc: 0.5574 - val_loss: 0.6882 - val_acc: 0.5330\n",
      "sample weight :  [4.34365654e-04 1.79899098e-05 2.27515481e-05 ... 2.04060033e-05\n",
      " 2.19171106e-04 5.86347197e-05]\n",
      "1336177.0\n",
      "new_x, new_y (6182, 10, 4068) (6182,)\n",
      "WARNING:tensorflow:Layer lstm_52 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_53 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_54 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_55 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/70\n",
      "2/2 [==============================] - 6s 730ms/step - loss: 0.7496 - acc: 0.4961 - val_loss: 0.7103 - val_acc: 0.5194\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.7267 - acc: 0.4944 - val_loss: 0.6978 - val_acc: 0.5194\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.7074 - acc: 0.4914 - val_loss: 0.6926 - val_acc: 0.5194\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.7005 - acc: 0.5101 - val_loss: 0.6940 - val_acc: 0.4806\n",
      "Epoch 5/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6983 - acc: 0.5035 - val_loss: 0.6992 - val_acc: 0.4806\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.7038 - acc: 0.4940 - val_loss: 0.7029 - val_acc: 0.4806\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.7020 - acc: 0.5024 - val_loss: 0.7032 - val_acc: 0.4806\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.7043 - acc: 0.4937 - val_loss: 0.7014 - val_acc: 0.4806\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 0.6999 - acc: 0.5056 - val_loss: 0.6986 - val_acc: 0.4806\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6984 - acc: 0.5037 - val_loss: 0.6963 - val_acc: 0.4806\n",
      "Epoch 11/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.7004 - acc: 0.4912 - val_loss: 0.6948 - val_acc: 0.4806\n",
      "Epoch 12/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6970 - acc: 0.4950 - val_loss: 0.6938 - val_acc: 0.4806\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.6960 - acc: 0.5052 - val_loss: 0.6932 - val_acc: 0.4806\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6995 - acc: 0.4959 - val_loss: 0.6928 - val_acc: 0.5194\n",
      "Epoch 15/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.7003 - acc: 0.4814 - val_loss: 0.6926 - val_acc: 0.5194\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6980 - acc: 0.4965 - val_loss: 0.6926 - val_acc: 0.5194\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6989 - acc: 0.4976 - val_loss: 0.6926 - val_acc: 0.5194\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6945 - acc: 0.5138 - val_loss: 0.6927 - val_acc: 0.5194\n",
      "Epoch 19/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6975 - acc: 0.5028 - val_loss: 0.6929 - val_acc: 0.5194\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6969 - acc: 0.4944 - val_loss: 0.6933 - val_acc: 0.4806\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6960 - acc: 0.5006 - val_loss: 0.6936 - val_acc: 0.4806\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6966 - acc: 0.5063 - val_loss: 0.6938 - val_acc: 0.4806\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 151ms/step - loss: 0.6959 - acc: 0.5022 - val_loss: 0.6939 - val_acc: 0.4806\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6984 - acc: 0.4963 - val_loss: 0.6939 - val_acc: 0.4806\n",
      "Epoch 25/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6953 - acc: 0.5052 - val_loss: 0.6938 - val_acc: 0.4806\n",
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6971 - acc: 0.5041 - val_loss: 0.6938 - val_acc: 0.4806\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6955 - acc: 0.5142 - val_loss: 0.6938 - val_acc: 0.4806\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6967 - acc: 0.4944 - val_loss: 0.6938 - val_acc: 0.4806\n",
      "Epoch 29/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6966 - acc: 0.5022 - val_loss: 0.6937 - val_acc: 0.4806\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6974 - acc: 0.4974 - val_loss: 0.6936 - val_acc: 0.4806\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6977 - acc: 0.4860 - val_loss: 0.6936 - val_acc: 0.4806\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6942 - acc: 0.5168 - val_loss: 0.6936 - val_acc: 0.4806\n",
      "Epoch 33/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6973 - acc: 0.4972 - val_loss: 0.6935 - val_acc: 0.4806\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6970 - acc: 0.5015 - val_loss: 0.6934 - val_acc: 0.4806\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6951 - acc: 0.5006 - val_loss: 0.6934 - val_acc: 0.4806\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6956 - acc: 0.5052 - val_loss: 0.6935 - val_acc: 0.4806\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6976 - acc: 0.4996 - val_loss: 0.6937 - val_acc: 0.4806\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6951 - acc: 0.5041 - val_loss: 0.6939 - val_acc: 0.4806\n",
      "Epoch 39/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6952 - acc: 0.5073 - val_loss: 0.6939 - val_acc: 0.4806\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6949 - acc: 0.5082 - val_loss: 0.6940 - val_acc: 0.4806\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6968 - acc: 0.4860 - val_loss: 0.6940 - val_acc: 0.4806\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6957 - acc: 0.4972 - val_loss: 0.6941 - val_acc: 0.4806\n",
      "Epoch 43/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6962 - acc: 0.4959 - val_loss: 0.6942 - val_acc: 0.4806\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6949 - acc: 0.5047 - val_loss: 0.6944 - val_acc: 0.4806\n",
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6948 - acc: 0.4989 - val_loss: 0.6946 - val_acc: 0.4806\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6953 - acc: 0.4978 - val_loss: 0.6945 - val_acc: 0.4806\n",
      "Epoch 47/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6943 - acc: 0.5088 - val_loss: 0.6942 - val_acc: 0.4806\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6936 - acc: 0.5091 - val_loss: 0.6938 - val_acc: 0.4806\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6957 - acc: 0.4976 - val_loss: 0.6936 - val_acc: 0.4806\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6955 - acc: 0.4963 - val_loss: 0.6934 - val_acc: 0.4806\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6941 - acc: 0.5078 - val_loss: 0.6933 - val_acc: 0.4806\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6947 - acc: 0.4959 - val_loss: 0.6932 - val_acc: 0.4806\n",
      "Epoch 53/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6942 - acc: 0.5058 - val_loss: 0.6931 - val_acc: 0.4845\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6950 - acc: 0.5000 - val_loss: 0.6930 - val_acc: 0.5246\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6969 - acc: 0.4791 - val_loss: 0.6931 - val_acc: 0.4922\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6945 - acc: 0.5011 - val_loss: 0.6932 - val_acc: 0.4806\n",
      "Epoch 57/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6950 - acc: 0.4972 - val_loss: 0.6935 - val_acc: 0.4806\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6944 - acc: 0.4989 - val_loss: 0.6938 - val_acc: 0.4806\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6948 - acc: 0.5017 - val_loss: 0.6943 - val_acc: 0.4806\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6939 - acc: 0.5104 - val_loss: 0.6944 - val_acc: 0.4806\n",
      "Epoch 61/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6965 - acc: 0.4940 - val_loss: 0.6943 - val_acc: 0.4806\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6950 - acc: 0.4981 - val_loss: 0.6939 - val_acc: 0.4806\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6956 - acc: 0.4931 - val_loss: 0.6935 - val_acc: 0.4806\n",
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6942 - acc: 0.5106 - val_loss: 0.6932 - val_acc: 0.4806\n",
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6950 - acc: 0.4944 - val_loss: 0.6930 - val_acc: 0.5052\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6935 - acc: 0.5069 - val_loss: 0.6929 - val_acc: 0.5330\n",
      "Epoch 67/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6941 - acc: 0.5112 - val_loss: 0.6928 - val_acc: 0.5556\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6955 - acc: 0.4886 - val_loss: 0.6927 - val_acc: 0.5440\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 152ms/step - loss: 0.6934 - acc: 0.5060 - val_loss: 0.6928 - val_acc: 0.5304\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6935 - acc: 0.5071 - val_loss: 0.6928 - val_acc: 0.5213\n",
      "sample weight :  [4.35707184e-04 1.80918595e-05 2.26648417e-05 ... 2.02642354e-05\n",
      " 2.18062509e-04 5.81517198e-05]\n",
      "1345210.0\n",
      "new_x, new_y (6182, 10, 4068) (6182,)\n",
      "WARNING:tensorflow:Layer lstm_56 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_57 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_58 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_59 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/70\n",
      "2/2 [==============================] - 6s 558ms/step - loss: 0.8097 - acc: 0.5136 - val_loss: 0.7642 - val_acc: 0.5175\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.7710 - acc: 0.5134 - val_loss: 0.7336 - val_acc: 0.5175\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.7403 - acc: 0.5140 - val_loss: 0.7115 - val_acc: 0.5175\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.7190 - acc: 0.5129 - val_loss: 0.6978 - val_acc: 0.5175\n",
      "Epoch 5/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.7051 - acc: 0.5026 - val_loss: 0.6926 - val_acc: 0.5175\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6996 - acc: 0.5052 - val_loss: 0.6950 - val_acc: 0.4825\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.7054 - acc: 0.4909 - val_loss: 0.6991 - val_acc: 0.4825\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.7040 - acc: 0.4981 - val_loss: 0.7000 - val_acc: 0.4825\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.7058 - acc: 0.4955 - val_loss: 0.6983 - val_acc: 0.4825\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.7023 - acc: 0.4942 - val_loss: 0.6956 - val_acc: 0.4825\n",
      "Epoch 11/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.7008 - acc: 0.4961 - val_loss: 0.6937 - val_acc: 0.4825\n",
      "Epoch 12/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.7011 - acc: 0.4922 - val_loss: 0.6927 - val_acc: 0.5175\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6961 - acc: 0.5110 - val_loss: 0.6925 - val_acc: 0.5175\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.7000 - acc: 0.4998 - val_loss: 0.6927 - val_acc: 0.5175\n",
      "Epoch 15/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.6967 - acc: 0.5114 - val_loss: 0.6930 - val_acc: 0.5175\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.6986 - acc: 0.5052 - val_loss: 0.6931 - val_acc: 0.5175\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.7000 - acc: 0.5071 - val_loss: 0.6931 - val_acc: 0.5175\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6993 - acc: 0.5078 - val_loss: 0.6929 - val_acc: 0.5175\n",
      "Epoch 19/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6988 - acc: 0.4959 - val_loss: 0.6928 - val_acc: 0.5175\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6975 - acc: 0.5054 - val_loss: 0.6926 - val_acc: 0.5175\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6980 - acc: 0.5050 - val_loss: 0.6925 - val_acc: 0.5175\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6965 - acc: 0.5050 - val_loss: 0.6925 - val_acc: 0.5175\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6959 - acc: 0.5032 - val_loss: 0.6926 - val_acc: 0.5175\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6967 - acc: 0.5075 - val_loss: 0.6927 - val_acc: 0.5175\n",
      "Epoch 25/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6969 - acc: 0.5058 - val_loss: 0.6928 - val_acc: 0.5175\n",
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6977 - acc: 0.5054 - val_loss: 0.6929 - val_acc: 0.5175\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6953 - acc: 0.5088 - val_loss: 0.6929 - val_acc: 0.5175\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6996 - acc: 0.4873 - val_loss: 0.6928 - val_acc: 0.5175\n",
      "Epoch 29/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6964 - acc: 0.5015 - val_loss: 0.6926 - val_acc: 0.5175\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6983 - acc: 0.5017 - val_loss: 0.6925 - val_acc: 0.5175\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.6955 - acc: 0.5082 - val_loss: 0.6925 - val_acc: 0.5175\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6968 - acc: 0.4978 - val_loss: 0.6925 - val_acc: 0.5175\n",
      "Epoch 33/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6951 - acc: 0.5086 - val_loss: 0.6925 - val_acc: 0.5175\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6967 - acc: 0.4968 - val_loss: 0.6925 - val_acc: 0.5175\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6991 - acc: 0.4927 - val_loss: 0.6925 - val_acc: 0.5175\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6955 - acc: 0.5045 - val_loss: 0.6925 - val_acc: 0.5175\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6964 - acc: 0.4953 - val_loss: 0.6925 - val_acc: 0.5175\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6945 - acc: 0.5080 - val_loss: 0.6925 - val_acc: 0.5175\n",
      "Epoch 39/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6973 - acc: 0.4983 - val_loss: 0.6925 - val_acc: 0.5175\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6964 - acc: 0.4948 - val_loss: 0.6925 - val_acc: 0.5175\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6962 - acc: 0.4961 - val_loss: 0.6925 - val_acc: 0.5175\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6952 - acc: 0.5065 - val_loss: 0.6926 - val_acc: 0.5175\n",
      "Epoch 43/70\n",
      "2/2 [==============================] - 0s 153ms/step - loss: 0.6951 - acc: 0.5088 - val_loss: 0.6926 - val_acc: 0.5175\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.6970 - acc: 0.4922 - val_loss: 0.6926 - val_acc: 0.5175\n",
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6958 - acc: 0.5116 - val_loss: 0.6926 - val_acc: 0.5175\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6947 - acc: 0.5127 - val_loss: 0.6925 - val_acc: 0.5175\n",
      "Epoch 47/70\n",
      "2/2 [==============================] - 0s 151ms/step - loss: 0.6953 - acc: 0.5006 - val_loss: 0.6925 - val_acc: 0.5175\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 154ms/step - loss: 0.6960 - acc: 0.5013 - val_loss: 0.6925 - val_acc: 0.5175\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6954 - acc: 0.5091 - val_loss: 0.6925 - val_acc: 0.5175\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 151ms/step - loss: 0.6943 - acc: 0.5097 - val_loss: 0.6925 - val_acc: 0.5175\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6937 - acc: 0.5160 - val_loss: 0.6925 - val_acc: 0.5175\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6963 - acc: 0.5006 - val_loss: 0.6925 - val_acc: 0.5175\n",
      "Epoch 53/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6946 - acc: 0.5129 - val_loss: 0.6925 - val_acc: 0.5175\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6938 - acc: 0.5181 - val_loss: 0.6925 - val_acc: 0.5175\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6950 - acc: 0.5022 - val_loss: 0.6925 - val_acc: 0.5175\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6965 - acc: 0.5004 - val_loss: 0.6925 - val_acc: 0.5175\n",
      "Epoch 57/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6955 - acc: 0.5000 - val_loss: 0.6926 - val_acc: 0.5175\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6955 - acc: 0.4978 - val_loss: 0.6926 - val_acc: 0.5175\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6946 - acc: 0.5019 - val_loss: 0.6927 - val_acc: 0.5175\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6945 - acc: 0.5155 - val_loss: 0.6927 - val_acc: 0.5175\n",
      "Epoch 61/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6942 - acc: 0.4972 - val_loss: 0.6926 - val_acc: 0.5175\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6952 - acc: 0.5019 - val_loss: 0.6926 - val_acc: 0.5175\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6952 - acc: 0.5032 - val_loss: 0.6926 - val_acc: 0.5175\n",
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6944 - acc: 0.5097 - val_loss: 0.6925 - val_acc: 0.5175\n",
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6959 - acc: 0.4948 - val_loss: 0.6925 - val_acc: 0.5175\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6949 - acc: 0.5101 - val_loss: 0.6924 - val_acc: 0.5175\n",
      "Epoch 67/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6936 - acc: 0.5095 - val_loss: 0.6924 - val_acc: 0.5175\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6962 - acc: 0.4978 - val_loss: 0.6924 - val_acc: 0.5175\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6942 - acc: 0.5106 - val_loss: 0.6924 - val_acc: 0.5175\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6955 - acc: 0.5065 - val_loss: 0.6924 - val_acc: 0.5175\n",
      "sample weight :  [4.54288236e-04 1.88870706e-05 2.18014363e-05 ... 1.94404401e-05\n",
      " 2.09624770e-04 5.56499109e-05]\n",
      "1320693.0\n",
      "new_x, new_y (6182, 10, 4068) (6182,)\n",
      "WARNING:tensorflow:Layer lstm_60 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_61 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_62 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_63 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/70\n",
      "2/2 [==============================] - 6s 542ms/step - loss: 0.7007 - acc: 0.4944 - val_loss: 0.6925 - val_acc: 0.5175\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 302ms/step - loss: 0.6987 - acc: 0.5054 - val_loss: 0.6927 - val_acc: 0.5175\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.7005 - acc: 0.4907 - val_loss: 0.6928 - val_acc: 0.5175\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.7024 - acc: 0.5002 - val_loss: 0.6927 - val_acc: 0.5175\n",
      "Epoch 5/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6983 - acc: 0.5024 - val_loss: 0.6926 - val_acc: 0.5175\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6958 - acc: 0.5164 - val_loss: 0.6926 - val_acc: 0.5175\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6957 - acc: 0.5168 - val_loss: 0.6928 - val_acc: 0.5175\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6983 - acc: 0.4963 - val_loss: 0.6931 - val_acc: 0.5175\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6965 - acc: 0.4987 - val_loss: 0.6930 - val_acc: 0.5175\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6973 - acc: 0.5054 - val_loss: 0.6928 - val_acc: 0.5175\n",
      "Epoch 11/70\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 0.6975 - acc: 0.4942 - val_loss: 0.6927 - val_acc: 0.5175\n",
      "Epoch 12/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6960 - acc: 0.5050 - val_loss: 0.6925 - val_acc: 0.5175\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6955 - acc: 0.5013 - val_loss: 0.6925 - val_acc: 0.5175\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 154ms/step - loss: 0.6967 - acc: 0.5082 - val_loss: 0.6926 - val_acc: 0.5175\n",
      "Epoch 15/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6949 - acc: 0.5095 - val_loss: 0.6925 - val_acc: 0.5175\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6975 - acc: 0.4991 - val_loss: 0.6925 - val_acc: 0.5175\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6958 - acc: 0.5097 - val_loss: 0.6925 - val_acc: 0.5175\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6969 - acc: 0.4937 - val_loss: 0.6925 - val_acc: 0.5175\n",
      "Epoch 19/70\n",
      "2/2 [==============================] - 0s 161ms/step - loss: 0.6952 - acc: 0.5022 - val_loss: 0.6926 - val_acc: 0.5175\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6959 - acc: 0.5032 - val_loss: 0.6926 - val_acc: 0.5175\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6952 - acc: 0.4989 - val_loss: 0.6926 - val_acc: 0.5175\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6943 - acc: 0.5052 - val_loss: 0.6925 - val_acc: 0.5175\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6961 - acc: 0.4987 - val_loss: 0.6925 - val_acc: 0.5175\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6956 - acc: 0.5104 - val_loss: 0.6926 - val_acc: 0.5175\n",
      "Epoch 25/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 148ms/step - loss: 0.6954 - acc: 0.5037 - val_loss: 0.6927 - val_acc: 0.5175\n",
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6955 - acc: 0.5035 - val_loss: 0.6926 - val_acc: 0.5175\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6954 - acc: 0.5043 - val_loss: 0.6925 - val_acc: 0.5175\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6946 - acc: 0.5056 - val_loss: 0.6927 - val_acc: 0.5175\n",
      "Epoch 29/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6957 - acc: 0.5017 - val_loss: 0.6930 - val_acc: 0.5175\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6961 - acc: 0.4864 - val_loss: 0.6931 - val_acc: 0.5233\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6955 - acc: 0.4996 - val_loss: 0.6930 - val_acc: 0.5175\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6950 - acc: 0.4972 - val_loss: 0.6928 - val_acc: 0.5175\n",
      "Epoch 33/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6918 - acc: 0.5257 - val_loss: 0.6926 - val_acc: 0.5175\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6934 - acc: 0.5073 - val_loss: 0.6925 - val_acc: 0.5175\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6954 - acc: 0.4965 - val_loss: 0.6925 - val_acc: 0.5175\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6946 - acc: 0.5035 - val_loss: 0.6926 - val_acc: 0.5175\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6949 - acc: 0.5110 - val_loss: 0.6926 - val_acc: 0.5175\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6939 - acc: 0.5121 - val_loss: 0.6927 - val_acc: 0.5175\n",
      "Epoch 39/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6951 - acc: 0.5145 - val_loss: 0.6926 - val_acc: 0.5175\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.6948 - acc: 0.5039 - val_loss: 0.6926 - val_acc: 0.5175\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6942 - acc: 0.5160 - val_loss: 0.6925 - val_acc: 0.5175\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6933 - acc: 0.5151 - val_loss: 0.6925 - val_acc: 0.5175\n",
      "Epoch 43/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6945 - acc: 0.5067 - val_loss: 0.6925 - val_acc: 0.5175\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6935 - acc: 0.5067 - val_loss: 0.6925 - val_acc: 0.5175\n",
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6927 - acc: 0.5179 - val_loss: 0.6925 - val_acc: 0.5175\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6946 - acc: 0.5019 - val_loss: 0.6925 - val_acc: 0.5175\n",
      "Epoch 47/70\n",
      "2/2 [==============================] - 0s 155ms/step - loss: 0.6930 - acc: 0.5080 - val_loss: 0.6925 - val_acc: 0.5175\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6918 - acc: 0.5160 - val_loss: 0.6925 - val_acc: 0.5175\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6945 - acc: 0.5019 - val_loss: 0.6925 - val_acc: 0.5175\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6940 - acc: 0.5071 - val_loss: 0.6924 - val_acc: 0.5175\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6929 - acc: 0.5170 - val_loss: 0.6924 - val_acc: 0.5175\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 153ms/step - loss: 0.6932 - acc: 0.5121 - val_loss: 0.6924 - val_acc: 0.5175\n",
      "Epoch 53/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6924 - acc: 0.5136 - val_loss: 0.6925 - val_acc: 0.5175\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6938 - acc: 0.5095 - val_loss: 0.6925 - val_acc: 0.5175\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6935 - acc: 0.5080 - val_loss: 0.6924 - val_acc: 0.5175\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.6921 - acc: 0.5216 - val_loss: 0.6923 - val_acc: 0.5175\n",
      "Epoch 57/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6928 - acc: 0.5181 - val_loss: 0.6923 - val_acc: 0.5175\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.6933 - acc: 0.5086 - val_loss: 0.6923 - val_acc: 0.5175\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6935 - acc: 0.5099 - val_loss: 0.6923 - val_acc: 0.5175\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6917 - acc: 0.5250 - val_loss: 0.6922 - val_acc: 0.5175\n",
      "Epoch 61/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6935 - acc: 0.5080 - val_loss: 0.6921 - val_acc: 0.5175\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.6919 - acc: 0.5166 - val_loss: 0.6921 - val_acc: 0.5175\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6925 - acc: 0.5179 - val_loss: 0.6920 - val_acc: 0.5246\n",
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6930 - acc: 0.5119 - val_loss: 0.6917 - val_acc: 0.5259\n",
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6913 - acc: 0.5207 - val_loss: 0.6914 - val_acc: 0.5233\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6912 - acc: 0.5285 - val_loss: 0.6912 - val_acc: 0.5298\n",
      "Epoch 67/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6895 - acc: 0.5382 - val_loss: 0.6907 - val_acc: 0.5233\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6898 - acc: 0.5356 - val_loss: 0.6902 - val_acc: 0.5213\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6889 - acc: 0.5365 - val_loss: 0.6896 - val_acc: 0.5349\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6865 - acc: 0.5557 - val_loss: 0.6885 - val_acc: 0.5660\n",
      "sample weight :  [4.32170959e-04 1.84885150e-05 2.22033862e-05 ... 1.97713127e-05\n",
      " 2.06772900e-04 5.68394675e-05]\n",
      "1314816.0\n",
      "new_x, new_y (6182, 10, 4068) (6182,)\n",
      "WARNING:tensorflow:Layer lstm_64 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_65 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_66 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_67 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/70\n",
      "2/2 [==============================] - 6s 798ms/step - loss: 0.7589 - acc: 0.5080 - val_loss: 0.7325 - val_acc: 0.5019\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.7322 - acc: 0.5080 - val_loss: 0.7119 - val_acc: 0.5019\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.7140 - acc: 0.5047 - val_loss: 0.6990 - val_acc: 0.5019\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.7015 - acc: 0.4983 - val_loss: 0.6935 - val_acc: 0.5019\n",
      "Epoch 5/70\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 0.6981 - acc: 0.5082 - val_loss: 0.6939 - val_acc: 0.4981\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.7007 - acc: 0.4981 - val_loss: 0.6955 - val_acc: 0.4981\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.7040 - acc: 0.4914 - val_loss: 0.6957 - val_acc: 0.4981\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.7032 - acc: 0.4925 - val_loss: 0.6950 - val_acc: 0.4981\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.7010 - acc: 0.5026 - val_loss: 0.6939 - val_acc: 0.4981\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.7002 - acc: 0.4918 - val_loss: 0.6932 - val_acc: 0.4981\n",
      "Epoch 11/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6987 - acc: 0.4950 - val_loss: 0.6932 - val_acc: 0.5019\n",
      "Epoch 12/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6980 - acc: 0.4950 - val_loss: 0.6936 - val_acc: 0.5019\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6964 - acc: 0.5065 - val_loss: 0.6942 - val_acc: 0.5019\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.7000 - acc: 0.4944 - val_loss: 0.6949 - val_acc: 0.5019\n",
      "Epoch 15/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.7001 - acc: 0.4944 - val_loss: 0.6954 - val_acc: 0.5019\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6974 - acc: 0.5112 - val_loss: 0.6955 - val_acc: 0.5019\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.7006 - acc: 0.5035 - val_loss: 0.6952 - val_acc: 0.5019\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6992 - acc: 0.5084 - val_loss: 0.6946 - val_acc: 0.5019\n",
      "Epoch 19/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6968 - acc: 0.5078 - val_loss: 0.6939 - val_acc: 0.5019\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6967 - acc: 0.5082 - val_loss: 0.6934 - val_acc: 0.5019\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6964 - acc: 0.5024 - val_loss: 0.6931 - val_acc: 0.5019\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6961 - acc: 0.5093 - val_loss: 0.6931 - val_acc: 0.5019\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6977 - acc: 0.5024 - val_loss: 0.6932 - val_acc: 0.4981\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6950 - acc: 0.5140 - val_loss: 0.6932 - val_acc: 0.4981\n",
      "Epoch 25/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6985 - acc: 0.4963 - val_loss: 0.6932 - val_acc: 0.4981\n",
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6972 - acc: 0.5000 - val_loss: 0.6932 - val_acc: 0.4981\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 152ms/step - loss: 0.6963 - acc: 0.5024 - val_loss: 0.6931 - val_acc: 0.5019\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6966 - acc: 0.5032 - val_loss: 0.6932 - val_acc: 0.5019\n",
      "Epoch 29/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6960 - acc: 0.5104 - val_loss: 0.6932 - val_acc: 0.5019\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6950 - acc: 0.5035 - val_loss: 0.6933 - val_acc: 0.5019\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6994 - acc: 0.4929 - val_loss: 0.6934 - val_acc: 0.5019\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6957 - acc: 0.5071 - val_loss: 0.6933 - val_acc: 0.5019\n",
      "Epoch 33/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6966 - acc: 0.5017 - val_loss: 0.6933 - val_acc: 0.5019\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6966 - acc: 0.5039 - val_loss: 0.6934 - val_acc: 0.5019\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.6948 - acc: 0.5088 - val_loss: 0.6935 - val_acc: 0.5019\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6979 - acc: 0.4942 - val_loss: 0.6935 - val_acc: 0.5019\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6947 - acc: 0.5168 - val_loss: 0.6935 - val_acc: 0.5019\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6948 - acc: 0.5067 - val_loss: 0.6935 - val_acc: 0.5019\n",
      "Epoch 39/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6942 - acc: 0.5104 - val_loss: 0.6934 - val_acc: 0.5019\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6972 - acc: 0.4991 - val_loss: 0.6934 - val_acc: 0.5019\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6962 - acc: 0.5009 - val_loss: 0.6933 - val_acc: 0.5019\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6962 - acc: 0.5093 - val_loss: 0.6933 - val_acc: 0.5019\n",
      "Epoch 43/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6957 - acc: 0.5009 - val_loss: 0.6933 - val_acc: 0.5019\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6961 - acc: 0.4981 - val_loss: 0.6933 - val_acc: 0.5019\n",
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6973 - acc: 0.4935 - val_loss: 0.6933 - val_acc: 0.5019\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 0.6962 - acc: 0.4972 - val_loss: 0.6932 - val_acc: 0.5019\n",
      "Epoch 47/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6950 - acc: 0.4994 - val_loss: 0.6932 - val_acc: 0.5019\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6951 - acc: 0.5080 - val_loss: 0.6932 - val_acc: 0.5019\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6960 - acc: 0.5032 - val_loss: 0.6932 - val_acc: 0.5019\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6970 - acc: 0.4961 - val_loss: 0.6933 - val_acc: 0.5019\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 0.6948 - acc: 0.4981 - val_loss: 0.6933 - val_acc: 0.5019\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6954 - acc: 0.4968 - val_loss: 0.6933 - val_acc: 0.5019\n",
      "Epoch 53/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6952 - acc: 0.5004 - val_loss: 0.6933 - val_acc: 0.5019\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6932 - acc: 0.5138 - val_loss: 0.6934 - val_acc: 0.5019\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6953 - acc: 0.5121 - val_loss: 0.6934 - val_acc: 0.5019\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6955 - acc: 0.5071 - val_loss: 0.6934 - val_acc: 0.5019\n",
      "Epoch 57/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6951 - acc: 0.5075 - val_loss: 0.6932 - val_acc: 0.5019\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6954 - acc: 0.5006 - val_loss: 0.6931 - val_acc: 0.5019\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6970 - acc: 0.4879 - val_loss: 0.6931 - val_acc: 0.5019\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 0s 151ms/step - loss: 0.6958 - acc: 0.4946 - val_loss: 0.6931 - val_acc: 0.5019\n",
      "Epoch 61/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6942 - acc: 0.5002 - val_loss: 0.6931 - val_acc: 0.4987\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.6942 - acc: 0.5104 - val_loss: 0.6931 - val_acc: 0.4981\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6939 - acc: 0.5088 - val_loss: 0.6930 - val_acc: 0.5019\n",
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6956 - acc: 0.4965 - val_loss: 0.6930 - val_acc: 0.5110\n",
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.6951 - acc: 0.5002 - val_loss: 0.6930 - val_acc: 0.5019\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6943 - acc: 0.5024 - val_loss: 0.6930 - val_acc: 0.5019\n",
      "Epoch 67/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6944 - acc: 0.4983 - val_loss: 0.6930 - val_acc: 0.5019\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6947 - acc: 0.4968 - val_loss: 0.6930 - val_acc: 0.5019\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 153ms/step - loss: 0.6946 - acc: 0.5067 - val_loss: 0.6931 - val_acc: 0.5019\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6944 - acc: 0.5065 - val_loss: 0.6932 - val_acc: 0.5019\n",
      "sample weight :  [4.44210044e-04 1.90117990e-05 2.16509059e-05 ... 1.92234184e-05\n",
      " 2.01453315e-04 5.51580382e-05]\n",
      "1335276.0\n",
      "new_x, new_y (6182, 10, 4068) (6182,)\n",
      "WARNING:tensorflow:Layer lstm_68 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_69 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_70 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_71 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/70\n",
      "2/2 [==============================] - 6s 554ms/step - loss: 0.7132 - acc: 0.4987 - val_loss: 0.6959 - val_acc: 0.4916\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.7041 - acc: 0.5013 - val_loss: 0.6930 - val_acc: 0.5084\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.7004 - acc: 0.5015 - val_loss: 0.6945 - val_acc: 0.5084\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6996 - acc: 0.5104 - val_loss: 0.6950 - val_acc: 0.5084\n",
      "Epoch 5/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.7009 - acc: 0.5039 - val_loss: 0.6942 - val_acc: 0.5084\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6998 - acc: 0.5114 - val_loss: 0.6933 - val_acc: 0.5084\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.7000 - acc: 0.4985 - val_loss: 0.6930 - val_acc: 0.5084\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.6978 - acc: 0.5099 - val_loss: 0.6931 - val_acc: 0.5084\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.7010 - acc: 0.4873 - val_loss: 0.6932 - val_acc: 0.4916\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.7005 - acc: 0.5022 - val_loss: 0.6933 - val_acc: 0.4916\n",
      "Epoch 11/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.7030 - acc: 0.4849 - val_loss: 0.6932 - val_acc: 0.4916\n",
      "Epoch 12/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.7020 - acc: 0.4929 - val_loss: 0.6931 - val_acc: 0.5084\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6976 - acc: 0.5091 - val_loss: 0.6930 - val_acc: 0.5084\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6967 - acc: 0.5179 - val_loss: 0.6930 - val_acc: 0.5084\n",
      "Epoch 15/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.7000 - acc: 0.4989 - val_loss: 0.6932 - val_acc: 0.5084\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6973 - acc: 0.5011 - val_loss: 0.6933 - val_acc: 0.5084\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6971 - acc: 0.5084 - val_loss: 0.6933 - val_acc: 0.5084\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6968 - acc: 0.5142 - val_loss: 0.6932 - val_acc: 0.5084\n",
      "Epoch 19/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6962 - acc: 0.5129 - val_loss: 0.6931 - val_acc: 0.5084\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6970 - acc: 0.5004 - val_loss: 0.6930 - val_acc: 0.5084\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6974 - acc: 0.4957 - val_loss: 0.6930 - val_acc: 0.5084\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6988 - acc: 0.5013 - val_loss: 0.6931 - val_acc: 0.5084\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6983 - acc: 0.5017 - val_loss: 0.6932 - val_acc: 0.4916\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6973 - acc: 0.5006 - val_loss: 0.6932 - val_acc: 0.4916\n",
      "Epoch 25/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6991 - acc: 0.5002 - val_loss: 0.6931 - val_acc: 0.5084\n",
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6972 - acc: 0.4991 - val_loss: 0.6930 - val_acc: 0.5084\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6968 - acc: 0.5015 - val_loss: 0.6930 - val_acc: 0.5084\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6971 - acc: 0.4953 - val_loss: 0.6933 - val_acc: 0.5084\n",
      "Epoch 29/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6960 - acc: 0.5104 - val_loss: 0.6935 - val_acc: 0.5084\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6953 - acc: 0.5106 - val_loss: 0.6935 - val_acc: 0.5084\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6957 - acc: 0.5119 - val_loss: 0.6934 - val_acc: 0.5084\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6961 - acc: 0.5060 - val_loss: 0.6930 - val_acc: 0.5084\n",
      "Epoch 33/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6951 - acc: 0.5060 - val_loss: 0.6929 - val_acc: 0.5084\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6954 - acc: 0.4994 - val_loss: 0.6930 - val_acc: 0.5084\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6951 - acc: 0.5050 - val_loss: 0.6931 - val_acc: 0.4916\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6953 - acc: 0.5011 - val_loss: 0.6931 - val_acc: 0.4916\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6954 - acc: 0.4961 - val_loss: 0.6930 - val_acc: 0.5084\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.6946 - acc: 0.5080 - val_loss: 0.6929 - val_acc: 0.5084\n",
      "Epoch 39/70\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 0.6955 - acc: 0.4968 - val_loss: 0.6930 - val_acc: 0.5084\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.6957 - acc: 0.5039 - val_loss: 0.6931 - val_acc: 0.5084\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6957 - acc: 0.5009 - val_loss: 0.6931 - val_acc: 0.5084\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6961 - acc: 0.4991 - val_loss: 0.6930 - val_acc: 0.5084\n",
      "Epoch 43/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6956 - acc: 0.4937 - val_loss: 0.6929 - val_acc: 0.5084\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6951 - acc: 0.5041 - val_loss: 0.6929 - val_acc: 0.5084\n",
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6952 - acc: 0.4978 - val_loss: 0.6931 - val_acc: 0.4916\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6958 - acc: 0.4888 - val_loss: 0.6931 - val_acc: 0.4916\n",
      "Epoch 47/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6946 - acc: 0.5045 - val_loss: 0.6930 - val_acc: 0.5893\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6961 - acc: 0.4957 - val_loss: 0.6929 - val_acc: 0.5084\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6942 - acc: 0.5086 - val_loss: 0.6928 - val_acc: 0.5084\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6943 - acc: 0.5026 - val_loss: 0.6928 - val_acc: 0.5084\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6928 - acc: 0.5069 - val_loss: 0.6930 - val_acc: 0.5084\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6949 - acc: 0.5047 - val_loss: 0.6934 - val_acc: 0.5084\n",
      "Epoch 53/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6956 - acc: 0.5043 - val_loss: 0.6936 - val_acc: 0.5084\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 1s 139ms/step - loss: 0.6951 - acc: 0.5099 - val_loss: 0.6933 - val_acc: 0.5084\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6948 - acc: 0.5101 - val_loss: 0.6929 - val_acc: 0.5084\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6946 - acc: 0.5037 - val_loss: 0.6926 - val_acc: 0.5084\n",
      "Epoch 57/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6948 - acc: 0.5024 - val_loss: 0.6927 - val_acc: 0.5375\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6929 - acc: 0.5101 - val_loss: 0.6928 - val_acc: 0.4922\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 0.6925 - acc: 0.5043 - val_loss: 0.6928 - val_acc: 0.4916\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 0.6940 - acc: 0.5069 - val_loss: 0.6926 - val_acc: 0.4935\n",
      "Epoch 61/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6923 - acc: 0.5186 - val_loss: 0.6924 - val_acc: 0.5427\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6924 - acc: 0.5119 - val_loss: 0.6923 - val_acc: 0.5084\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6927 - acc: 0.5162 - val_loss: 0.6924 - val_acc: 0.5084\n",
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6917 - acc: 0.5261 - val_loss: 0.6924 - val_acc: 0.5084\n",
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6925 - acc: 0.5097 - val_loss: 0.6922 - val_acc: 0.5084\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6925 - acc: 0.5110 - val_loss: 0.6920 - val_acc: 0.5084\n",
      "Epoch 67/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6917 - acc: 0.5155 - val_loss: 0.6916 - val_acc: 0.5246\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6903 - acc: 0.5300 - val_loss: 0.6911 - val_acc: 0.5589\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.6890 - acc: 0.5440 - val_loss: 0.6905 - val_acc: 0.5627\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6887 - acc: 0.5414 - val_loss: 0.6899 - val_acc: 0.5647\n",
      "sample weight :  [4.34566597e-04 1.87403221e-05 2.25302086e-05 ... 1.98659800e-05\n",
      " 1.97546596e-04 5.53074989e-05]\n",
      "1308700.0\n",
      "new_x, new_y (6182, 10, 4068) (6182,)\n",
      "WARNING:tensorflow:Layer lstm_72 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_73 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_74 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_75 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/70\n",
      "2/2 [==============================] - 6s 556ms/step - loss: 0.7265 - acc: 0.4860 - val_loss: 0.7036 - val_acc: 0.4897\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.7109 - acc: 0.4836 - val_loss: 0.6947 - val_acc: 0.4897\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.7006 - acc: 0.5000 - val_loss: 0.6932 - val_acc: 0.5103\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6997 - acc: 0.5082 - val_loss: 0.6962 - val_acc: 0.5103\n",
      "Epoch 5/70\n",
      "2/2 [==============================] - 0s 151ms/step - loss: 0.7001 - acc: 0.5123 - val_loss: 0.6981 - val_acc: 0.5103\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.7003 - acc: 0.5231 - val_loss: 0.6969 - val_acc: 0.5103\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.7002 - acc: 0.5151 - val_loss: 0.6947 - val_acc: 0.5103\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6992 - acc: 0.5104 - val_loss: 0.6933 - val_acc: 0.5103\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6955 - acc: 0.5110 - val_loss: 0.6929 - val_acc: 0.5103\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6978 - acc: 0.4937 - val_loss: 0.6931 - val_acc: 0.5103\n",
      "Epoch 11/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6983 - acc: 0.5054 - val_loss: 0.6934 - val_acc: 0.4897\n",
      "Epoch 12/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6981 - acc: 0.5073 - val_loss: 0.6936 - val_acc: 0.4897\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6982 - acc: 0.4970 - val_loss: 0.6937 - val_acc: 0.4897\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6999 - acc: 0.4983 - val_loss: 0.6936 - val_acc: 0.4897\n",
      "Epoch 15/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6974 - acc: 0.5106 - val_loss: 0.6934 - val_acc: 0.4897\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6988 - acc: 0.4981 - val_loss: 0.6932 - val_acc: 0.4897\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6958 - acc: 0.5073 - val_loss: 0.6930 - val_acc: 0.5103\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6952 - acc: 0.5080 - val_loss: 0.6929 - val_acc: 0.5103\n",
      "Epoch 19/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6954 - acc: 0.5078 - val_loss: 0.6931 - val_acc: 0.5103\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6986 - acc: 0.4950 - val_loss: 0.6932 - val_acc: 0.5103\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 156ms/step - loss: 0.6983 - acc: 0.5028 - val_loss: 0.6934 - val_acc: 0.5103\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6988 - acc: 0.4996 - val_loss: 0.6933 - val_acc: 0.5103\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 151ms/step - loss: 0.6981 - acc: 0.5013 - val_loss: 0.6931 - val_acc: 0.5103\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6981 - acc: 0.4991 - val_loss: 0.6930 - val_acc: 0.5103\n",
      "Epoch 25/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.6977 - acc: 0.5011 - val_loss: 0.6929 - val_acc: 0.5103\n",
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6972 - acc: 0.4925 - val_loss: 0.6929 - val_acc: 0.5103\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6954 - acc: 0.5088 - val_loss: 0.6929 - val_acc: 0.5103\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6953 - acc: 0.5099 - val_loss: 0.6929 - val_acc: 0.5103\n",
      "Epoch 29/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6957 - acc: 0.5080 - val_loss: 0.6929 - val_acc: 0.5103\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6956 - acc: 0.5041 - val_loss: 0.6929 - val_acc: 0.5103\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6969 - acc: 0.5039 - val_loss: 0.6929 - val_acc: 0.5103\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6954 - acc: 0.5017 - val_loss: 0.6930 - val_acc: 0.5103\n",
      "Epoch 33/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6970 - acc: 0.5073 - val_loss: 0.6932 - val_acc: 0.5103\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 0.6941 - acc: 0.5119 - val_loss: 0.6933 - val_acc: 0.5103\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6961 - acc: 0.5041 - val_loss: 0.6932 - val_acc: 0.5103\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6940 - acc: 0.5104 - val_loss: 0.6931 - val_acc: 0.5103\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 151ms/step - loss: 0.6969 - acc: 0.4970 - val_loss: 0.6931 - val_acc: 0.5103\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6942 - acc: 0.5132 - val_loss: 0.6932 - val_acc: 0.5103\n",
      "Epoch 39/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.6953 - acc: 0.5091 - val_loss: 0.6931 - val_acc: 0.5103\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6938 - acc: 0.5104 - val_loss: 0.6930 - val_acc: 0.5103\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6968 - acc: 0.4931 - val_loss: 0.6930 - val_acc: 0.5103\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6954 - acc: 0.5058 - val_loss: 0.6929 - val_acc: 0.5103\n",
      "Epoch 43/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6942 - acc: 0.5123 - val_loss: 0.6929 - val_acc: 0.5103\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6967 - acc: 0.5026 - val_loss: 0.6930 - val_acc: 0.5103\n",
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6953 - acc: 0.5009 - val_loss: 0.6930 - val_acc: 0.5103\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6952 - acc: 0.5028 - val_loss: 0.6930 - val_acc: 0.5103\n",
      "Epoch 47/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6935 - acc: 0.5147 - val_loss: 0.6930 - val_acc: 0.5103\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6947 - acc: 0.5114 - val_loss: 0.6930 - val_acc: 0.5103\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6947 - acc: 0.5028 - val_loss: 0.6930 - val_acc: 0.5103\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6948 - acc: 0.5058 - val_loss: 0.6930 - val_acc: 0.5103\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6953 - acc: 0.5060 - val_loss: 0.6930 - val_acc: 0.5103\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6924 - acc: 0.5183 - val_loss: 0.6930 - val_acc: 0.5103\n",
      "Epoch 53/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6938 - acc: 0.5075 - val_loss: 0.6930 - val_acc: 0.5103\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6942 - acc: 0.5075 - val_loss: 0.6930 - val_acc: 0.5103\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 0s 154ms/step - loss: 0.6939 - acc: 0.5067 - val_loss: 0.6929 - val_acc: 0.5103\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.6932 - acc: 0.5179 - val_loss: 0.6928 - val_acc: 0.5103\n",
      "Epoch 57/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6949 - acc: 0.5004 - val_loss: 0.6928 - val_acc: 0.5103\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6942 - acc: 0.5006 - val_loss: 0.6928 - val_acc: 0.5103\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6932 - acc: 0.5125 - val_loss: 0.6928 - val_acc: 0.5103\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6941 - acc: 0.5073 - val_loss: 0.6928 - val_acc: 0.5103\n",
      "Epoch 61/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6942 - acc: 0.5080 - val_loss: 0.6928 - val_acc: 0.5103\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6939 - acc: 0.5019 - val_loss: 0.6928 - val_acc: 0.5103\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6946 - acc: 0.5052 - val_loss: 0.6927 - val_acc: 0.5103\n",
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6939 - acc: 0.5112 - val_loss: 0.6926 - val_acc: 0.5103\n",
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6930 - acc: 0.5183 - val_loss: 0.6926 - val_acc: 0.5103\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6943 - acc: 0.5058 - val_loss: 0.6926 - val_acc: 0.5103\n",
      "Epoch 67/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6931 - acc: 0.5119 - val_loss: 0.6927 - val_acc: 0.5103\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6926 - acc: 0.5151 - val_loss: 0.6926 - val_acc: 0.5103\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6935 - acc: 0.5134 - val_loss: 0.6925 - val_acc: 0.5103\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6927 - acc: 0.5207 - val_loss: 0.6923 - val_acc: 0.5103\n",
      "sample weight :  [4.15051243e-04 1.79785206e-05 2.36886498e-05 ... 2.06270409e-05\n",
      " 2.05598373e-04 5.73984102e-05]\n",
      "1344759.0\n",
      "new_x, new_y (6182, 10, 4068) (6182,)\n",
      "WARNING:tensorflow:Layer lstm_76 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_77 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_78 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_79 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/70\n",
      "2/2 [==============================] - 6s 618ms/step - loss: 0.6976 - acc: 0.5035 - val_loss: 0.6934 - val_acc: 0.5013\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6957 - acc: 0.5112 - val_loss: 0.6935 - val_acc: 0.5013\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6993 - acc: 0.4845 - val_loss: 0.6934 - val_acc: 0.5013\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6978 - acc: 0.4905 - val_loss: 0.6934 - val_acc: 0.5013\n",
      "Epoch 5/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6966 - acc: 0.5022 - val_loss: 0.6935 - val_acc: 0.5013\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6953 - acc: 0.5050 - val_loss: 0.6940 - val_acc: 0.5013\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6954 - acc: 0.5084 - val_loss: 0.6940 - val_acc: 0.5013\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6946 - acc: 0.5157 - val_loss: 0.6936 - val_acc: 0.5013\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6968 - acc: 0.5015 - val_loss: 0.6933 - val_acc: 0.5013\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6958 - acc: 0.5017 - val_loss: 0.6933 - val_acc: 0.5013\n",
      "Epoch 11/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6950 - acc: 0.5052 - val_loss: 0.6932 - val_acc: 0.5013\n",
      "Epoch 12/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.6943 - acc: 0.5075 - val_loss: 0.6932 - val_acc: 0.5013\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6948 - acc: 0.5037 - val_loss: 0.6933 - val_acc: 0.5013\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 156ms/step - loss: 0.6948 - acc: 0.4950 - val_loss: 0.6936 - val_acc: 0.5013\n",
      "Epoch 15/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6937 - acc: 0.5112 - val_loss: 0.6938 - val_acc: 0.5013\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6943 - acc: 0.5088 - val_loss: 0.6935 - val_acc: 0.5013\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6936 - acc: 0.5136 - val_loss: 0.6932 - val_acc: 0.5013\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6949 - acc: 0.5009 - val_loss: 0.6932 - val_acc: 0.5013\n",
      "Epoch 19/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6940 - acc: 0.4989 - val_loss: 0.6933 - val_acc: 0.5013\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6949 - acc: 0.4860 - val_loss: 0.6934 - val_acc: 0.5013\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6946 - acc: 0.4978 - val_loss: 0.6936 - val_acc: 0.5013\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6929 - acc: 0.5099 - val_loss: 0.6938 - val_acc: 0.5013\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.6950 - acc: 0.5112 - val_loss: 0.6937 - val_acc: 0.5013\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6944 - acc: 0.5138 - val_loss: 0.6934 - val_acc: 0.5013\n",
      "Epoch 25/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6939 - acc: 0.5011 - val_loss: 0.6932 - val_acc: 0.5013\n",
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6931 - acc: 0.5170 - val_loss: 0.6932 - val_acc: 0.5013\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6929 - acc: 0.5054 - val_loss: 0.6932 - val_acc: 0.5013\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6941 - acc: 0.5041 - val_loss: 0.6933 - val_acc: 0.5013\n",
      "Epoch 29/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6932 - acc: 0.5071 - val_loss: 0.6935 - val_acc: 0.5013\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6939 - acc: 0.5054 - val_loss: 0.6938 - val_acc: 0.5013\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6940 - acc: 0.5155 - val_loss: 0.6939 - val_acc: 0.5013\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6946 - acc: 0.5037 - val_loss: 0.6939 - val_acc: 0.5013\n",
      "Epoch 33/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6936 - acc: 0.5047 - val_loss: 0.6939 - val_acc: 0.5013\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6928 - acc: 0.5134 - val_loss: 0.6936 - val_acc: 0.5013\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6939 - acc: 0.5056 - val_loss: 0.6934 - val_acc: 0.5013\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6940 - acc: 0.5039 - val_loss: 0.6934 - val_acc: 0.5013\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6932 - acc: 0.5166 - val_loss: 0.6934 - val_acc: 0.5013\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6937 - acc: 0.5052 - val_loss: 0.6934 - val_acc: 0.5013\n",
      "Epoch 39/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6935 - acc: 0.5116 - val_loss: 0.6935 - val_acc: 0.5013\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6931 - acc: 0.5181 - val_loss: 0.6935 - val_acc: 0.5013\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6937 - acc: 0.5073 - val_loss: 0.6934 - val_acc: 0.5013\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6931 - acc: 0.5039 - val_loss: 0.6933 - val_acc: 0.5013\n",
      "Epoch 43/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6935 - acc: 0.5093 - val_loss: 0.6932 - val_acc: 0.5013\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6941 - acc: 0.5002 - val_loss: 0.6932 - val_acc: 0.5013\n",
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6935 - acc: 0.5002 - val_loss: 0.6932 - val_acc: 0.5013\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6927 - acc: 0.5153 - val_loss: 0.6933 - val_acc: 0.5013\n",
      "Epoch 47/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6938 - acc: 0.4991 - val_loss: 0.6934 - val_acc: 0.5013\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6937 - acc: 0.5056 - val_loss: 0.6935 - val_acc: 0.5013\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6933 - acc: 0.5108 - val_loss: 0.6936 - val_acc: 0.5013\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6930 - acc: 0.5123 - val_loss: 0.6936 - val_acc: 0.5013\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6924 - acc: 0.5121 - val_loss: 0.6935 - val_acc: 0.5013\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6936 - acc: 0.5080 - val_loss: 0.6933 - val_acc: 0.5013\n",
      "Epoch 53/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6934 - acc: 0.5073 - val_loss: 0.6932 - val_acc: 0.5013\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6928 - acc: 0.5151 - val_loss: 0.6932 - val_acc: 0.5013\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6945 - acc: 0.4946 - val_loss: 0.6931 - val_acc: 0.5013\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6935 - acc: 0.5011 - val_loss: 0.6931 - val_acc: 0.5013\n",
      "Epoch 57/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6936 - acc: 0.5030 - val_loss: 0.6931 - val_acc: 0.5155\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6929 - acc: 0.5095 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6934 - acc: 0.4981 - val_loss: 0.6931 - val_acc: 0.5013\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6932 - acc: 0.5000 - val_loss: 0.6932 - val_acc: 0.5013\n",
      "Epoch 61/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6937 - acc: 0.5069 - val_loss: 0.6935 - val_acc: 0.5013\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6936 - acc: 0.5114 - val_loss: 0.6940 - val_acc: 0.5013\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6934 - acc: 0.5151 - val_loss: 0.6941 - val_acc: 0.5013\n",
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6934 - acc: 0.5116 - val_loss: 0.6938 - val_acc: 0.5013\n",
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6941 - acc: 0.5043 - val_loss: 0.6935 - val_acc: 0.5013\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6934 - acc: 0.5106 - val_loss: 0.6933 - val_acc: 0.5013\n",
      "Epoch 67/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6937 - acc: 0.4996 - val_loss: 0.6931 - val_acc: 0.5013\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6931 - acc: 0.5127 - val_loss: 0.6931 - val_acc: 0.4987\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6937 - acc: 0.5022 - val_loss: 0.6931 - val_acc: 0.5336\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6933 - acc: 0.5088 - val_loss: 0.6931 - val_acc: 0.5013\n",
      "sample weight :  [4.17903844e-04 1.80691751e-05 2.35871386e-05 ... 2.05299518e-05\n",
      " 2.04778603e-04 5.70302836e-05]\n",
      "1307252.0\n",
      "new_x, new_y (6182, 10, 4068) (6182,)\n",
      "WARNING:tensorflow:Layer lstm_80 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_81 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_82 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_83 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/70\n",
      "2/2 [==============================] - 7s 542ms/step - loss: 0.6981 - acc: 0.5086 - val_loss: 0.6933 - val_acc: 0.5136\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6941 - acc: 0.5209 - val_loss: 0.6931 - val_acc: 0.5136\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6985 - acc: 0.5065 - val_loss: 0.6929 - val_acc: 0.5136\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6979 - acc: 0.5104 - val_loss: 0.6929 - val_acc: 0.5136\n",
      "Epoch 5/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6972 - acc: 0.5104 - val_loss: 0.6931 - val_acc: 0.5136\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6941 - acc: 0.5207 - val_loss: 0.6938 - val_acc: 0.5136\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6961 - acc: 0.5129 - val_loss: 0.6941 - val_acc: 0.5136\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6946 - acc: 0.5280 - val_loss: 0.6937 - val_acc: 0.5136\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6956 - acc: 0.5166 - val_loss: 0.6933 - val_acc: 0.5136\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6937 - acc: 0.5166 - val_loss: 0.6932 - val_acc: 0.5136\n",
      "Epoch 11/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6928 - acc: 0.5183 - val_loss: 0.6932 - val_acc: 0.5136\n",
      "Epoch 12/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6942 - acc: 0.5179 - val_loss: 0.6932 - val_acc: 0.5136\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6927 - acc: 0.5250 - val_loss: 0.6930 - val_acc: 0.5136\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6937 - acc: 0.5186 - val_loss: 0.6929 - val_acc: 0.5136\n",
      "Epoch 15/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6936 - acc: 0.5132 - val_loss: 0.6928 - val_acc: 0.5136\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6937 - acc: 0.5104 - val_loss: 0.6928 - val_acc: 0.5136\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6928 - acc: 0.5239 - val_loss: 0.6928 - val_acc: 0.5136\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6929 - acc: 0.5177 - val_loss: 0.6928 - val_acc: 0.5136\n",
      "Epoch 19/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6933 - acc: 0.5153 - val_loss: 0.6930 - val_acc: 0.5136\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6920 - acc: 0.5259 - val_loss: 0.6933 - val_acc: 0.5136\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6929 - acc: 0.5235 - val_loss: 0.6940 - val_acc: 0.5136\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6926 - acc: 0.5216 - val_loss: 0.6944 - val_acc: 0.5136\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6927 - acc: 0.5291 - val_loss: 0.6941 - val_acc: 0.5136\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6915 - acc: 0.5317 - val_loss: 0.6936 - val_acc: 0.5136\n",
      "Epoch 25/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6923 - acc: 0.5261 - val_loss: 0.6932 - val_acc: 0.5136\n",
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6927 - acc: 0.5328 - val_loss: 0.6933 - val_acc: 0.5136\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6930 - acc: 0.5257 - val_loss: 0.6934 - val_acc: 0.5136\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6920 - acc: 0.5205 - val_loss: 0.6932 - val_acc: 0.5136\n",
      "Epoch 29/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6935 - acc: 0.5211 - val_loss: 0.6929 - val_acc: 0.5136\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6918 - acc: 0.5291 - val_loss: 0.6928 - val_acc: 0.5136\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6920 - acc: 0.5216 - val_loss: 0.6928 - val_acc: 0.5136\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6934 - acc: 0.5149 - val_loss: 0.6929 - val_acc: 0.5136\n",
      "Epoch 33/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6934 - acc: 0.5086 - val_loss: 0.6933 - val_acc: 0.5136\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6923 - acc: 0.5246 - val_loss: 0.6939 - val_acc: 0.5136\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6917 - acc: 0.5311 - val_loss: 0.6943 - val_acc: 0.5136\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6918 - acc: 0.5311 - val_loss: 0.6943 - val_acc: 0.5136\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6904 - acc: 0.5408 - val_loss: 0.6941 - val_acc: 0.5136\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6923 - acc: 0.5280 - val_loss: 0.6939 - val_acc: 0.5136\n",
      "Epoch 39/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6929 - acc: 0.5276 - val_loss: 0.6936 - val_acc: 0.5136\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6916 - acc: 0.5242 - val_loss: 0.6932 - val_acc: 0.5136\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6927 - acc: 0.5244 - val_loss: 0.6930 - val_acc: 0.5136\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6922 - acc: 0.5289 - val_loss: 0.6929 - val_acc: 0.5136\n",
      "Epoch 43/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6926 - acc: 0.5216 - val_loss: 0.6930 - val_acc: 0.5136\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6925 - acc: 0.5276 - val_loss: 0.6932 - val_acc: 0.5136\n",
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6930 - acc: 0.5164 - val_loss: 0.6934 - val_acc: 0.5136\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6917 - acc: 0.5330 - val_loss: 0.6936 - val_acc: 0.5136\n",
      "Epoch 47/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6916 - acc: 0.5274 - val_loss: 0.6936 - val_acc: 0.5136\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6918 - acc: 0.5272 - val_loss: 0.6934 - val_acc: 0.5136\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6905 - acc: 0.5324 - val_loss: 0.6932 - val_acc: 0.5136\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6919 - acc: 0.5280 - val_loss: 0.6932 - val_acc: 0.5136\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6913 - acc: 0.5280 - val_loss: 0.6931 - val_acc: 0.5136\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6923 - acc: 0.5278 - val_loss: 0.6931 - val_acc: 0.5136\n",
      "Epoch 53/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6921 - acc: 0.5239 - val_loss: 0.6931 - val_acc: 0.5136\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6914 - acc: 0.5308 - val_loss: 0.6931 - val_acc: 0.5136\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6928 - acc: 0.5261 - val_loss: 0.6932 - val_acc: 0.5136\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6918 - acc: 0.5257 - val_loss: 0.6935 - val_acc: 0.5136\n",
      "Epoch 57/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6911 - acc: 0.5311 - val_loss: 0.6937 - val_acc: 0.5136\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6915 - acc: 0.5306 - val_loss: 0.6936 - val_acc: 0.5136\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6920 - acc: 0.5311 - val_loss: 0.6933 - val_acc: 0.5136\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6921 - acc: 0.5267 - val_loss: 0.6930 - val_acc: 0.5136\n",
      "Epoch 61/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6916 - acc: 0.5300 - val_loss: 0.6929 - val_acc: 0.5136\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6912 - acc: 0.5311 - val_loss: 0.6929 - val_acc: 0.5136\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6909 - acc: 0.5289 - val_loss: 0.6929 - val_acc: 0.5136\n",
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6908 - acc: 0.5349 - val_loss: 0.6931 - val_acc: 0.5136\n",
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6910 - acc: 0.5291 - val_loss: 0.6933 - val_acc: 0.5136\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6921 - acc: 0.5267 - val_loss: 0.6931 - val_acc: 0.5136\n",
      "Epoch 67/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6912 - acc: 0.5326 - val_loss: 0.6928 - val_acc: 0.5136\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6895 - acc: 0.5302 - val_loss: 0.6926 - val_acc: 0.5136\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6906 - acc: 0.5302 - val_loss: 0.6927 - val_acc: 0.5136\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6907 - acc: 0.5360 - val_loss: 0.6925 - val_acc: 0.5136\n",
      "sample weight :  [4.48929358e-04 1.94620994e-05 2.24272682e-05 ... 1.89629053e-05\n",
      " 1.90080097e-04 5.28266935e-05]\n",
      "1332738.0\n",
      "new_x, new_y (6182, 10, 4068) (6182,)\n",
      "WARNING:tensorflow:Layer lstm_84 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_85 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_86 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_87 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/70\n",
      "2/2 [==============================] - 5s 559ms/step - loss: 0.7481 - acc: 0.4836 - val_loss: 0.7166 - val_acc: 0.4922\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.7253 - acc: 0.4853 - val_loss: 0.7009 - val_acc: 0.4922\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.7100 - acc: 0.4927 - val_loss: 0.6938 - val_acc: 0.4922\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.7010 - acc: 0.4957 - val_loss: 0.6938 - val_acc: 0.5078\n",
      "Epoch 5/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6988 - acc: 0.5142 - val_loss: 0.6975 - val_acc: 0.5078\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.7010 - acc: 0.5170 - val_loss: 0.7001 - val_acc: 0.5078\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.7062 - acc: 0.5129 - val_loss: 0.7001 - val_acc: 0.5078\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.7063 - acc: 0.5058 - val_loss: 0.6982 - val_acc: 0.5078\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.7032 - acc: 0.5145 - val_loss: 0.6960 - val_acc: 0.5078\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.7003 - acc: 0.5095 - val_loss: 0.6945 - val_acc: 0.5078\n",
      "Epoch 11/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6978 - acc: 0.5132 - val_loss: 0.6935 - val_acc: 0.5078\n",
      "Epoch 12/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6978 - acc: 0.5106 - val_loss: 0.6931 - val_acc: 0.5078\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6970 - acc: 0.5069 - val_loss: 0.6930 - val_acc: 0.5078\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.7006 - acc: 0.4946 - val_loss: 0.6931 - val_acc: 0.5078\n",
      "Epoch 15/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6956 - acc: 0.5086 - val_loss: 0.6931 - val_acc: 0.5078\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6963 - acc: 0.5047 - val_loss: 0.6931 - val_acc: 0.5078\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6992 - acc: 0.4916 - val_loss: 0.6930 - val_acc: 0.5078\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6984 - acc: 0.4948 - val_loss: 0.6930 - val_acc: 0.5078\n",
      "Epoch 19/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6970 - acc: 0.5039 - val_loss: 0.6930 - val_acc: 0.5078\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6973 - acc: 0.5080 - val_loss: 0.6931 - val_acc: 0.5078\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6989 - acc: 0.5073 - val_loss: 0.6932 - val_acc: 0.5078\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6977 - acc: 0.5035 - val_loss: 0.6934 - val_acc: 0.5078\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6976 - acc: 0.4998 - val_loss: 0.6935 - val_acc: 0.5078\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6977 - acc: 0.5060 - val_loss: 0.6936 - val_acc: 0.5078\n",
      "Epoch 25/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6970 - acc: 0.5002 - val_loss: 0.6936 - val_acc: 0.5078\n",
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6964 - acc: 0.5004 - val_loss: 0.6934 - val_acc: 0.5078\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6966 - acc: 0.4994 - val_loss: 0.6933 - val_acc: 0.5078\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6943 - acc: 0.5138 - val_loss: 0.6932 - val_acc: 0.5078\n",
      "Epoch 29/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6978 - acc: 0.4957 - val_loss: 0.6931 - val_acc: 0.5078\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6976 - acc: 0.4968 - val_loss: 0.6931 - val_acc: 0.5078\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6947 - acc: 0.5095 - val_loss: 0.6931 - val_acc: 0.5078\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6965 - acc: 0.4994 - val_loss: 0.6930 - val_acc: 0.5078\n",
      "Epoch 33/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6968 - acc: 0.4976 - val_loss: 0.6930 - val_acc: 0.5078\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6967 - acc: 0.5006 - val_loss: 0.6930 - val_acc: 0.5078\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6966 - acc: 0.5063 - val_loss: 0.6930 - val_acc: 0.5078\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6949 - acc: 0.5019 - val_loss: 0.6931 - val_acc: 0.5078\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6967 - acc: 0.4983 - val_loss: 0.6931 - val_acc: 0.5078\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6968 - acc: 0.5013 - val_loss: 0.6932 - val_acc: 0.5078\n",
      "Epoch 39/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6965 - acc: 0.5069 - val_loss: 0.6933 - val_acc: 0.5078\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6963 - acc: 0.4989 - val_loss: 0.6934 - val_acc: 0.5078\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6967 - acc: 0.5006 - val_loss: 0.6933 - val_acc: 0.5078\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6938 - acc: 0.5132 - val_loss: 0.6932 - val_acc: 0.5078\n",
      "Epoch 43/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6957 - acc: 0.5069 - val_loss: 0.6931 - val_acc: 0.5078\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6954 - acc: 0.5022 - val_loss: 0.6931 - val_acc: 0.5078\n",
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6955 - acc: 0.4991 - val_loss: 0.6931 - val_acc: 0.5078\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6958 - acc: 0.5063 - val_loss: 0.6931 - val_acc: 0.5078\n",
      "Epoch 47/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6945 - acc: 0.5063 - val_loss: 0.6931 - val_acc: 0.5078\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6947 - acc: 0.5028 - val_loss: 0.6931 - val_acc: 0.5078\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6958 - acc: 0.5019 - val_loss: 0.6931 - val_acc: 0.5078\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6930 - acc: 0.5168 - val_loss: 0.6931 - val_acc: 0.5078\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6960 - acc: 0.5050 - val_loss: 0.6931 - val_acc: 0.5078\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6951 - acc: 0.5086 - val_loss: 0.6932 - val_acc: 0.5078\n",
      "Epoch 53/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6947 - acc: 0.5019 - val_loss: 0.6933 - val_acc: 0.5078\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6957 - acc: 0.4974 - val_loss: 0.6933 - val_acc: 0.5078\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6927 - acc: 0.5121 - val_loss: 0.6934 - val_acc: 0.5078\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6964 - acc: 0.5078 - val_loss: 0.6934 - val_acc: 0.5078\n",
      "Epoch 57/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6939 - acc: 0.5104 - val_loss: 0.6935 - val_acc: 0.5078\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6957 - acc: 0.5088 - val_loss: 0.6935 - val_acc: 0.5078\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6947 - acc: 0.5054 - val_loss: 0.6933 - val_acc: 0.5078\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6944 - acc: 0.5013 - val_loss: 0.6932 - val_acc: 0.5078\n",
      "Epoch 61/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6947 - acc: 0.5026 - val_loss: 0.6932 - val_acc: 0.5078\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6955 - acc: 0.5060 - val_loss: 0.6932 - val_acc: 0.5078\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6939 - acc: 0.5114 - val_loss: 0.6932 - val_acc: 0.5078\n",
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6939 - acc: 0.5078 - val_loss: 0.6931 - val_acc: 0.5078\n",
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6939 - acc: 0.5129 - val_loss: 0.6931 - val_acc: 0.5078\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6935 - acc: 0.5127 - val_loss: 0.6930 - val_acc: 0.5078\n",
      "Epoch 67/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6936 - acc: 0.5114 - val_loss: 0.6930 - val_acc: 0.5078\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6947 - acc: 0.5035 - val_loss: 0.6929 - val_acc: 0.5078\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6928 - acc: 0.5162 - val_loss: 0.6928 - val_acc: 0.5078\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6930 - acc: 0.5138 - val_loss: 0.6928 - val_acc: 0.5078\n",
      "sample weight :  [4.40277233e-04 1.90936127e-05 2.29777532e-05 ... 1.93134653e-05\n",
      " 1.93962357e-04 5.37372862e-05]\n",
      "1317403.0\n",
      "new_x, new_y (6182, 10, 4068) (6182,)\n",
      "WARNING:tensorflow:Layer lstm_88 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_89 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_90 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_91 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/70\n",
      "2/2 [==============================] - 5s 537ms/step - loss: 0.7078 - acc: 0.4899 - val_loss: 0.6951 - val_acc: 0.4638\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.7017 - acc: 0.4981 - val_loss: 0.6908 - val_acc: 0.5362\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6995 - acc: 0.5071 - val_loss: 0.6908 - val_acc: 0.5362\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 0.7037 - acc: 0.5097 - val_loss: 0.6908 - val_acc: 0.5362\n",
      "Epoch 5/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.7027 - acc: 0.5073 - val_loss: 0.6905 - val_acc: 0.5362\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6992 - acc: 0.5116 - val_loss: 0.6909 - val_acc: 0.5362\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.6984 - acc: 0.5108 - val_loss: 0.6919 - val_acc: 0.5362\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.6979 - acc: 0.5080 - val_loss: 0.6928 - val_acc: 0.5362\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6979 - acc: 0.5052 - val_loss: 0.6927 - val_acc: 0.5362\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6983 - acc: 0.5045 - val_loss: 0.6920 - val_acc: 0.5362\n",
      "Epoch 11/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6995 - acc: 0.5015 - val_loss: 0.6914 - val_acc: 0.5362\n",
      "Epoch 12/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6963 - acc: 0.5112 - val_loss: 0.6910 - val_acc: 0.5362\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6989 - acc: 0.5127 - val_loss: 0.6907 - val_acc: 0.5362\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6976 - acc: 0.5108 - val_loss: 0.6906 - val_acc: 0.5362\n",
      "Epoch 15/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6970 - acc: 0.5134 - val_loss: 0.6906 - val_acc: 0.5362\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6956 - acc: 0.5129 - val_loss: 0.6907 - val_acc: 0.5362\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6970 - acc: 0.5086 - val_loss: 0.6909 - val_acc: 0.5362\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6961 - acc: 0.5056 - val_loss: 0.6913 - val_acc: 0.5362\n",
      "Epoch 19/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6959 - acc: 0.5149 - val_loss: 0.6921 - val_acc: 0.5362\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6966 - acc: 0.5043 - val_loss: 0.6926 - val_acc: 0.5362\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6976 - acc: 0.4946 - val_loss: 0.6926 - val_acc: 0.5362\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6944 - acc: 0.5101 - val_loss: 0.6925 - val_acc: 0.5362\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6950 - acc: 0.5088 - val_loss: 0.6921 - val_acc: 0.5362\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6963 - acc: 0.4978 - val_loss: 0.6916 - val_acc: 0.5362\n",
      "Epoch 25/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6953 - acc: 0.5063 - val_loss: 0.6911 - val_acc: 0.5362\n",
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6949 - acc: 0.5112 - val_loss: 0.6907 - val_acc: 0.5362\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6945 - acc: 0.5097 - val_loss: 0.6906 - val_acc: 0.5362\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6951 - acc: 0.5095 - val_loss: 0.6906 - val_acc: 0.5362\n",
      "Epoch 29/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6971 - acc: 0.5069 - val_loss: 0.6906 - val_acc: 0.5362\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6954 - acc: 0.5164 - val_loss: 0.6907 - val_acc: 0.5362\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6948 - acc: 0.5188 - val_loss: 0.6908 - val_acc: 0.5362\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6934 - acc: 0.5248 - val_loss: 0.6910 - val_acc: 0.5362\n",
      "Epoch 33/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6947 - acc: 0.5050 - val_loss: 0.6913 - val_acc: 0.5362\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6940 - acc: 0.5110 - val_loss: 0.6919 - val_acc: 0.5362\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6938 - acc: 0.5097 - val_loss: 0.6924 - val_acc: 0.5362\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6943 - acc: 0.5095 - val_loss: 0.6926 - val_acc: 0.5362\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6954 - acc: 0.4978 - val_loss: 0.6925 - val_acc: 0.5362\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6953 - acc: 0.5067 - val_loss: 0.6920 - val_acc: 0.5362\n",
      "Epoch 39/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6948 - acc: 0.5004 - val_loss: 0.6912 - val_acc: 0.5362\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6942 - acc: 0.5101 - val_loss: 0.6907 - val_acc: 0.5362\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6951 - acc: 0.5119 - val_loss: 0.6905 - val_acc: 0.5362\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6943 - acc: 0.5166 - val_loss: 0.6905 - val_acc: 0.5362\n",
      "Epoch 43/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6953 - acc: 0.5153 - val_loss: 0.6906 - val_acc: 0.5362\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6942 - acc: 0.5175 - val_loss: 0.6908 - val_acc: 0.5362\n",
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6949 - acc: 0.5050 - val_loss: 0.6912 - val_acc: 0.5362\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6935 - acc: 0.5140 - val_loss: 0.6915 - val_acc: 0.5362\n",
      "Epoch 47/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6961 - acc: 0.5009 - val_loss: 0.6916 - val_acc: 0.5362\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6955 - acc: 0.4899 - val_loss: 0.6915 - val_acc: 0.5362\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6948 - acc: 0.5026 - val_loss: 0.6915 - val_acc: 0.5362\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6943 - acc: 0.5119 - val_loss: 0.6915 - val_acc: 0.5362\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6939 - acc: 0.5032 - val_loss: 0.6915 - val_acc: 0.5362\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6932 - acc: 0.5127 - val_loss: 0.6914 - val_acc: 0.5362\n",
      "Epoch 53/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6931 - acc: 0.5186 - val_loss: 0.6914 - val_acc: 0.5362\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6932 - acc: 0.5073 - val_loss: 0.6915 - val_acc: 0.5362\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6945 - acc: 0.5026 - val_loss: 0.6915 - val_acc: 0.5362\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6939 - acc: 0.5145 - val_loss: 0.6914 - val_acc: 0.5362\n",
      "Epoch 57/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6940 - acc: 0.5035 - val_loss: 0.6912 - val_acc: 0.5362\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6937 - acc: 0.5043 - val_loss: 0.6909 - val_acc: 0.5362\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6939 - acc: 0.5140 - val_loss: 0.6907 - val_acc: 0.5362\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6939 - acc: 0.5099 - val_loss: 0.6906 - val_acc: 0.5362\n",
      "Epoch 61/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6945 - acc: 0.5078 - val_loss: 0.6907 - val_acc: 0.5362\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6926 - acc: 0.5188 - val_loss: 0.6908 - val_acc: 0.5362\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6928 - acc: 0.5080 - val_loss: 0.6909 - val_acc: 0.5362\n",
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6934 - acc: 0.5084 - val_loss: 0.6911 - val_acc: 0.5362\n",
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6935 - acc: 0.5097 - val_loss: 0.6912 - val_acc: 0.5362\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6935 - acc: 0.5080 - val_loss: 0.6915 - val_acc: 0.5362\n",
      "Epoch 67/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6939 - acc: 0.5000 - val_loss: 0.6915 - val_acc: 0.5362\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 0.6937 - acc: 0.5097 - val_loss: 0.6914 - val_acc: 0.5362\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.6942 - acc: 0.5084 - val_loss: 0.6913 - val_acc: 0.5362\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6924 - acc: 0.5175 - val_loss: 0.6912 - val_acc: 0.5362\n",
      "sample weight :  [4.26471957e-04 1.85017429e-05 2.37678879e-05 ... 1.99375345e-05\n",
      " 2.00578234e-04 5.54288806e-05]\n",
      "1316829.0\n",
      "new_x, new_y (6182, 10, 4068) (6182,)\n",
      "WARNING:tensorflow:Layer lstm_92 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_93 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_94 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_95 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/70\n",
      "2/2 [==============================] - 5s 545ms/step - loss: 0.7004 - acc: 0.5043 - val_loss: 0.6927 - val_acc: 0.5168\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.7009 - acc: 0.4877 - val_loss: 0.6928 - val_acc: 0.5168\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6978 - acc: 0.5028 - val_loss: 0.6926 - val_acc: 0.5168\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6967 - acc: 0.5121 - val_loss: 0.6928 - val_acc: 0.5168\n",
      "Epoch 5/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6989 - acc: 0.5009 - val_loss: 0.6936 - val_acc: 0.4832\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6967 - acc: 0.5041 - val_loss: 0.6947 - val_acc: 0.4832\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6967 - acc: 0.5026 - val_loss: 0.6953 - val_acc: 0.4832\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6958 - acc: 0.5170 - val_loss: 0.6951 - val_acc: 0.4832\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 0.6977 - acc: 0.4985 - val_loss: 0.6941 - val_acc: 0.4832\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 0.6971 - acc: 0.4991 - val_loss: 0.6931 - val_acc: 0.5168\n",
      "Epoch 11/70\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 0.6978 - acc: 0.4905 - val_loss: 0.6926 - val_acc: 0.5168\n",
      "Epoch 12/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6982 - acc: 0.4845 - val_loss: 0.6926 - val_acc: 0.5168\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6953 - acc: 0.5157 - val_loss: 0.6926 - val_acc: 0.5168\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6968 - acc: 0.5009 - val_loss: 0.6926 - val_acc: 0.5168\n",
      "Epoch 15/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6965 - acc: 0.4948 - val_loss: 0.6929 - val_acc: 0.5168\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6949 - acc: 0.5047 - val_loss: 0.6934 - val_acc: 0.4832\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6962 - acc: 0.5065 - val_loss: 0.6938 - val_acc: 0.4832\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6967 - acc: 0.4985 - val_loss: 0.6938 - val_acc: 0.4832\n",
      "Epoch 19/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6940 - acc: 0.5157 - val_loss: 0.6934 - val_acc: 0.4832\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.6957 - acc: 0.4974 - val_loss: 0.6929 - val_acc: 0.5168\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6956 - acc: 0.5043 - val_loss: 0.6926 - val_acc: 0.5168\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6941 - acc: 0.5116 - val_loss: 0.6926 - val_acc: 0.5168\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6960 - acc: 0.5006 - val_loss: 0.6926 - val_acc: 0.5168\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6965 - acc: 0.4931 - val_loss: 0.6926 - val_acc: 0.5168\n",
      "Epoch 25/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6958 - acc: 0.4953 - val_loss: 0.6927 - val_acc: 0.5168\n",
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6946 - acc: 0.4989 - val_loss: 0.6928 - val_acc: 0.5168\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6955 - acc: 0.4927 - val_loss: 0.6929 - val_acc: 0.5168\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6965 - acc: 0.4998 - val_loss: 0.6930 - val_acc: 0.5168\n",
      "Epoch 29/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6963 - acc: 0.4909 - val_loss: 0.6929 - val_acc: 0.5168\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6964 - acc: 0.4873 - val_loss: 0.6928 - val_acc: 0.5168\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6952 - acc: 0.4983 - val_loss: 0.6928 - val_acc: 0.5168\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6951 - acc: 0.4912 - val_loss: 0.6929 - val_acc: 0.5168\n",
      "Epoch 33/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6948 - acc: 0.4996 - val_loss: 0.6932 - val_acc: 0.4832\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6945 - acc: 0.5009 - val_loss: 0.6935 - val_acc: 0.4832\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6934 - acc: 0.5164 - val_loss: 0.6936 - val_acc: 0.4832\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6954 - acc: 0.4940 - val_loss: 0.6936 - val_acc: 0.4832\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6945 - acc: 0.4976 - val_loss: 0.6932 - val_acc: 0.4832\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6940 - acc: 0.5019 - val_loss: 0.6929 - val_acc: 0.5168\n",
      "Epoch 39/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6943 - acc: 0.4996 - val_loss: 0.6930 - val_acc: 0.5168\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6943 - acc: 0.5047 - val_loss: 0.6933 - val_acc: 0.4832\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6947 - acc: 0.4976 - val_loss: 0.6936 - val_acc: 0.4832\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6943 - acc: 0.5024 - val_loss: 0.6938 - val_acc: 0.4832\n",
      "Epoch 43/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6940 - acc: 0.5097 - val_loss: 0.6939 - val_acc: 0.4832\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6937 - acc: 0.5078 - val_loss: 0.6937 - val_acc: 0.4832\n",
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6953 - acc: 0.4871 - val_loss: 0.6933 - val_acc: 0.4832\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6943 - acc: 0.4929 - val_loss: 0.6930 - val_acc: 0.5168\n",
      "Epoch 47/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6947 - acc: 0.4983 - val_loss: 0.6929 - val_acc: 0.5168\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6937 - acc: 0.5026 - val_loss: 0.6927 - val_acc: 0.5168\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6932 - acc: 0.5073 - val_loss: 0.6926 - val_acc: 0.5168\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6940 - acc: 0.4994 - val_loss: 0.6925 - val_acc: 0.5168\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6945 - acc: 0.5000 - val_loss: 0.6925 - val_acc: 0.5168\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6939 - acc: 0.4985 - val_loss: 0.6925 - val_acc: 0.5168\n",
      "Epoch 53/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6938 - acc: 0.5045 - val_loss: 0.6926 - val_acc: 0.5168\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6939 - acc: 0.5009 - val_loss: 0.6927 - val_acc: 0.5168\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6932 - acc: 0.5136 - val_loss: 0.6929 - val_acc: 0.5168\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6941 - acc: 0.5013 - val_loss: 0.6930 - val_acc: 0.5511\n",
      "Epoch 57/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6932 - acc: 0.5037 - val_loss: 0.6929 - val_acc: 0.5168\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6925 - acc: 0.5194 - val_loss: 0.6927 - val_acc: 0.5168\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6938 - acc: 0.5002 - val_loss: 0.6926 - val_acc: 0.5168\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6940 - acc: 0.5009 - val_loss: 0.6925 - val_acc: 0.5168\n",
      "Epoch 61/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6944 - acc: 0.5019 - val_loss: 0.6924 - val_acc: 0.5168\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6937 - acc: 0.5052 - val_loss: 0.6924 - val_acc: 0.5168\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6929 - acc: 0.5106 - val_loss: 0.6924 - val_acc: 0.5168\n",
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 0.6934 - acc: 0.5067 - val_loss: 0.6924 - val_acc: 0.5168\n",
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6921 - acc: 0.5222 - val_loss: 0.6924 - val_acc: 0.5168\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6933 - acc: 0.5086 - val_loss: 0.6926 - val_acc: 0.5252\n",
      "Epoch 67/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6940 - acc: 0.4950 - val_loss: 0.6926 - val_acc: 0.5576\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6930 - acc: 0.5134 - val_loss: 0.6925 - val_acc: 0.5517\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6941 - acc: 0.5054 - val_loss: 0.6925 - val_acc: 0.5498\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6922 - acc: 0.5188 - val_loss: 0.6923 - val_acc: 0.5408\n",
      "sample weight :  [4.24451030e-04 1.84553718e-05 2.40000090e-05 ... 2.00143447e-05\n",
      " 2.02159945e-04 5.55668944e-05]\n",
      "1327477.0\n",
      "new_x, new_y (6182, 10, 4068) (6182,)\n",
      "WARNING:tensorflow:Layer lstm_96 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_97 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_98 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_99 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/70\n",
      "2/2 [==============================] - 6s 763ms/step - loss: 0.7440 - acc: 0.4920 - val_loss: 0.7144 - val_acc: 0.4845\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.7195 - acc: 0.4881 - val_loss: 0.6986 - val_acc: 0.4845\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.7007 - acc: 0.5058 - val_loss: 0.6929 - val_acc: 0.5155\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.7017 - acc: 0.4944 - val_loss: 0.6937 - val_acc: 0.5155\n",
      "Epoch 5/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.7049 - acc: 0.4963 - val_loss: 0.6958 - val_acc: 0.5155\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.7045 - acc: 0.5119 - val_loss: 0.6960 - val_acc: 0.5155\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.7052 - acc: 0.5039 - val_loss: 0.6945 - val_acc: 0.5155\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.7065 - acc: 0.4944 - val_loss: 0.6931 - val_acc: 0.5155\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 0.6968 - acc: 0.5149 - val_loss: 0.6927 - val_acc: 0.5155\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6995 - acc: 0.5086 - val_loss: 0.6932 - val_acc: 0.4845\n",
      "Epoch 11/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6964 - acc: 0.5142 - val_loss: 0.6942 - val_acc: 0.4845\n",
      "Epoch 12/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.7014 - acc: 0.4909 - val_loss: 0.6951 - val_acc: 0.4845\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.6985 - acc: 0.5104 - val_loss: 0.6955 - val_acc: 0.4845\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.7021 - acc: 0.4970 - val_loss: 0.6954 - val_acc: 0.4845\n",
      "Epoch 15/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6999 - acc: 0.4978 - val_loss: 0.6949 - val_acc: 0.4845\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6987 - acc: 0.5147 - val_loss: 0.6943 - val_acc: 0.4845\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.6991 - acc: 0.4920 - val_loss: 0.6938 - val_acc: 0.4845\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6994 - acc: 0.4978 - val_loss: 0.6933 - val_acc: 0.4845\n",
      "Epoch 19/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6975 - acc: 0.5056 - val_loss: 0.6930 - val_acc: 0.5155\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6994 - acc: 0.4922 - val_loss: 0.6928 - val_acc: 0.5155\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6974 - acc: 0.5069 - val_loss: 0.6927 - val_acc: 0.5155\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6998 - acc: 0.4929 - val_loss: 0.6927 - val_acc: 0.5155\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.7005 - acc: 0.4935 - val_loss: 0.6927 - val_acc: 0.5155\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6971 - acc: 0.4985 - val_loss: 0.6927 - val_acc: 0.5155\n",
      "Epoch 25/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6948 - acc: 0.5086 - val_loss: 0.6929 - val_acc: 0.5155\n",
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6967 - acc: 0.5099 - val_loss: 0.6932 - val_acc: 0.4845\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6975 - acc: 0.4944 - val_loss: 0.6934 - val_acc: 0.4845\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6964 - acc: 0.5058 - val_loss: 0.6935 - val_acc: 0.4845\n",
      "Epoch 29/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6968 - acc: 0.5127 - val_loss: 0.6933 - val_acc: 0.4845\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6969 - acc: 0.5082 - val_loss: 0.6931 - val_acc: 0.5155\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6965 - acc: 0.5056 - val_loss: 0.6930 - val_acc: 0.5155\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.7003 - acc: 0.4892 - val_loss: 0.6929 - val_acc: 0.5155\n",
      "Epoch 33/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6993 - acc: 0.4819 - val_loss: 0.6928 - val_acc: 0.5155\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6978 - acc: 0.4884 - val_loss: 0.6929 - val_acc: 0.5155\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6966 - acc: 0.4933 - val_loss: 0.6930 - val_acc: 0.5155\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6965 - acc: 0.4987 - val_loss: 0.6930 - val_acc: 0.5155\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6968 - acc: 0.5037 - val_loss: 0.6930 - val_acc: 0.5155\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6966 - acc: 0.4961 - val_loss: 0.6929 - val_acc: 0.5155\n",
      "Epoch 39/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6972 - acc: 0.4948 - val_loss: 0.6929 - val_acc: 0.5155\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6979 - acc: 0.5015 - val_loss: 0.6929 - val_acc: 0.5155\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6962 - acc: 0.5041 - val_loss: 0.6929 - val_acc: 0.5155\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6985 - acc: 0.4845 - val_loss: 0.6929 - val_acc: 0.5155\n",
      "Epoch 43/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6957 - acc: 0.5056 - val_loss: 0.6928 - val_acc: 0.5155\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6955 - acc: 0.5075 - val_loss: 0.6927 - val_acc: 0.5155\n",
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6968 - acc: 0.4884 - val_loss: 0.6926 - val_acc: 0.5155\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6952 - acc: 0.5136 - val_loss: 0.6926 - val_acc: 0.5155\n",
      "Epoch 47/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6971 - acc: 0.4858 - val_loss: 0.6927 - val_acc: 0.5155\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6948 - acc: 0.5116 - val_loss: 0.6928 - val_acc: 0.5155\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6940 - acc: 0.5088 - val_loss: 0.6929 - val_acc: 0.5155\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6960 - acc: 0.5065 - val_loss: 0.6930 - val_acc: 0.5155\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.6955 - acc: 0.5078 - val_loss: 0.6931 - val_acc: 0.5498\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6953 - acc: 0.4994 - val_loss: 0.6932 - val_acc: 0.4845\n",
      "Epoch 53/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6945 - acc: 0.5019 - val_loss: 0.6932 - val_acc: 0.4845\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6963 - acc: 0.4976 - val_loss: 0.6931 - val_acc: 0.4845\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6955 - acc: 0.4959 - val_loss: 0.6929 - val_acc: 0.5155\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6962 - acc: 0.4996 - val_loss: 0.6927 - val_acc: 0.5155\n",
      "Epoch 57/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6961 - acc: 0.4946 - val_loss: 0.6926 - val_acc: 0.5155\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6949 - acc: 0.5091 - val_loss: 0.6926 - val_acc: 0.5155\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6964 - acc: 0.4884 - val_loss: 0.6927 - val_acc: 0.5155\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6952 - acc: 0.5069 - val_loss: 0.6927 - val_acc: 0.5155\n",
      "Epoch 61/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6965 - acc: 0.4950 - val_loss: 0.6926 - val_acc: 0.5155\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6947 - acc: 0.5052 - val_loss: 0.6926 - val_acc: 0.5155\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6946 - acc: 0.5093 - val_loss: 0.6926 - val_acc: 0.5155\n",
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6952 - acc: 0.4955 - val_loss: 0.6928 - val_acc: 0.5155\n",
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6939 - acc: 0.5056 - val_loss: 0.6929 - val_acc: 0.5155\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6952 - acc: 0.5086 - val_loss: 0.6930 - val_acc: 0.5155\n",
      "Epoch 67/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6946 - acc: 0.5004 - val_loss: 0.6931 - val_acc: 0.4845\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6944 - acc: 0.4948 - val_loss: 0.6932 - val_acc: 0.4845\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6950 - acc: 0.5002 - val_loss: 0.6930 - val_acc: 0.5589\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6945 - acc: 0.4976 - val_loss: 0.6928 - val_acc: 0.5155\n",
      "sample weight :  [4.27955995e-04 1.85937927e-05 2.38990533e-05 ... 1.98297737e-05\n",
      " 2.00495104e-04 5.50854553e-05]\n",
      "1324900.0\n",
      "new_x, new_y (6182, 10, 4068) (6182,)\n",
      "WARNING:tensorflow:Layer lstm_100 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_101 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_102 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_103 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/70\n",
      "2/2 [==============================] - 7s 536ms/step - loss: 0.7139 - acc: 0.5004 - val_loss: 0.6957 - val_acc: 0.4987\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.7047 - acc: 0.4890 - val_loss: 0.6932 - val_acc: 0.5013\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 156ms/step - loss: 0.7029 - acc: 0.4933 - val_loss: 0.6951 - val_acc: 0.5013\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 0s 159ms/step - loss: 0.7014 - acc: 0.5011 - val_loss: 0.6959 - val_acc: 0.5013\n",
      "Epoch 5/70\n",
      "2/2 [==============================] - 0s 156ms/step - loss: 0.7011 - acc: 0.5019 - val_loss: 0.6953 - val_acc: 0.5013\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 159ms/step - loss: 0.7014 - acc: 0.5116 - val_loss: 0.6940 - val_acc: 0.5013\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 158ms/step - loss: 0.7003 - acc: 0.4937 - val_loss: 0.6933 - val_acc: 0.5013\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6999 - acc: 0.4953 - val_loss: 0.6931 - val_acc: 0.5013\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 204ms/step - loss: 0.6987 - acc: 0.4998 - val_loss: 0.6932 - val_acc: 0.4987\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6981 - acc: 0.4905 - val_loss: 0.6933 - val_acc: 0.4987\n",
      "Epoch 11/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6976 - acc: 0.5035 - val_loss: 0.6932 - val_acc: 0.4987\n",
      "Epoch 12/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.7004 - acc: 0.4855 - val_loss: 0.6932 - val_acc: 0.4987\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6978 - acc: 0.5095 - val_loss: 0.6932 - val_acc: 0.4987\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.7001 - acc: 0.4821 - val_loss: 0.6931 - val_acc: 0.5013\n",
      "Epoch 15/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6952 - acc: 0.5155 - val_loss: 0.6932 - val_acc: 0.5013\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6980 - acc: 0.4877 - val_loss: 0.6933 - val_acc: 0.5013\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6965 - acc: 0.5024 - val_loss: 0.6933 - val_acc: 0.5013\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6967 - acc: 0.5009 - val_loss: 0.6932 - val_acc: 0.5013\n",
      "Epoch 19/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6972 - acc: 0.4996 - val_loss: 0.6932 - val_acc: 0.5013\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 153ms/step - loss: 0.6975 - acc: 0.4884 - val_loss: 0.6931 - val_acc: 0.5013\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6966 - acc: 0.5060 - val_loss: 0.6931 - val_acc: 0.5013\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6975 - acc: 0.4927 - val_loss: 0.6931 - val_acc: 0.5013\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6968 - acc: 0.4965 - val_loss: 0.6931 - val_acc: 0.5013\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6966 - acc: 0.4944 - val_loss: 0.6931 - val_acc: 0.5013\n",
      "Epoch 25/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6980 - acc: 0.4860 - val_loss: 0.6931 - val_acc: 0.5013\n",
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6966 - acc: 0.4922 - val_loss: 0.6931 - val_acc: 0.5013\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6946 - acc: 0.5056 - val_loss: 0.6931 - val_acc: 0.5013\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6977 - acc: 0.4875 - val_loss: 0.6932 - val_acc: 0.5013\n",
      "Epoch 29/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6944 - acc: 0.5088 - val_loss: 0.6932 - val_acc: 0.5013\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6954 - acc: 0.5000 - val_loss: 0.6933 - val_acc: 0.5013\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6943 - acc: 0.5039 - val_loss: 0.6932 - val_acc: 0.5013\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6965 - acc: 0.4944 - val_loss: 0.6932 - val_acc: 0.5013\n",
      "Epoch 33/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6945 - acc: 0.5091 - val_loss: 0.6931 - val_acc: 0.5013\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6958 - acc: 0.5032 - val_loss: 0.6931 - val_acc: 0.5013\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6940 - acc: 0.5058 - val_loss: 0.6931 - val_acc: 0.4987\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6953 - acc: 0.4955 - val_loss: 0.6932 - val_acc: 0.4987\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6943 - acc: 0.5045 - val_loss: 0.6932 - val_acc: 0.4987\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6956 - acc: 0.4886 - val_loss: 0.6931 - val_acc: 0.4987\n",
      "Epoch 39/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6944 - acc: 0.4998 - val_loss: 0.6931 - val_acc: 0.5013\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6949 - acc: 0.5073 - val_loss: 0.6931 - val_acc: 0.5013\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6948 - acc: 0.5032 - val_loss: 0.6931 - val_acc: 0.5013\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6939 - acc: 0.5017 - val_loss: 0.6931 - val_acc: 0.5013\n",
      "Epoch 43/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6956 - acc: 0.4903 - val_loss: 0.6932 - val_acc: 0.5013\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6943 - acc: 0.5060 - val_loss: 0.6933 - val_acc: 0.5013\n",
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6944 - acc: 0.5019 - val_loss: 0.6933 - val_acc: 0.5013\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6946 - acc: 0.4976 - val_loss: 0.6932 - val_acc: 0.5013\n",
      "Epoch 47/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6940 - acc: 0.5015 - val_loss: 0.6932 - val_acc: 0.5013\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.6941 - acc: 0.5035 - val_loss: 0.6932 - val_acc: 0.5013\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6940 - acc: 0.5073 - val_loss: 0.6931 - val_acc: 0.5013\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6943 - acc: 0.4925 - val_loss: 0.6931 - val_acc: 0.5013\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6942 - acc: 0.5002 - val_loss: 0.6931 - val_acc: 0.5013\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6942 - acc: 0.5050 - val_loss: 0.6931 - val_acc: 0.5013\n",
      "Epoch 53/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6943 - acc: 0.4976 - val_loss: 0.6931 - val_acc: 0.5013\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6939 - acc: 0.5028 - val_loss: 0.6931 - val_acc: 0.5013\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6935 - acc: 0.5037 - val_loss: 0.6931 - val_acc: 0.4987\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6942 - acc: 0.5015 - val_loss: 0.6931 - val_acc: 0.4987\n",
      "Epoch 57/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6937 - acc: 0.5075 - val_loss: 0.6931 - val_acc: 0.4987\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6946 - acc: 0.4985 - val_loss: 0.6931 - val_acc: 0.5175\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6928 - acc: 0.5151 - val_loss: 0.6931 - val_acc: 0.5013\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6924 - acc: 0.5209 - val_loss: 0.6931 - val_acc: 0.5013\n",
      "Epoch 61/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6934 - acc: 0.5119 - val_loss: 0.6931 - val_acc: 0.5013\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6935 - acc: 0.4994 - val_loss: 0.6931 - val_acc: 0.5013\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6946 - acc: 0.5002 - val_loss: 0.6931 - val_acc: 0.5013\n",
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6944 - acc: 0.4974 - val_loss: 0.6931 - val_acc: 0.5013\n",
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6930 - acc: 0.5071 - val_loss: 0.6930 - val_acc: 0.5013\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6942 - acc: 0.4940 - val_loss: 0.6930 - val_acc: 0.5252\n",
      "Epoch 67/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6941 - acc: 0.4944 - val_loss: 0.6930 - val_acc: 0.5168\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6940 - acc: 0.4985 - val_loss: 0.6930 - val_acc: 0.5201\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6940 - acc: 0.4925 - val_loss: 0.6929 - val_acc: 0.5078\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6942 - acc: 0.5006 - val_loss: 0.6929 - val_acc: 0.5026\n",
      "sample weight :  [4.31842206e-04 1.86909389e-05 2.38407575e-05 ... 1.96315612e-05\n",
      " 1.98484223e-04 5.44578905e-05]\n",
      "1305828.0\n",
      "new_x, new_y (6182, 10, 4068) (6182,)\n",
      "WARNING:tensorflow:Layer lstm_104 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_105 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_106 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_107 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/70\n",
      "2/2 [==============================] - 6s 542ms/step - loss: 0.7028 - acc: 0.4950 - val_loss: 0.6959 - val_acc: 0.4909\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6996 - acc: 0.5080 - val_loss: 0.6982 - val_acc: 0.4909\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.7007 - acc: 0.5004 - val_loss: 0.6964 - val_acc: 0.4909\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.7002 - acc: 0.5082 - val_loss: 0.6945 - val_acc: 0.4909\n",
      "Epoch 5/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6982 - acc: 0.5056 - val_loss: 0.6933 - val_acc: 0.4909\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.7006 - acc: 0.4931 - val_loss: 0.6931 - val_acc: 0.5091\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6995 - acc: 0.4922 - val_loss: 0.6930 - val_acc: 0.5091\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6980 - acc: 0.5088 - val_loss: 0.6932 - val_acc: 0.4909\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6972 - acc: 0.5058 - val_loss: 0.6938 - val_acc: 0.4909\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6968 - acc: 0.5086 - val_loss: 0.6944 - val_acc: 0.4909\n",
      "Epoch 11/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6987 - acc: 0.4953 - val_loss: 0.6944 - val_acc: 0.4909\n",
      "Epoch 12/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6982 - acc: 0.4963 - val_loss: 0.6942 - val_acc: 0.4909\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6962 - acc: 0.5028 - val_loss: 0.6940 - val_acc: 0.4909\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6948 - acc: 0.5138 - val_loss: 0.6938 - val_acc: 0.4909\n",
      "Epoch 15/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6971 - acc: 0.5024 - val_loss: 0.6938 - val_acc: 0.4909\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6961 - acc: 0.4987 - val_loss: 0.6939 - val_acc: 0.4909\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6960 - acc: 0.5082 - val_loss: 0.6938 - val_acc: 0.4909\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6964 - acc: 0.5110 - val_loss: 0.6941 - val_acc: 0.4909\n",
      "Epoch 19/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6951 - acc: 0.5170 - val_loss: 0.6953 - val_acc: 0.4909\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.7004 - acc: 0.4946 - val_loss: 0.6966 - val_acc: 0.4909\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6963 - acc: 0.5116 - val_loss: 0.6969 - val_acc: 0.4909\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6966 - acc: 0.5017 - val_loss: 0.6964 - val_acc: 0.4909\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6975 - acc: 0.5104 - val_loss: 0.6956 - val_acc: 0.4909\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6940 - acc: 0.5153 - val_loss: 0.6946 - val_acc: 0.4909\n",
      "Epoch 25/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6955 - acc: 0.4985 - val_loss: 0.6937 - val_acc: 0.4909\n",
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6953 - acc: 0.5056 - val_loss: 0.6934 - val_acc: 0.4909\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6984 - acc: 0.4886 - val_loss: 0.6934 - val_acc: 0.4909\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6964 - acc: 0.5000 - val_loss: 0.6935 - val_acc: 0.4909\n",
      "Epoch 29/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6948 - acc: 0.5017 - val_loss: 0.6938 - val_acc: 0.4909\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6936 - acc: 0.5069 - val_loss: 0.6943 - val_acc: 0.4909\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6957 - acc: 0.5045 - val_loss: 0.6948 - val_acc: 0.4909\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6955 - acc: 0.5013 - val_loss: 0.6951 - val_acc: 0.4909\n",
      "Epoch 33/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6941 - acc: 0.5175 - val_loss: 0.6948 - val_acc: 0.4909\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6940 - acc: 0.5119 - val_loss: 0.6941 - val_acc: 0.4909\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6925 - acc: 0.5224 - val_loss: 0.6935 - val_acc: 0.4909\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6945 - acc: 0.5013 - val_loss: 0.6933 - val_acc: 0.4909\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6955 - acc: 0.4918 - val_loss: 0.6934 - val_acc: 0.4909\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6945 - acc: 0.5110 - val_loss: 0.6936 - val_acc: 0.4909\n",
      "Epoch 39/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6932 - acc: 0.5162 - val_loss: 0.6939 - val_acc: 0.4909\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6933 - acc: 0.5160 - val_loss: 0.6943 - val_acc: 0.4909\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6946 - acc: 0.5065 - val_loss: 0.6944 - val_acc: 0.4909\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6953 - acc: 0.4959 - val_loss: 0.6943 - val_acc: 0.4909\n",
      "Epoch 43/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6917 - acc: 0.5244 - val_loss: 0.6939 - val_acc: 0.4909\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6943 - acc: 0.5138 - val_loss: 0.6939 - val_acc: 0.4909\n",
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6938 - acc: 0.5050 - val_loss: 0.6939 - val_acc: 0.4909\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6947 - acc: 0.5006 - val_loss: 0.6937 - val_acc: 0.4909\n",
      "Epoch 47/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6938 - acc: 0.5082 - val_loss: 0.6936 - val_acc: 0.4909\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6947 - acc: 0.5011 - val_loss: 0.6935 - val_acc: 0.4909\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6944 - acc: 0.4965 - val_loss: 0.6935 - val_acc: 0.4909\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6931 - acc: 0.5136 - val_loss: 0.6937 - val_acc: 0.4909\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6942 - acc: 0.5069 - val_loss: 0.6937 - val_acc: 0.4909\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6953 - acc: 0.5024 - val_loss: 0.6938 - val_acc: 0.4909\n",
      "Epoch 53/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6920 - acc: 0.5147 - val_loss: 0.6941 - val_acc: 0.4909\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6940 - acc: 0.5039 - val_loss: 0.6946 - val_acc: 0.4909\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6924 - acc: 0.5237 - val_loss: 0.6955 - val_acc: 0.4909\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.6936 - acc: 0.5129 - val_loss: 0.6966 - val_acc: 0.4909\n",
      "Epoch 57/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6939 - acc: 0.5149 - val_loss: 0.6967 - val_acc: 0.4909\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6942 - acc: 0.5112 - val_loss: 0.6959 - val_acc: 0.4909\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.6939 - acc: 0.5186 - val_loss: 0.6951 - val_acc: 0.4909\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6929 - acc: 0.5164 - val_loss: 0.6945 - val_acc: 0.4909\n",
      "Epoch 61/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6931 - acc: 0.5067 - val_loss: 0.6938 - val_acc: 0.4909\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6926 - acc: 0.5198 - val_loss: 0.6934 - val_acc: 0.4909\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6921 - acc: 0.5121 - val_loss: 0.6933 - val_acc: 0.4909\n",
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6929 - acc: 0.5207 - val_loss: 0.6932 - val_acc: 0.4909\n",
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6919 - acc: 0.5188 - val_loss: 0.6928 - val_acc: 0.4948\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6901 - acc: 0.5410 - val_loss: 0.6925 - val_acc: 0.4955\n",
      "Epoch 67/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6906 - acc: 0.5347 - val_loss: 0.6924 - val_acc: 0.4987\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6897 - acc: 0.5315 - val_loss: 0.6921 - val_acc: 0.4981\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6894 - acc: 0.5339 - val_loss: 0.6912 - val_acc: 0.5091\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6868 - acc: 0.5563 - val_loss: 0.6904 - val_acc: 0.5220\n",
      "sample weight :  [4.27053007e-04 1.82745566e-05 2.59692612e-05 ... 2.04015589e-05\n",
      " 2.01133134e-04 5.52989075e-05]\n",
      "1313278.0\n",
      "new_x, new_y (6182, 10, 4068) (6182,)\n",
      "WARNING:tensorflow:Layer lstm_108 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_109 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_110 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_111 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/70\n",
      "2/2 [==============================] - 5s 672ms/step - loss: 0.7471 - acc: 0.4907 - val_loss: 0.7272 - val_acc: 0.4709\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.7245 - acc: 0.4912 - val_loss: 0.7081 - val_acc: 0.4709\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.7077 - acc: 0.4890 - val_loss: 0.6963 - val_acc: 0.4709\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6995 - acc: 0.4989 - val_loss: 0.6916 - val_acc: 0.5291\n",
      "Epoch 5/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6992 - acc: 0.5004 - val_loss: 0.6925 - val_acc: 0.5291\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.7039 - acc: 0.5013 - val_loss: 0.6937 - val_acc: 0.5291\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 0.7045 - acc: 0.5091 - val_loss: 0.6932 - val_acc: 0.5291\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.7031 - acc: 0.5063 - val_loss: 0.6921 - val_acc: 0.5291\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.7018 - acc: 0.4981 - val_loss: 0.6915 - val_acc: 0.5291\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.7000 - acc: 0.4953 - val_loss: 0.6915 - val_acc: 0.5291\n",
      "Epoch 11/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6981 - acc: 0.5093 - val_loss: 0.6920 - val_acc: 0.5291\n",
      "Epoch 12/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6979 - acc: 0.5050 - val_loss: 0.6927 - val_acc: 0.5291\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.7003 - acc: 0.4991 - val_loss: 0.6935 - val_acc: 0.4709\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6994 - acc: 0.4957 - val_loss: 0.6941 - val_acc: 0.4709\n",
      "Epoch 15/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6972 - acc: 0.5011 - val_loss: 0.6944 - val_acc: 0.4709\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6955 - acc: 0.5125 - val_loss: 0.6943 - val_acc: 0.4709\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6988 - acc: 0.4881 - val_loss: 0.6939 - val_acc: 0.4709\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6964 - acc: 0.5043 - val_loss: 0.6935 - val_acc: 0.4709\n",
      "Epoch 19/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6971 - acc: 0.5056 - val_loss: 0.6931 - val_acc: 0.5291\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6984 - acc: 0.4955 - val_loss: 0.6927 - val_acc: 0.5291\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6969 - acc: 0.4922 - val_loss: 0.6925 - val_acc: 0.5291\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6945 - acc: 0.5110 - val_loss: 0.6924 - val_acc: 0.5291\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6971 - acc: 0.5009 - val_loss: 0.6922 - val_acc: 0.5291\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6959 - acc: 0.5022 - val_loss: 0.6921 - val_acc: 0.5291\n",
      "Epoch 25/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6962 - acc: 0.5039 - val_loss: 0.6920 - val_acc: 0.5291\n",
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6955 - acc: 0.5032 - val_loss: 0.6919 - val_acc: 0.5291\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6972 - acc: 0.5015 - val_loss: 0.6919 - val_acc: 0.5291\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6975 - acc: 0.4959 - val_loss: 0.6920 - val_acc: 0.5291\n",
      "Epoch 29/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6981 - acc: 0.4948 - val_loss: 0.6923 - val_acc: 0.5291\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6971 - acc: 0.4955 - val_loss: 0.6924 - val_acc: 0.5291\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6971 - acc: 0.4968 - val_loss: 0.6924 - val_acc: 0.5291\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6977 - acc: 0.5011 - val_loss: 0.6924 - val_acc: 0.5291\n",
      "Epoch 33/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6953 - acc: 0.5136 - val_loss: 0.6923 - val_acc: 0.5291\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6946 - acc: 0.5030 - val_loss: 0.6922 - val_acc: 0.5291\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6937 - acc: 0.5082 - val_loss: 0.6921 - val_acc: 0.5291\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6979 - acc: 0.5015 - val_loss: 0.6919 - val_acc: 0.5291\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6954 - acc: 0.5017 - val_loss: 0.6918 - val_acc: 0.5291\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6955 - acc: 0.5039 - val_loss: 0.6918 - val_acc: 0.5291\n",
      "Epoch 39/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6957 - acc: 0.4905 - val_loss: 0.6918 - val_acc: 0.5291\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6951 - acc: 0.5080 - val_loss: 0.6919 - val_acc: 0.5291\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6949 - acc: 0.5043 - val_loss: 0.6921 - val_acc: 0.5291\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6956 - acc: 0.5015 - val_loss: 0.6924 - val_acc: 0.5291\n",
      "Epoch 43/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6970 - acc: 0.4918 - val_loss: 0.6927 - val_acc: 0.5291\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6978 - acc: 0.4901 - val_loss: 0.6928 - val_acc: 0.5291\n",
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6957 - acc: 0.4950 - val_loss: 0.6927 - val_acc: 0.5291\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6935 - acc: 0.5078 - val_loss: 0.6927 - val_acc: 0.5291\n",
      "Epoch 47/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6949 - acc: 0.5028 - val_loss: 0.6925 - val_acc: 0.5291\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6957 - acc: 0.4896 - val_loss: 0.6924 - val_acc: 0.5291\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6953 - acc: 0.5037 - val_loss: 0.6922 - val_acc: 0.5291\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6940 - acc: 0.5017 - val_loss: 0.6919 - val_acc: 0.5291\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6947 - acc: 0.5060 - val_loss: 0.6918 - val_acc: 0.5291\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6952 - acc: 0.4985 - val_loss: 0.6917 - val_acc: 0.5291\n",
      "Epoch 53/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6943 - acc: 0.5075 - val_loss: 0.6918 - val_acc: 0.5291\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6945 - acc: 0.5140 - val_loss: 0.6919 - val_acc: 0.5291\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6933 - acc: 0.5153 - val_loss: 0.6922 - val_acc: 0.5291\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6960 - acc: 0.5041 - val_loss: 0.6924 - val_acc: 0.5291\n",
      "Epoch 57/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6959 - acc: 0.4957 - val_loss: 0.6925 - val_acc: 0.5291\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6945 - acc: 0.5004 - val_loss: 0.6923 - val_acc: 0.5291\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6948 - acc: 0.4968 - val_loss: 0.6922 - val_acc: 0.5291\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6955 - acc: 0.5047 - val_loss: 0.6921 - val_acc: 0.5291\n",
      "Epoch 61/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6933 - acc: 0.5041 - val_loss: 0.6922 - val_acc: 0.5291\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6945 - acc: 0.5039 - val_loss: 0.6924 - val_acc: 0.5291\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6934 - acc: 0.5142 - val_loss: 0.6926 - val_acc: 0.5291\n",
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6951 - acc: 0.4929 - val_loss: 0.6926 - val_acc: 0.5291\n",
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6955 - acc: 0.4903 - val_loss: 0.6924 - val_acc: 0.5291\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6945 - acc: 0.5080 - val_loss: 0.6923 - val_acc: 0.5291\n",
      "Epoch 67/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6950 - acc: 0.4978 - val_loss: 0.6921 - val_acc: 0.5291\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6942 - acc: 0.5082 - val_loss: 0.6920 - val_acc: 0.5291\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6952 - acc: 0.4944 - val_loss: 0.6919 - val_acc: 0.5291\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6951 - acc: 0.4983 - val_loss: 0.6919 - val_acc: 0.5291\n",
      "sample weight :  [4.38256039e-04 1.87566621e-05 2.53561193e-05 ... 1.98739770e-05\n",
      " 1.96475558e-04 5.38643605e-05]\n",
      "1318772.0\n",
      "new_x, new_y (6182, 10, 4068) (6182,)\n",
      "WARNING:tensorflow:Layer lstm_112 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_113 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_114 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_115 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/70\n",
      "2/2 [==============================] - 5s 690ms/step - loss: 0.7582 - acc: 0.4940 - val_loss: 0.7276 - val_acc: 0.4877\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.7323 - acc: 0.4931 - val_loss: 0.7070 - val_acc: 0.4877\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.7113 - acc: 0.4976 - val_loss: 0.6960 - val_acc: 0.4877\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.7017 - acc: 0.4965 - val_loss: 0.6928 - val_acc: 0.5123\n",
      "Epoch 5/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6988 - acc: 0.4989 - val_loss: 0.6944 - val_acc: 0.5123\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.7025 - acc: 0.5065 - val_loss: 0.6964 - val_acc: 0.5123\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.7064 - acc: 0.4994 - val_loss: 0.6963 - val_acc: 0.5123\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.7011 - acc: 0.5060 - val_loss: 0.6949 - val_acc: 0.5123\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.7018 - acc: 0.5032 - val_loss: 0.6935 - val_acc: 0.5123\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6983 - acc: 0.5080 - val_loss: 0.6929 - val_acc: 0.5123\n",
      "Epoch 11/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6989 - acc: 0.5004 - val_loss: 0.6929 - val_acc: 0.5123\n",
      "Epoch 12/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.7003 - acc: 0.4961 - val_loss: 0.6933 - val_acc: 0.4877\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6973 - acc: 0.5043 - val_loss: 0.6937 - val_acc: 0.4877\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.7027 - acc: 0.4845 - val_loss: 0.6941 - val_acc: 0.4877\n",
      "Epoch 15/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.7012 - acc: 0.4987 - val_loss: 0.6943 - val_acc: 0.4877\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6999 - acc: 0.4963 - val_loss: 0.6943 - val_acc: 0.4877\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.7004 - acc: 0.4989 - val_loss: 0.6942 - val_acc: 0.4877\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6990 - acc: 0.4985 - val_loss: 0.6939 - val_acc: 0.4877\n",
      "Epoch 19/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6993 - acc: 0.5032 - val_loss: 0.6935 - val_acc: 0.4877\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6997 - acc: 0.4985 - val_loss: 0.6932 - val_acc: 0.4877\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6979 - acc: 0.5060 - val_loss: 0.6930 - val_acc: 0.5123\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6984 - acc: 0.5011 - val_loss: 0.6929 - val_acc: 0.5123\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6987 - acc: 0.4976 - val_loss: 0.6929 - val_acc: 0.5123\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6977 - acc: 0.5013 - val_loss: 0.6929 - val_acc: 0.5123\n",
      "Epoch 25/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6991 - acc: 0.4961 - val_loss: 0.6929 - val_acc: 0.5123\n",
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6966 - acc: 0.5080 - val_loss: 0.6929 - val_acc: 0.5123\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6996 - acc: 0.4903 - val_loss: 0.6929 - val_acc: 0.5123\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6974 - acc: 0.4918 - val_loss: 0.6928 - val_acc: 0.5123\n",
      "Epoch 29/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.7004 - acc: 0.4847 - val_loss: 0.6928 - val_acc: 0.5123\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6982 - acc: 0.4953 - val_loss: 0.6928 - val_acc: 0.5123\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6982 - acc: 0.4974 - val_loss: 0.6928 - val_acc: 0.5123\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6959 - acc: 0.4974 - val_loss: 0.6929 - val_acc: 0.5123\n",
      "Epoch 33/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6967 - acc: 0.4983 - val_loss: 0.6929 - val_acc: 0.5123\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6965 - acc: 0.5086 - val_loss: 0.6929 - val_acc: 0.5123\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6975 - acc: 0.4948 - val_loss: 0.6929 - val_acc: 0.5123\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6962 - acc: 0.4974 - val_loss: 0.6929 - val_acc: 0.5123\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6968 - acc: 0.4955 - val_loss: 0.6930 - val_acc: 0.5123\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6973 - acc: 0.4914 - val_loss: 0.6930 - val_acc: 0.5123\n",
      "Epoch 39/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6951 - acc: 0.5054 - val_loss: 0.6931 - val_acc: 0.5123\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6968 - acc: 0.4937 - val_loss: 0.6930 - val_acc: 0.5123\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6969 - acc: 0.5043 - val_loss: 0.6929 - val_acc: 0.5123\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6952 - acc: 0.5112 - val_loss: 0.6928 - val_acc: 0.5123\n",
      "Epoch 43/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6958 - acc: 0.5045 - val_loss: 0.6928 - val_acc: 0.5123\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6945 - acc: 0.5091 - val_loss: 0.6928 - val_acc: 0.5123\n",
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6955 - acc: 0.5002 - val_loss: 0.6928 - val_acc: 0.5123\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6978 - acc: 0.4996 - val_loss: 0.6928 - val_acc: 0.5123\n",
      "Epoch 47/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6959 - acc: 0.5013 - val_loss: 0.6928 - val_acc: 0.5123\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6960 - acc: 0.4974 - val_loss: 0.6929 - val_acc: 0.5123\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6932 - acc: 0.5205 - val_loss: 0.6929 - val_acc: 0.5123\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6973 - acc: 0.4957 - val_loss: 0.6930 - val_acc: 0.5123\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6960 - acc: 0.4955 - val_loss: 0.6930 - val_acc: 0.5123\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6952 - acc: 0.4976 - val_loss: 0.6928 - val_acc: 0.5123\n",
      "Epoch 53/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6958 - acc: 0.4933 - val_loss: 0.6928 - val_acc: 0.5123\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6923 - acc: 0.5209 - val_loss: 0.6928 - val_acc: 0.5123\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6962 - acc: 0.5015 - val_loss: 0.6928 - val_acc: 0.5123\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6955 - acc: 0.5078 - val_loss: 0.6928 - val_acc: 0.5123\n",
      "Epoch 57/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6952 - acc: 0.5030 - val_loss: 0.6929 - val_acc: 0.5123\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6964 - acc: 0.4890 - val_loss: 0.6929 - val_acc: 0.5123\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6955 - acc: 0.5101 - val_loss: 0.6928 - val_acc: 0.5123\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6939 - acc: 0.5082 - val_loss: 0.6929 - val_acc: 0.5123\n",
      "Epoch 61/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6964 - acc: 0.4983 - val_loss: 0.6929 - val_acc: 0.5123\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6945 - acc: 0.5011 - val_loss: 0.6930 - val_acc: 0.5129\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6963 - acc: 0.4963 - val_loss: 0.6930 - val_acc: 0.5259\n",
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6962 - acc: 0.4976 - val_loss: 0.6929 - val_acc: 0.5123\n",
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6944 - acc: 0.5086 - val_loss: 0.6929 - val_acc: 0.5123\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6942 - acc: 0.5043 - val_loss: 0.6928 - val_acc: 0.5123\n",
      "Epoch 67/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6951 - acc: 0.4996 - val_loss: 0.6928 - val_acc: 0.5123\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 0.6938 - acc: 0.5088 - val_loss: 0.6927 - val_acc: 0.5123\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 0.6948 - acc: 0.5060 - val_loss: 0.6927 - val_acc: 0.5123\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.6923 - acc: 0.5177 - val_loss: 0.6927 - val_acc: 0.5123\n",
      "sample weight :  [4.25094863e-04 1.81707926e-05 2.61435335e-05 ... 2.04968490e-05\n",
      " 2.02405256e-04 5.53485563e-05]\n",
      "1304279.0\n",
      "new_x, new_y (6182, 10, 4068) (6182,)\n",
      "WARNING:tensorflow:Layer lstm_116 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_117 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_118 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_119 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/70\n",
      "2/2 [==============================] - 5s 535ms/step - loss: 0.7148 - acc: 0.5198 - val_loss: 0.6941 - val_acc: 0.5252\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.7035 - acc: 0.5082 - val_loss: 0.6921 - val_acc: 0.5252\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.7026 - acc: 0.4978 - val_loss: 0.6956 - val_acc: 0.4748\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.7004 - acc: 0.5002 - val_loss: 0.6982 - val_acc: 0.4748\n",
      "Epoch 5/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.7064 - acc: 0.4866 - val_loss: 0.6978 - val_acc: 0.4748\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.7021 - acc: 0.4965 - val_loss: 0.6955 - val_acc: 0.4748\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.6999 - acc: 0.4994 - val_loss: 0.6931 - val_acc: 0.5252\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6999 - acc: 0.4983 - val_loss: 0.6920 - val_acc: 0.5252\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.7005 - acc: 0.4970 - val_loss: 0.6920 - val_acc: 0.5252\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6974 - acc: 0.5127 - val_loss: 0.6925 - val_acc: 0.5252\n",
      "Epoch 11/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6990 - acc: 0.5123 - val_loss: 0.6925 - val_acc: 0.5252\n",
      "Epoch 12/70\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.7008 - acc: 0.5084 - val_loss: 0.6921 - val_acc: 0.5252\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.7002 - acc: 0.5030 - val_loss: 0.6919 - val_acc: 0.5252\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6987 - acc: 0.5006 - val_loss: 0.6920 - val_acc: 0.5252\n",
      "Epoch 15/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6987 - acc: 0.4994 - val_loss: 0.6925 - val_acc: 0.5252\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6996 - acc: 0.4933 - val_loss: 0.6930 - val_acc: 0.5252\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6984 - acc: 0.4976 - val_loss: 0.6935 - val_acc: 0.4748\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6982 - acc: 0.5026 - val_loss: 0.6938 - val_acc: 0.4748\n",
      "Epoch 19/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6956 - acc: 0.5095 - val_loss: 0.6937 - val_acc: 0.4748\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6964 - acc: 0.5028 - val_loss: 0.6934 - val_acc: 0.4748\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6987 - acc: 0.4968 - val_loss: 0.6927 - val_acc: 0.5252\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6976 - acc: 0.4991 - val_loss: 0.6922 - val_acc: 0.5252\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6972 - acc: 0.4996 - val_loss: 0.6919 - val_acc: 0.5252\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6977 - acc: 0.5035 - val_loss: 0.6919 - val_acc: 0.5252\n",
      "Epoch 25/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6965 - acc: 0.5170 - val_loss: 0.6919 - val_acc: 0.5252\n",
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6963 - acc: 0.5119 - val_loss: 0.6919 - val_acc: 0.5252\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6961 - acc: 0.5060 - val_loss: 0.6919 - val_acc: 0.5252\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6958 - acc: 0.5110 - val_loss: 0.6919 - val_acc: 0.5252\n",
      "Epoch 29/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6968 - acc: 0.5058 - val_loss: 0.6919 - val_acc: 0.5252\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6969 - acc: 0.5071 - val_loss: 0.6920 - val_acc: 0.5252\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6990 - acc: 0.4976 - val_loss: 0.6920 - val_acc: 0.5252\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6982 - acc: 0.4927 - val_loss: 0.6920 - val_acc: 0.5252\n",
      "Epoch 33/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6951 - acc: 0.5186 - val_loss: 0.6920 - val_acc: 0.5252\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6958 - acc: 0.5026 - val_loss: 0.6921 - val_acc: 0.5252\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6958 - acc: 0.4953 - val_loss: 0.6922 - val_acc: 0.5252\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6976 - acc: 0.4931 - val_loss: 0.6925 - val_acc: 0.5252\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6960 - acc: 0.4959 - val_loss: 0.6927 - val_acc: 0.5252\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6971 - acc: 0.5039 - val_loss: 0.6926 - val_acc: 0.5252\n",
      "Epoch 39/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6941 - acc: 0.5136 - val_loss: 0.6923 - val_acc: 0.5252\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6955 - acc: 0.4996 - val_loss: 0.6921 - val_acc: 0.5252\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6943 - acc: 0.5088 - val_loss: 0.6920 - val_acc: 0.5252\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6963 - acc: 0.5045 - val_loss: 0.6919 - val_acc: 0.5252\n",
      "Epoch 43/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6963 - acc: 0.4968 - val_loss: 0.6919 - val_acc: 0.5252\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6945 - acc: 0.5095 - val_loss: 0.6919 - val_acc: 0.5252\n",
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6935 - acc: 0.5177 - val_loss: 0.6919 - val_acc: 0.5252\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6952 - acc: 0.4987 - val_loss: 0.6919 - val_acc: 0.5252\n",
      "Epoch 47/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6946 - acc: 0.5009 - val_loss: 0.6920 - val_acc: 0.5252\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6940 - acc: 0.5168 - val_loss: 0.6921 - val_acc: 0.5252\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6948 - acc: 0.5071 - val_loss: 0.6921 - val_acc: 0.5252\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6940 - acc: 0.5058 - val_loss: 0.6921 - val_acc: 0.5252\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6948 - acc: 0.5039 - val_loss: 0.6921 - val_acc: 0.5252\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6940 - acc: 0.5104 - val_loss: 0.6922 - val_acc: 0.5252\n",
      "Epoch 53/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6956 - acc: 0.5002 - val_loss: 0.6922 - val_acc: 0.5252\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6936 - acc: 0.5082 - val_loss: 0.6921 - val_acc: 0.5252\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6934 - acc: 0.5071 - val_loss: 0.6921 - val_acc: 0.5252\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6939 - acc: 0.5086 - val_loss: 0.6919 - val_acc: 0.5252\n",
      "Epoch 57/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6936 - acc: 0.5017 - val_loss: 0.6919 - val_acc: 0.5252\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6933 - acc: 0.5134 - val_loss: 0.6919 - val_acc: 0.5252\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6929 - acc: 0.5229 - val_loss: 0.6919 - val_acc: 0.5252\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6933 - acc: 0.5160 - val_loss: 0.6919 - val_acc: 0.5252\n",
      "Epoch 61/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6939 - acc: 0.5045 - val_loss: 0.6920 - val_acc: 0.5252\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6937 - acc: 0.5091 - val_loss: 0.6921 - val_acc: 0.5252\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6930 - acc: 0.5134 - val_loss: 0.6921 - val_acc: 0.5252\n",
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6923 - acc: 0.5203 - val_loss: 0.6921 - val_acc: 0.5252\n",
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6931 - acc: 0.5123 - val_loss: 0.6920 - val_acc: 0.5252\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6934 - acc: 0.5123 - val_loss: 0.6920 - val_acc: 0.5252\n",
      "Epoch 67/70\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.6932 - acc: 0.5073 - val_loss: 0.6919 - val_acc: 0.5252\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6929 - acc: 0.5056 - val_loss: 0.6918 - val_acc: 0.5252\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6953 - acc: 0.4970 - val_loss: 0.6918 - val_acc: 0.5252\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6933 - acc: 0.5104 - val_loss: 0.6920 - val_acc: 0.5252\n",
      "sample weight :  [4.36241014e-04 1.86745849e-05 2.55097462e-05 ... 1.99515615e-05\n",
      " 1.97610712e-04 5.37876938e-05]\n",
      "1334171.0\n",
      "new_x, new_y (6182, 10, 4068) (6182,)\n",
      "WARNING:tensorflow:Layer lstm_120 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_121 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_122 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_123 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/70\n",
      "2/2 [==============================] - 5s 536ms/step - loss: 0.7025 - acc: 0.4935 - val_loss: 0.6939 - val_acc: 0.4903\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6979 - acc: 0.5114 - val_loss: 0.6959 - val_acc: 0.4903\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.7002 - acc: 0.4976 - val_loss: 0.6951 - val_acc: 0.4903\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.7006 - acc: 0.4994 - val_loss: 0.6933 - val_acc: 0.4903\n",
      "Epoch 5/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 139ms/step - loss: 0.7028 - acc: 0.4899 - val_loss: 0.6930 - val_acc: 0.5097\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.7010 - acc: 0.4929 - val_loss: 0.6933 - val_acc: 0.5097\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.7000 - acc: 0.4937 - val_loss: 0.6930 - val_acc: 0.5097\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.7006 - acc: 0.4963 - val_loss: 0.6930 - val_acc: 0.5097\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6964 - acc: 0.5082 - val_loss: 0.6941 - val_acc: 0.4903\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6991 - acc: 0.5047 - val_loss: 0.6956 - val_acc: 0.4903\n",
      "Epoch 11/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6988 - acc: 0.4963 - val_loss: 0.6960 - val_acc: 0.4903\n",
      "Epoch 12/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.7001 - acc: 0.5060 - val_loss: 0.6954 - val_acc: 0.4903\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6986 - acc: 0.4991 - val_loss: 0.6944 - val_acc: 0.4903\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6984 - acc: 0.4963 - val_loss: 0.6936 - val_acc: 0.4903\n",
      "Epoch 15/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6969 - acc: 0.5035 - val_loss: 0.6932 - val_acc: 0.4903\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6943 - acc: 0.5125 - val_loss: 0.6930 - val_acc: 0.5097\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6953 - acc: 0.5121 - val_loss: 0.6930 - val_acc: 0.5097\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6944 - acc: 0.5149 - val_loss: 0.6930 - val_acc: 0.5097\n",
      "Epoch 19/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6947 - acc: 0.5108 - val_loss: 0.6931 - val_acc: 0.5097\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6942 - acc: 0.5123 - val_loss: 0.6936 - val_acc: 0.4903\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6949 - acc: 0.5164 - val_loss: 0.6940 - val_acc: 0.4903\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6958 - acc: 0.4972 - val_loss: 0.6940 - val_acc: 0.4903\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6974 - acc: 0.4892 - val_loss: 0.6937 - val_acc: 0.4903\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6974 - acc: 0.4931 - val_loss: 0.6935 - val_acc: 0.4903\n",
      "Epoch 25/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6966 - acc: 0.4935 - val_loss: 0.6933 - val_acc: 0.4903\n",
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6960 - acc: 0.4983 - val_loss: 0.6930 - val_acc: 0.5097\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6961 - acc: 0.4922 - val_loss: 0.6930 - val_acc: 0.5097\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6953 - acc: 0.4991 - val_loss: 0.6929 - val_acc: 0.5097\n",
      "Epoch 29/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6947 - acc: 0.5047 - val_loss: 0.6929 - val_acc: 0.5097\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6942 - acc: 0.4981 - val_loss: 0.6930 - val_acc: 0.5097\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6954 - acc: 0.5015 - val_loss: 0.6931 - val_acc: 0.5097\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6952 - acc: 0.4978 - val_loss: 0.6932 - val_acc: 0.4903\n",
      "Epoch 33/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6949 - acc: 0.5028 - val_loss: 0.6933 - val_acc: 0.4903\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6953 - acc: 0.4968 - val_loss: 0.6935 - val_acc: 0.4903\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6949 - acc: 0.5052 - val_loss: 0.6938 - val_acc: 0.4903\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6937 - acc: 0.5060 - val_loss: 0.6939 - val_acc: 0.4903\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6940 - acc: 0.5138 - val_loss: 0.6938 - val_acc: 0.4903\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6951 - acc: 0.5063 - val_loss: 0.6934 - val_acc: 0.4903\n",
      "Epoch 39/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6958 - acc: 0.4989 - val_loss: 0.6931 - val_acc: 0.5097\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6966 - acc: 0.4881 - val_loss: 0.6930 - val_acc: 0.5097\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6942 - acc: 0.5127 - val_loss: 0.6929 - val_acc: 0.5097\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6928 - acc: 0.5095 - val_loss: 0.6929 - val_acc: 0.5097\n",
      "Epoch 43/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6942 - acc: 0.5106 - val_loss: 0.6929 - val_acc: 0.5097\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6954 - acc: 0.4927 - val_loss: 0.6930 - val_acc: 0.5097\n",
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6955 - acc: 0.4948 - val_loss: 0.6931 - val_acc: 0.5097\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6943 - acc: 0.4940 - val_loss: 0.6932 - val_acc: 0.4903\n",
      "Epoch 47/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6953 - acc: 0.4948 - val_loss: 0.6934 - val_acc: 0.4903\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6946 - acc: 0.4925 - val_loss: 0.6936 - val_acc: 0.4903\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6935 - acc: 0.5078 - val_loss: 0.6940 - val_acc: 0.4903\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6947 - acc: 0.4972 - val_loss: 0.6941 - val_acc: 0.4903\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6948 - acc: 0.5011 - val_loss: 0.6939 - val_acc: 0.4903\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6938 - acc: 0.5140 - val_loss: 0.6936 - val_acc: 0.4903\n",
      "Epoch 53/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6944 - acc: 0.5065 - val_loss: 0.6933 - val_acc: 0.4903\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6947 - acc: 0.4950 - val_loss: 0.6930 - val_acc: 0.5097\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6937 - acc: 0.5013 - val_loss: 0.6930 - val_acc: 0.5097\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6922 - acc: 0.5211 - val_loss: 0.6929 - val_acc: 0.5097\n",
      "Epoch 57/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6947 - acc: 0.4994 - val_loss: 0.6929 - val_acc: 0.5097\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6940 - acc: 0.5037 - val_loss: 0.6929 - val_acc: 0.5097\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6931 - acc: 0.5125 - val_loss: 0.6930 - val_acc: 0.5097\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6944 - acc: 0.4965 - val_loss: 0.6930 - val_acc: 0.5097\n",
      "Epoch 61/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6948 - acc: 0.4916 - val_loss: 0.6930 - val_acc: 0.5194\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6935 - acc: 0.4965 - val_loss: 0.6932 - val_acc: 0.4903\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6936 - acc: 0.5110 - val_loss: 0.6934 - val_acc: 0.4903\n",
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6937 - acc: 0.5000 - val_loss: 0.6937 - val_acc: 0.4903\n",
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6925 - acc: 0.5093 - val_loss: 0.6937 - val_acc: 0.4903\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.6947 - acc: 0.4985 - val_loss: 0.6934 - val_acc: 0.4903\n",
      "Epoch 67/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6930 - acc: 0.5056 - val_loss: 0.6930 - val_acc: 0.5097\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6926 - acc: 0.5095 - val_loss: 0.6928 - val_acc: 0.5097\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6923 - acc: 0.5127 - val_loss: 0.6928 - val_acc: 0.5097\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6939 - acc: 0.5045 - val_loss: 0.6927 - val_acc: 0.5097\n",
      "sample weight :  [4.27263357e-04 1.83127436e-05 2.60815584e-05 ... 2.02523089e-05\n",
      " 2.00167828e-04 5.46609840e-05]\n",
      "1321971.0\n",
      "new_x, new_y (6182, 10, 4068) (6182,)\n",
      "WARNING:tensorflow:Layer lstm_124 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_125 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_126 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_127 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/70\n",
      "2/2 [==============================] - 5s 672ms/step - loss: 0.7481 - acc: 0.5078 - val_loss: 0.7289 - val_acc: 0.4903\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 0.7260 - acc: 0.5071 - val_loss: 0.7096 - val_acc: 0.4903\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.7087 - acc: 0.5097 - val_loss: 0.6978 - val_acc: 0.4903\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.7019 - acc: 0.4950 - val_loss: 0.6931 - val_acc: 0.4974\n",
      "Epoch 5/70\n",
      "2/2 [==============================] - 0s 156ms/step - loss: 0.6984 - acc: 0.4983 - val_loss: 0.6937 - val_acc: 0.5097\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 173ms/step - loss: 0.6999 - acc: 0.4987 - val_loss: 0.6948 - val_acc: 0.5097\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 172ms/step - loss: 0.7037 - acc: 0.4935 - val_loss: 0.6947 - val_acc: 0.5097\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 177ms/step - loss: 0.7016 - acc: 0.5058 - val_loss: 0.6938 - val_acc: 0.5097\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 184ms/step - loss: 0.6989 - acc: 0.5065 - val_loss: 0.6931 - val_acc: 0.5097\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 157ms/step - loss: 0.6978 - acc: 0.4985 - val_loss: 0.6930 - val_acc: 0.5097\n",
      "Epoch 11/70\n",
      "2/2 [==============================] - 0s 152ms/step - loss: 0.6976 - acc: 0.4940 - val_loss: 0.6933 - val_acc: 0.4903\n",
      "Epoch 12/70\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 0.6969 - acc: 0.4972 - val_loss: 0.6938 - val_acc: 0.4903\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6981 - acc: 0.5004 - val_loss: 0.6943 - val_acc: 0.4903\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 153ms/step - loss: 0.6969 - acc: 0.5043 - val_loss: 0.6947 - val_acc: 0.4903\n",
      "Epoch 15/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6973 - acc: 0.5039 - val_loss: 0.6948 - val_acc: 0.4903\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6959 - acc: 0.5188 - val_loss: 0.6947 - val_acc: 0.4903\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 159ms/step - loss: 0.6958 - acc: 0.5011 - val_loss: 0.6945 - val_acc: 0.4903\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 163ms/step - loss: 0.6963 - acc: 0.5147 - val_loss: 0.6941 - val_acc: 0.4903\n",
      "Epoch 19/70\n",
      "2/2 [==============================] - 0s 162ms/step - loss: 0.6963 - acc: 0.5078 - val_loss: 0.6937 - val_acc: 0.4903\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 166ms/step - loss: 0.6958 - acc: 0.5125 - val_loss: 0.6935 - val_acc: 0.4903\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 158ms/step - loss: 0.6982 - acc: 0.4940 - val_loss: 0.6934 - val_acc: 0.4903\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 156ms/step - loss: 0.6970 - acc: 0.5078 - val_loss: 0.6934 - val_acc: 0.4903\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 154ms/step - loss: 0.6968 - acc: 0.4991 - val_loss: 0.6934 - val_acc: 0.4903\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.6965 - acc: 0.5019 - val_loss: 0.6934 - val_acc: 0.4903\n",
      "Epoch 25/70\n",
      "2/2 [==============================] - 0s 160ms/step - loss: 0.6960 - acc: 0.5000 - val_loss: 0.6933 - val_acc: 0.4903\n",
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 155ms/step - loss: 0.6950 - acc: 0.5075 - val_loss: 0.6933 - val_acc: 0.4903\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 155ms/step - loss: 0.6934 - acc: 0.5121 - val_loss: 0.6933 - val_acc: 0.4903\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 158ms/step - loss: 0.6971 - acc: 0.4953 - val_loss: 0.6934 - val_acc: 0.4903\n",
      "Epoch 29/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6971 - acc: 0.5037 - val_loss: 0.6935 - val_acc: 0.4903\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 155ms/step - loss: 0.6948 - acc: 0.5024 - val_loss: 0.6936 - val_acc: 0.4903\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6956 - acc: 0.5058 - val_loss: 0.6937 - val_acc: 0.4903\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.6946 - acc: 0.5071 - val_loss: 0.6938 - val_acc: 0.4903\n",
      "Epoch 33/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6942 - acc: 0.5155 - val_loss: 0.6937 - val_acc: 0.4903\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6953 - acc: 0.5084 - val_loss: 0.6936 - val_acc: 0.4903\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6972 - acc: 0.4929 - val_loss: 0.6934 - val_acc: 0.4903\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6950 - acc: 0.5082 - val_loss: 0.6932 - val_acc: 0.4903\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6968 - acc: 0.4894 - val_loss: 0.6931 - val_acc: 0.5019\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6966 - acc: 0.4903 - val_loss: 0.6931 - val_acc: 0.5653\n",
      "Epoch 39/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6958 - acc: 0.4978 - val_loss: 0.6932 - val_acc: 0.4903\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6964 - acc: 0.5004 - val_loss: 0.6933 - val_acc: 0.4903\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6952 - acc: 0.5009 - val_loss: 0.6935 - val_acc: 0.4903\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6961 - acc: 0.5013 - val_loss: 0.6937 - val_acc: 0.4903\n",
      "Epoch 43/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6950 - acc: 0.5047 - val_loss: 0.6936 - val_acc: 0.4903\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6943 - acc: 0.5138 - val_loss: 0.6934 - val_acc: 0.4903\n",
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6954 - acc: 0.4991 - val_loss: 0.6934 - val_acc: 0.4903\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6955 - acc: 0.4987 - val_loss: 0.6934 - val_acc: 0.4903\n",
      "Epoch 47/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6946 - acc: 0.5093 - val_loss: 0.6934 - val_acc: 0.4903\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6938 - acc: 0.5024 - val_loss: 0.6934 - val_acc: 0.4903\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6929 - acc: 0.5134 - val_loss: 0.6934 - val_acc: 0.4903\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6952 - acc: 0.4981 - val_loss: 0.6933 - val_acc: 0.4903\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6963 - acc: 0.4989 - val_loss: 0.6931 - val_acc: 0.4903\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6956 - acc: 0.5004 - val_loss: 0.6931 - val_acc: 0.5427\n",
      "Epoch 53/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6943 - acc: 0.5078 - val_loss: 0.6930 - val_acc: 0.5097\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6934 - acc: 0.5157 - val_loss: 0.6930 - val_acc: 0.5116\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6961 - acc: 0.4985 - val_loss: 0.6932 - val_acc: 0.4903\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6962 - acc: 0.4916 - val_loss: 0.6935 - val_acc: 0.4903\n",
      "Epoch 57/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6948 - acc: 0.5037 - val_loss: 0.6938 - val_acc: 0.4903\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6945 - acc: 0.5116 - val_loss: 0.6941 - val_acc: 0.4903\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6940 - acc: 0.5082 - val_loss: 0.6943 - val_acc: 0.4903\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6959 - acc: 0.5006 - val_loss: 0.6941 - val_acc: 0.4903\n",
      "Epoch 61/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6939 - acc: 0.5088 - val_loss: 0.6937 - val_acc: 0.4903\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6950 - acc: 0.5047 - val_loss: 0.6932 - val_acc: 0.4903\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6952 - acc: 0.4892 - val_loss: 0.6929 - val_acc: 0.5116\n",
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6951 - acc: 0.4961 - val_loss: 0.6928 - val_acc: 0.5097\n",
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6938 - acc: 0.5041 - val_loss: 0.6928 - val_acc: 0.5097\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6953 - acc: 0.5006 - val_loss: 0.6928 - val_acc: 0.5116\n",
      "Epoch 67/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6941 - acc: 0.5024 - val_loss: 0.6929 - val_acc: 0.5116\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6956 - acc: 0.4866 - val_loss: 0.6932 - val_acc: 0.4903\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6952 - acc: 0.4916 - val_loss: 0.6933 - val_acc: 0.4903\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6939 - acc: 0.5043 - val_loss: 0.6934 - val_acc: 0.4903\n",
      "sample weight :  [4.33792847e-04 1.86242484e-05 2.56241956e-05 ... 1.98712473e-05\n",
      " 1.96695072e-04 5.35979266e-05]\n",
      "1290814.0\n",
      "new_x, new_y (6182, 10, 4068) (6182,)\n",
      "WARNING:tensorflow:Layer lstm_128 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_129 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_130 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_131 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/70\n",
      "2/2 [==============================] - 6s 549ms/step - loss: 0.7176 - acc: 0.4989 - val_loss: 0.6995 - val_acc: 0.4916\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.7068 - acc: 0.4927 - val_loss: 0.6936 - val_acc: 0.4916\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.7029 - acc: 0.4996 - val_loss: 0.6931 - val_acc: 0.5084\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.7015 - acc: 0.4981 - val_loss: 0.6937 - val_acc: 0.5084\n",
      "Epoch 5/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.7015 - acc: 0.5060 - val_loss: 0.6935 - val_acc: 0.5084\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.7033 - acc: 0.4916 - val_loss: 0.6930 - val_acc: 0.5084\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.7032 - acc: 0.4929 - val_loss: 0.6931 - val_acc: 0.5084\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.7010 - acc: 0.4968 - val_loss: 0.6935 - val_acc: 0.4916\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.7028 - acc: 0.4834 - val_loss: 0.6940 - val_acc: 0.4916\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6993 - acc: 0.5039 - val_loss: 0.6942 - val_acc: 0.4916\n",
      "Epoch 11/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.7012 - acc: 0.4922 - val_loss: 0.6943 - val_acc: 0.4916\n",
      "Epoch 12/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.7015 - acc: 0.4948 - val_loss: 0.6941 - val_acc: 0.4916\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6970 - acc: 0.5173 - val_loss: 0.6936 - val_acc: 0.4916\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6999 - acc: 0.5017 - val_loss: 0.6932 - val_acc: 0.4916\n",
      "Epoch 15/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6982 - acc: 0.5035 - val_loss: 0.6930 - val_acc: 0.5084\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6984 - acc: 0.5011 - val_loss: 0.6930 - val_acc: 0.5084\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6970 - acc: 0.5084 - val_loss: 0.6930 - val_acc: 0.5084\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6988 - acc: 0.4961 - val_loss: 0.6930 - val_acc: 0.5084\n",
      "Epoch 19/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6956 - acc: 0.5095 - val_loss: 0.6930 - val_acc: 0.5084\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6960 - acc: 0.5080 - val_loss: 0.6931 - val_acc: 0.4916\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6984 - acc: 0.4963 - val_loss: 0.6932 - val_acc: 0.4916\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6970 - acc: 0.5073 - val_loss: 0.6932 - val_acc: 0.4916\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6976 - acc: 0.4965 - val_loss: 0.6931 - val_acc: 0.5084\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6967 - acc: 0.5119 - val_loss: 0.6930 - val_acc: 0.5084\n",
      "Epoch 25/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6966 - acc: 0.5004 - val_loss: 0.6930 - val_acc: 0.5084\n",
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.7003 - acc: 0.4974 - val_loss: 0.6931 - val_acc: 0.5084\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6966 - acc: 0.5041 - val_loss: 0.6932 - val_acc: 0.4916\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6949 - acc: 0.5155 - val_loss: 0.6935 - val_acc: 0.4916\n",
      "Epoch 29/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6955 - acc: 0.5056 - val_loss: 0.6937 - val_acc: 0.4916\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6962 - acc: 0.5026 - val_loss: 0.6938 - val_acc: 0.4916\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6962 - acc: 0.5024 - val_loss: 0.6937 - val_acc: 0.4916\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6943 - acc: 0.5110 - val_loss: 0.6934 - val_acc: 0.4916\n",
      "Epoch 33/70\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.6978 - acc: 0.4950 - val_loss: 0.6932 - val_acc: 0.4916\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6958 - acc: 0.4925 - val_loss: 0.6930 - val_acc: 0.5084\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6988 - acc: 0.4851 - val_loss: 0.6930 - val_acc: 0.5084\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6968 - acc: 0.4914 - val_loss: 0.6930 - val_acc: 0.5084\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6979 - acc: 0.4916 - val_loss: 0.6930 - val_acc: 0.5084\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6970 - acc: 0.4961 - val_loss: 0.6930 - val_acc: 0.5084\n",
      "Epoch 39/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6976 - acc: 0.4946 - val_loss: 0.6930 - val_acc: 0.5084\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6961 - acc: 0.4935 - val_loss: 0.6932 - val_acc: 0.4916\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6942 - acc: 0.5106 - val_loss: 0.6934 - val_acc: 0.4916\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6941 - acc: 0.5104 - val_loss: 0.6936 - val_acc: 0.4916\n",
      "Epoch 43/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6959 - acc: 0.4972 - val_loss: 0.6936 - val_acc: 0.4916\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6951 - acc: 0.4963 - val_loss: 0.6935 - val_acc: 0.4916\n",
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6945 - acc: 0.4989 - val_loss: 0.6933 - val_acc: 0.4916\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6955 - acc: 0.5006 - val_loss: 0.6931 - val_acc: 0.5084\n",
      "Epoch 47/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6955 - acc: 0.4994 - val_loss: 0.6930 - val_acc: 0.5084\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6953 - acc: 0.4944 - val_loss: 0.6930 - val_acc: 0.5084\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6951 - acc: 0.5039 - val_loss: 0.6930 - val_acc: 0.5084\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6956 - acc: 0.5009 - val_loss: 0.6930 - val_acc: 0.5084\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6944 - acc: 0.5043 - val_loss: 0.6930 - val_acc: 0.5084\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6953 - acc: 0.4961 - val_loss: 0.6932 - val_acc: 0.4916\n",
      "Epoch 53/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6957 - acc: 0.4873 - val_loss: 0.6934 - val_acc: 0.4916\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6956 - acc: 0.4866 - val_loss: 0.6936 - val_acc: 0.4916\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6958 - acc: 0.5022 - val_loss: 0.6937 - val_acc: 0.4916\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6950 - acc: 0.5039 - val_loss: 0.6937 - val_acc: 0.4916\n",
      "Epoch 57/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6941 - acc: 0.5043 - val_loss: 0.6934 - val_acc: 0.4916\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6952 - acc: 0.4991 - val_loss: 0.6932 - val_acc: 0.4916\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6944 - acc: 0.5082 - val_loss: 0.6930 - val_acc: 0.5084\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6949 - acc: 0.4959 - val_loss: 0.6930 - val_acc: 0.5084\n",
      "Epoch 61/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6940 - acc: 0.5084 - val_loss: 0.6931 - val_acc: 0.5084\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6945 - acc: 0.5022 - val_loss: 0.6931 - val_acc: 0.4916\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6951 - acc: 0.5000 - val_loss: 0.6932 - val_acc: 0.4916\n",
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6950 - acc: 0.4985 - val_loss: 0.6933 - val_acc: 0.4916\n",
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6942 - acc: 0.5004 - val_loss: 0.6934 - val_acc: 0.4916\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6944 - acc: 0.4959 - val_loss: 0.6934 - val_acc: 0.4916\n",
      "Epoch 67/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6941 - acc: 0.5056 - val_loss: 0.6933 - val_acc: 0.4916\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6937 - acc: 0.5017 - val_loss: 0.6932 - val_acc: 0.4916\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6943 - acc: 0.4991 - val_loss: 0.6931 - val_acc: 0.4916\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6932 - acc: 0.5093 - val_loss: 0.6931 - val_acc: 0.5582\n",
      "sample weight :  [4.33972206e-04 1.86267678e-05 2.56321387e-05 ... 1.98568090e-05\n",
      " 1.96574182e-04 5.35620220e-05]\n",
      "1354237.0\n",
      "new_x, new_y (6182, 10, 4068) (6182,)\n",
      "WARNING:tensorflow:Layer lstm_132 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_133 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_134 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_135 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/70\n",
      "2/2 [==============================] - 5s 544ms/step - loss: 0.7121 - acc: 0.4942 - val_loss: 0.6928 - val_acc: 0.5136\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.7011 - acc: 0.5028 - val_loss: 0.6951 - val_acc: 0.4864\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.7044 - acc: 0.5009 - val_loss: 0.6973 - val_acc: 0.4864\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.7042 - acc: 0.4985 - val_loss: 0.6958 - val_acc: 0.4864\n",
      "Epoch 5/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6993 - acc: 0.5084 - val_loss: 0.6938 - val_acc: 0.4864\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6998 - acc: 0.5071 - val_loss: 0.6929 - val_acc: 0.5136\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 0.7006 - acc: 0.5009 - val_loss: 0.6928 - val_acc: 0.5136\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 0.7007 - acc: 0.4989 - val_loss: 0.6928 - val_acc: 0.5136\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6988 - acc: 0.5056 - val_loss: 0.6928 - val_acc: 0.5136\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6987 - acc: 0.5022 - val_loss: 0.6929 - val_acc: 0.5136\n",
      "Epoch 11/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.7009 - acc: 0.4899 - val_loss: 0.6931 - val_acc: 0.5136\n",
      "Epoch 12/70\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 0.6985 - acc: 0.4983 - val_loss: 0.6934 - val_acc: 0.4864\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.6976 - acc: 0.5019 - val_loss: 0.6936 - val_acc: 0.4864\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6961 - acc: 0.5110 - val_loss: 0.6936 - val_acc: 0.4864\n",
      "Epoch 15/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6973 - acc: 0.5013 - val_loss: 0.6935 - val_acc: 0.4864\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6979 - acc: 0.4989 - val_loss: 0.6935 - val_acc: 0.4864\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6989 - acc: 0.4851 - val_loss: 0.6937 - val_acc: 0.4864\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6955 - acc: 0.5145 - val_loss: 0.6939 - val_acc: 0.4864\n",
      "Epoch 19/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6979 - acc: 0.4976 - val_loss: 0.6938 - val_acc: 0.4864\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6961 - acc: 0.5026 - val_loss: 0.6935 - val_acc: 0.4864\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6986 - acc: 0.4922 - val_loss: 0.6933 - val_acc: 0.4864\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6963 - acc: 0.5058 - val_loss: 0.6931 - val_acc: 0.5343\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6951 - acc: 0.5011 - val_loss: 0.6929 - val_acc: 0.5136\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6974 - acc: 0.4978 - val_loss: 0.6929 - val_acc: 0.5136\n",
      "Epoch 25/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6962 - acc: 0.4991 - val_loss: 0.6929 - val_acc: 0.5136\n",
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6964 - acc: 0.4968 - val_loss: 0.6931 - val_acc: 0.4864\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6944 - acc: 0.5088 - val_loss: 0.6936 - val_acc: 0.4864\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6974 - acc: 0.4987 - val_loss: 0.6940 - val_acc: 0.4864\n",
      "Epoch 29/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6981 - acc: 0.4896 - val_loss: 0.6939 - val_acc: 0.4864\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6961 - acc: 0.5000 - val_loss: 0.6936 - val_acc: 0.4864\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6952 - acc: 0.5030 - val_loss: 0.6932 - val_acc: 0.4864\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6940 - acc: 0.5095 - val_loss: 0.6929 - val_acc: 0.5136\n",
      "Epoch 33/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6953 - acc: 0.5112 - val_loss: 0.6927 - val_acc: 0.5136\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6945 - acc: 0.4994 - val_loss: 0.6927 - val_acc: 0.5136\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6970 - acc: 0.4942 - val_loss: 0.6928 - val_acc: 0.5136\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6959 - acc: 0.4963 - val_loss: 0.6931 - val_acc: 0.5142\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6958 - acc: 0.4804 - val_loss: 0.6936 - val_acc: 0.4864\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6944 - acc: 0.5067 - val_loss: 0.6942 - val_acc: 0.4864\n",
      "Epoch 39/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6944 - acc: 0.5032 - val_loss: 0.6942 - val_acc: 0.4864\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6936 - acc: 0.5188 - val_loss: 0.6937 - val_acc: 0.4864\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6960 - acc: 0.4905 - val_loss: 0.6934 - val_acc: 0.4864\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6934 - acc: 0.5095 - val_loss: 0.6933 - val_acc: 0.4864\n",
      "Epoch 43/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6941 - acc: 0.5065 - val_loss: 0.6931 - val_acc: 0.5608\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6955 - acc: 0.4972 - val_loss: 0.6930 - val_acc: 0.5136\n",
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6951 - acc: 0.5004 - val_loss: 0.6931 - val_acc: 0.4864\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6947 - acc: 0.4991 - val_loss: 0.6934 - val_acc: 0.4864\n",
      "Epoch 47/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6939 - acc: 0.5052 - val_loss: 0.6937 - val_acc: 0.4864\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6960 - acc: 0.4797 - val_loss: 0.6938 - val_acc: 0.4864\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6939 - acc: 0.5052 - val_loss: 0.6937 - val_acc: 0.4864\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6942 - acc: 0.5039 - val_loss: 0.6934 - val_acc: 0.4864\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6948 - acc: 0.4991 - val_loss: 0.6930 - val_acc: 0.5278\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6944 - acc: 0.4981 - val_loss: 0.6927 - val_acc: 0.5136\n",
      "Epoch 53/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6940 - acc: 0.5009 - val_loss: 0.6926 - val_acc: 0.5136\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6943 - acc: 0.5004 - val_loss: 0.6926 - val_acc: 0.5136\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 0s 151ms/step - loss: 0.6941 - acc: 0.5011 - val_loss: 0.6926 - val_acc: 0.5136\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6931 - acc: 0.5088 - val_loss: 0.6927 - val_acc: 0.5136\n",
      "Epoch 57/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6938 - acc: 0.5060 - val_loss: 0.6926 - val_acc: 0.5136\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6947 - acc: 0.4972 - val_loss: 0.6925 - val_acc: 0.5136\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6950 - acc: 0.4948 - val_loss: 0.6926 - val_acc: 0.5136\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6935 - acc: 0.5093 - val_loss: 0.6929 - val_acc: 0.5058\n",
      "Epoch 61/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6940 - acc: 0.5039 - val_loss: 0.6936 - val_acc: 0.4864\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6939 - acc: 0.5069 - val_loss: 0.6940 - val_acc: 0.4864\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6937 - acc: 0.5022 - val_loss: 0.6940 - val_acc: 0.4864\n",
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6940 - acc: 0.5067 - val_loss: 0.6936 - val_acc: 0.4864\n",
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6929 - acc: 0.5155 - val_loss: 0.6933 - val_acc: 0.4864\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6934 - acc: 0.5084 - val_loss: 0.6928 - val_acc: 0.4864\n",
      "Epoch 67/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6929 - acc: 0.5095 - val_loss: 0.6925 - val_acc: 0.5388\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6920 - acc: 0.5162 - val_loss: 0.6923 - val_acc: 0.5479\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6928 - acc: 0.5140 - val_loss: 0.6921 - val_acc: 0.5602\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6932 - acc: 0.5065 - val_loss: 0.6918 - val_acc: 0.5614\n",
      "sample weight :  [4.36017787e-04 1.87188111e-05 2.59079772e-05 ... 1.95488424e-05\n",
      " 1.92874149e-04 5.30598753e-05]\n",
      "1301796.0\n",
      "new_x, new_y (6182, 10, 4068) (6182,)\n",
      "WARNING:tensorflow:Layer lstm_136 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_137 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_138 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_139 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/70\n",
      "2/2 [==============================] - 6s 537ms/step - loss: 0.7051 - acc: 0.5145 - val_loss: 0.6930 - val_acc: 0.5084\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.7013 - acc: 0.5063 - val_loss: 0.6946 - val_acc: 0.4916\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.7011 - acc: 0.5078 - val_loss: 0.6952 - val_acc: 0.4916\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.7011 - acc: 0.5123 - val_loss: 0.6942 - val_acc: 0.4916\n",
      "Epoch 5/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.7015 - acc: 0.5063 - val_loss: 0.6932 - val_acc: 0.4916\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.7003 - acc: 0.4978 - val_loss: 0.6931 - val_acc: 0.5084\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.7016 - acc: 0.4985 - val_loss: 0.6933 - val_acc: 0.5084\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.7004 - acc: 0.5028 - val_loss: 0.6931 - val_acc: 0.5084\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.7027 - acc: 0.4940 - val_loss: 0.6930 - val_acc: 0.5084\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.7005 - acc: 0.4963 - val_loss: 0.6934 - val_acc: 0.4916\n",
      "Epoch 11/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 132ms/step - loss: 0.7001 - acc: 0.4944 - val_loss: 0.6941 - val_acc: 0.4916\n",
      "Epoch 12/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.7004 - acc: 0.4948 - val_loss: 0.6945 - val_acc: 0.4916\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.7002 - acc: 0.4974 - val_loss: 0.6942 - val_acc: 0.4916\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6983 - acc: 0.5037 - val_loss: 0.6936 - val_acc: 0.4916\n",
      "Epoch 15/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6987 - acc: 0.5000 - val_loss: 0.6933 - val_acc: 0.4916\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6988 - acc: 0.5030 - val_loss: 0.6931 - val_acc: 0.5084\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6985 - acc: 0.4983 - val_loss: 0.6931 - val_acc: 0.5084\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6984 - acc: 0.4955 - val_loss: 0.6931 - val_acc: 0.5084\n",
      "Epoch 19/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6988 - acc: 0.4994 - val_loss: 0.6931 - val_acc: 0.5103\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6939 - acc: 0.5179 - val_loss: 0.6932 - val_acc: 0.4916\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6978 - acc: 0.5024 - val_loss: 0.6931 - val_acc: 0.4916\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6976 - acc: 0.4935 - val_loss: 0.6932 - val_acc: 0.4916\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.7006 - acc: 0.4774 - val_loss: 0.6934 - val_acc: 0.4916\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 152ms/step - loss: 0.6966 - acc: 0.5009 - val_loss: 0.6935 - val_acc: 0.4916\n",
      "Epoch 25/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6963 - acc: 0.4998 - val_loss: 0.6937 - val_acc: 0.4916\n",
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6974 - acc: 0.5000 - val_loss: 0.6938 - val_acc: 0.4916\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6968 - acc: 0.4946 - val_loss: 0.6937 - val_acc: 0.4916\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6960 - acc: 0.5082 - val_loss: 0.6937 - val_acc: 0.4916\n",
      "Epoch 29/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6968 - acc: 0.5086 - val_loss: 0.6937 - val_acc: 0.4916\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6964 - acc: 0.4965 - val_loss: 0.6935 - val_acc: 0.4916\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6956 - acc: 0.4985 - val_loss: 0.6933 - val_acc: 0.4916\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6953 - acc: 0.5067 - val_loss: 0.6932 - val_acc: 0.4916\n",
      "Epoch 33/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6947 - acc: 0.4972 - val_loss: 0.6932 - val_acc: 0.4916\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6953 - acc: 0.5026 - val_loss: 0.6934 - val_acc: 0.4916\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6960 - acc: 0.4916 - val_loss: 0.6938 - val_acc: 0.4916\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6961 - acc: 0.4946 - val_loss: 0.6940 - val_acc: 0.4916\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6955 - acc: 0.5050 - val_loss: 0.6937 - val_acc: 0.4916\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6938 - acc: 0.5050 - val_loss: 0.6933 - val_acc: 0.4916\n",
      "Epoch 39/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6940 - acc: 0.5082 - val_loss: 0.6930 - val_acc: 0.5084\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6954 - acc: 0.5043 - val_loss: 0.6930 - val_acc: 0.5084\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6959 - acc: 0.4987 - val_loss: 0.6930 - val_acc: 0.5084\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6949 - acc: 0.4963 - val_loss: 0.6930 - val_acc: 0.5084\n",
      "Epoch 43/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6952 - acc: 0.4946 - val_loss: 0.6930 - val_acc: 0.5084\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6951 - acc: 0.4968 - val_loss: 0.6931 - val_acc: 0.4922\n",
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6933 - acc: 0.5181 - val_loss: 0.6936 - val_acc: 0.4916\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6950 - acc: 0.5035 - val_loss: 0.6941 - val_acc: 0.4916\n",
      "Epoch 47/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6953 - acc: 0.5050 - val_loss: 0.6941 - val_acc: 0.4916\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6949 - acc: 0.4968 - val_loss: 0.6937 - val_acc: 0.4916\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6951 - acc: 0.5009 - val_loss: 0.6935 - val_acc: 0.4916\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6944 - acc: 0.5028 - val_loss: 0.6934 - val_acc: 0.4916\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6948 - acc: 0.5035 - val_loss: 0.6933 - val_acc: 0.4916\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6936 - acc: 0.5069 - val_loss: 0.6932 - val_acc: 0.4916\n",
      "Epoch 53/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6937 - acc: 0.5104 - val_loss: 0.6931 - val_acc: 0.4916\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6951 - acc: 0.4955 - val_loss: 0.6931 - val_acc: 0.4916\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6946 - acc: 0.5056 - val_loss: 0.6932 - val_acc: 0.4916\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6959 - acc: 0.4922 - val_loss: 0.6932 - val_acc: 0.4916\n",
      "Epoch 57/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6948 - acc: 0.4933 - val_loss: 0.6932 - val_acc: 0.4916\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6939 - acc: 0.5071 - val_loss: 0.6932 - val_acc: 0.4916\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6937 - acc: 0.4987 - val_loss: 0.6933 - val_acc: 0.4916\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6932 - acc: 0.5168 - val_loss: 0.6933 - val_acc: 0.4916\n",
      "Epoch 61/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6951 - acc: 0.4978 - val_loss: 0.6934 - val_acc: 0.4916\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6938 - acc: 0.5050 - val_loss: 0.6935 - val_acc: 0.4916\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6935 - acc: 0.5041 - val_loss: 0.6935 - val_acc: 0.4916\n",
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6945 - acc: 0.4965 - val_loss: 0.6935 - val_acc: 0.4916\n",
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6946 - acc: 0.5037 - val_loss: 0.6935 - val_acc: 0.4916\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6938 - acc: 0.5123 - val_loss: 0.6933 - val_acc: 0.4916\n",
      "Epoch 67/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6938 - acc: 0.5041 - val_loss: 0.6932 - val_acc: 0.4916\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6932 - acc: 0.5235 - val_loss: 0.6930 - val_acc: 0.5627\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6941 - acc: 0.4950 - val_loss: 0.6928 - val_acc: 0.5084\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6947 - acc: 0.4899 - val_loss: 0.6928 - val_acc: 0.5084\n",
      "sample weight :  [4.41380342e-04 1.89340803e-05 2.56629894e-05 ... 1.92980531e-05\n",
      " 1.90879198e-04 5.23447780e-05]\n",
      "1301509.0\n",
      "new_x, new_y (6182, 10, 4068) (6182,)\n",
      "WARNING:tensorflow:Layer lstm_140 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_141 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_142 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_143 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/70\n",
      "2/2 [==============================] - 5s 546ms/step - loss: 0.7259 - acc: 0.5097 - val_loss: 0.6939 - val_acc: 0.5291\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.7028 - acc: 0.5112 - val_loss: 0.6921 - val_acc: 0.5291\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 158ms/step - loss: 0.7035 - acc: 0.4950 - val_loss: 0.6972 - val_acc: 0.4709\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 0s 157ms/step - loss: 0.7026 - acc: 0.5017 - val_loss: 0.6986 - val_acc: 0.4709\n",
      "Epoch 5/70\n",
      "2/2 [==============================] - 0s 152ms/step - loss: 0.7016 - acc: 0.5060 - val_loss: 0.6964 - val_acc: 0.4709\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 157ms/step - loss: 0.6995 - acc: 0.5011 - val_loss: 0.6939 - val_acc: 0.4709\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 155ms/step - loss: 0.7005 - acc: 0.5017 - val_loss: 0.6924 - val_acc: 0.5291\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6983 - acc: 0.5075 - val_loss: 0.6916 - val_acc: 0.5291\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6973 - acc: 0.5093 - val_loss: 0.6914 - val_acc: 0.5291\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.7009 - acc: 0.5056 - val_loss: 0.6914 - val_acc: 0.5291\n",
      "Epoch 11/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.7007 - acc: 0.5015 - val_loss: 0.6914 - val_acc: 0.5291\n",
      "Epoch 12/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6998 - acc: 0.5043 - val_loss: 0.6915 - val_acc: 0.5291\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.7010 - acc: 0.4888 - val_loss: 0.6917 - val_acc: 0.5291\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.7012 - acc: 0.5039 - val_loss: 0.6920 - val_acc: 0.5291\n",
      "Epoch 15/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.7003 - acc: 0.4953 - val_loss: 0.6922 - val_acc: 0.5291\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6999 - acc: 0.4978 - val_loss: 0.6922 - val_acc: 0.5291\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.7010 - acc: 0.4860 - val_loss: 0.6922 - val_acc: 0.5291\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6974 - acc: 0.5024 - val_loss: 0.6922 - val_acc: 0.5291\n",
      "Epoch 19/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6956 - acc: 0.5252 - val_loss: 0.6920 - val_acc: 0.5291\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.7010 - acc: 0.4855 - val_loss: 0.6918 - val_acc: 0.5291\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6977 - acc: 0.5078 - val_loss: 0.6916 - val_acc: 0.5291\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6973 - acc: 0.5047 - val_loss: 0.6916 - val_acc: 0.5291\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6971 - acc: 0.4989 - val_loss: 0.6916 - val_acc: 0.5291\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6982 - acc: 0.5054 - val_loss: 0.6915 - val_acc: 0.5291\n",
      "Epoch 25/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6969 - acc: 0.5136 - val_loss: 0.6915 - val_acc: 0.5291\n",
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6981 - acc: 0.5086 - val_loss: 0.6915 - val_acc: 0.5291\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6949 - acc: 0.5170 - val_loss: 0.6915 - val_acc: 0.5291\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6955 - acc: 0.5114 - val_loss: 0.6916 - val_acc: 0.5291\n",
      "Epoch 29/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6967 - acc: 0.5006 - val_loss: 0.6918 - val_acc: 0.5291\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.6982 - acc: 0.4851 - val_loss: 0.6922 - val_acc: 0.5291\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6955 - acc: 0.5028 - val_loss: 0.6923 - val_acc: 0.5291\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6975 - acc: 0.4940 - val_loss: 0.6921 - val_acc: 0.5291\n",
      "Epoch 33/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6965 - acc: 0.4940 - val_loss: 0.6920 - val_acc: 0.5291\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6978 - acc: 0.4944 - val_loss: 0.6918 - val_acc: 0.5291\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6970 - acc: 0.4978 - val_loss: 0.6916 - val_acc: 0.5291\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6957 - acc: 0.5063 - val_loss: 0.6916 - val_acc: 0.5291\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6968 - acc: 0.4998 - val_loss: 0.6916 - val_acc: 0.5291\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6957 - acc: 0.5045 - val_loss: 0.6917 - val_acc: 0.5291\n",
      "Epoch 39/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.6938 - acc: 0.5095 - val_loss: 0.6918 - val_acc: 0.5291\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 152ms/step - loss: 0.6963 - acc: 0.4901 - val_loss: 0.6917 - val_acc: 0.5291\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 166ms/step - loss: 0.6952 - acc: 0.5032 - val_loss: 0.6917 - val_acc: 0.5291\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 166ms/step - loss: 0.6946 - acc: 0.5065 - val_loss: 0.6917 - val_acc: 0.5291\n",
      "Epoch 43/70\n",
      "2/2 [==============================] - 0s 164ms/step - loss: 0.6937 - acc: 0.5104 - val_loss: 0.6917 - val_acc: 0.5291\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 166ms/step - loss: 0.6964 - acc: 0.4976 - val_loss: 0.6919 - val_acc: 0.5291\n",
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 166ms/step - loss: 0.6950 - acc: 0.4996 - val_loss: 0.6919 - val_acc: 0.5291\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 173ms/step - loss: 0.6937 - acc: 0.5093 - val_loss: 0.6920 - val_acc: 0.5291\n",
      "Epoch 47/70\n",
      "2/2 [==============================] - 0s 163ms/step - loss: 0.6938 - acc: 0.5132 - val_loss: 0.6921 - val_acc: 0.5291\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 166ms/step - loss: 0.6951 - acc: 0.4994 - val_loss: 0.6920 - val_acc: 0.5291\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 165ms/step - loss: 0.6948 - acc: 0.5060 - val_loss: 0.6920 - val_acc: 0.5291\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 160ms/step - loss: 0.6941 - acc: 0.5145 - val_loss: 0.6919 - val_acc: 0.5291\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 164ms/step - loss: 0.6947 - acc: 0.4985 - val_loss: 0.6917 - val_acc: 0.5291\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 165ms/step - loss: 0.6938 - acc: 0.5136 - val_loss: 0.6915 - val_acc: 0.5291\n",
      "Epoch 53/70\n",
      "2/2 [==============================] - 0s 154ms/step - loss: 0.6942 - acc: 0.5084 - val_loss: 0.6917 - val_acc: 0.5291\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 0s 159ms/step - loss: 0.6934 - acc: 0.5108 - val_loss: 0.6921 - val_acc: 0.5291\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 0s 162ms/step - loss: 0.6940 - acc: 0.5030 - val_loss: 0.6923 - val_acc: 0.5330\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 0s 162ms/step - loss: 0.6942 - acc: 0.5047 - val_loss: 0.6923 - val_acc: 0.5433\n",
      "Epoch 57/70\n",
      "2/2 [==============================] - 0s 165ms/step - loss: 0.6938 - acc: 0.5101 - val_loss: 0.6920 - val_acc: 0.5330\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 0s 157ms/step - loss: 0.6922 - acc: 0.5216 - val_loss: 0.6917 - val_acc: 0.5330\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 0s 164ms/step - loss: 0.6926 - acc: 0.5175 - val_loss: 0.6915 - val_acc: 0.5304\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 0s 163ms/step - loss: 0.6921 - acc: 0.5216 - val_loss: 0.6911 - val_acc: 0.5291\n",
      "Epoch 61/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 166ms/step - loss: 0.6915 - acc: 0.5259 - val_loss: 0.6909 - val_acc: 0.5291\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 166ms/step - loss: 0.6917 - acc: 0.5145 - val_loss: 0.6908 - val_acc: 0.5323\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 164ms/step - loss: 0.6910 - acc: 0.5220 - val_loss: 0.6902 - val_acc: 0.5298\n",
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 166ms/step - loss: 0.6904 - acc: 0.5321 - val_loss: 0.6898 - val_acc: 0.5369\n",
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 164ms/step - loss: 0.6889 - acc: 0.5475 - val_loss: 0.6899 - val_acc: 0.5640\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 163ms/step - loss: 0.6883 - acc: 0.5412 - val_loss: 0.6891 - val_acc: 0.5298\n",
      "Epoch 67/70\n",
      "2/2 [==============================] - 0s 160ms/step - loss: 0.6854 - acc: 0.5537 - val_loss: 0.6884 - val_acc: 0.5168\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 154ms/step - loss: 0.6839 - acc: 0.5643 - val_loss: 0.6878 - val_acc: 0.5692\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 158ms/step - loss: 0.6806 - acc: 0.5869 - val_loss: 0.6859 - val_acc: 0.5472\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 152ms/step - loss: 0.6794 - acc: 0.5755 - val_loss: 0.6844 - val_acc: 0.5834\n",
      "sample weight :  [4.60282949e-04 1.85224492e-05 2.96189136e-05 ... 1.83968523e-05\n",
      " 1.81792897e-04 5.15078483e-05]\n",
      "1308341.0\n",
      "new_x, new_y (6182, 10, 4068) (6182,)\n",
      "WARNING:tensorflow:Layer lstm_144 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_145 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_146 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_147 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/70\n",
      "2/2 [==============================] - 6s 544ms/step - loss: 0.7395 - acc: 0.5063 - val_loss: 0.7102 - val_acc: 0.5129\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.7189 - acc: 0.5060 - val_loss: 0.6976 - val_acc: 0.5129\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.7038 - acc: 0.5054 - val_loss: 0.6929 - val_acc: 0.5129\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 0s 161ms/step - loss: 0.6994 - acc: 0.4937 - val_loss: 0.6945 - val_acc: 0.4871\n",
      "Epoch 5/70\n",
      "2/2 [==============================] - 0s 153ms/step - loss: 0.7003 - acc: 0.4927 - val_loss: 0.6981 - val_acc: 0.4871\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 158ms/step - loss: 0.7040 - acc: 0.4948 - val_loss: 0.6997 - val_acc: 0.4871\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 158ms/step - loss: 0.7026 - acc: 0.5030 - val_loss: 0.6987 - val_acc: 0.4871\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 152ms/step - loss: 0.7051 - acc: 0.4890 - val_loss: 0.6963 - val_acc: 0.4871\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.7017 - acc: 0.4922 - val_loss: 0.6944 - val_acc: 0.4871\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6971 - acc: 0.5078 - val_loss: 0.6933 - val_acc: 0.4871\n",
      "Epoch 11/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6970 - acc: 0.5129 - val_loss: 0.6929 - val_acc: 0.5129\n",
      "Epoch 12/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6980 - acc: 0.5099 - val_loss: 0.6928 - val_acc: 0.5129\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6973 - acc: 0.4998 - val_loss: 0.6930 - val_acc: 0.5129\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6972 - acc: 0.5080 - val_loss: 0.6932 - val_acc: 0.5129\n",
      "Epoch 15/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6992 - acc: 0.5026 - val_loss: 0.6932 - val_acc: 0.5129\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.7001 - acc: 0.4942 - val_loss: 0.6930 - val_acc: 0.5129\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6985 - acc: 0.4931 - val_loss: 0.6929 - val_acc: 0.5129\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6965 - acc: 0.5082 - val_loss: 0.6928 - val_acc: 0.5129\n",
      "Epoch 19/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6963 - acc: 0.5054 - val_loss: 0.6928 - val_acc: 0.5129\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6994 - acc: 0.4946 - val_loss: 0.6929 - val_acc: 0.5129\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6974 - acc: 0.4922 - val_loss: 0.6929 - val_acc: 0.5129\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6965 - acc: 0.5030 - val_loss: 0.6929 - val_acc: 0.5129\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6975 - acc: 0.5028 - val_loss: 0.6929 - val_acc: 0.5129\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6964 - acc: 0.5006 - val_loss: 0.6930 - val_acc: 0.5129\n",
      "Epoch 25/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6964 - acc: 0.4972 - val_loss: 0.6930 - val_acc: 0.5129\n",
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6973 - acc: 0.5009 - val_loss: 0.6930 - val_acc: 0.5129\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6973 - acc: 0.5035 - val_loss: 0.6930 - val_acc: 0.5129\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6971 - acc: 0.5011 - val_loss: 0.6930 - val_acc: 0.5129\n",
      "Epoch 29/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6964 - acc: 0.5095 - val_loss: 0.6930 - val_acc: 0.5129\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6976 - acc: 0.4920 - val_loss: 0.6929 - val_acc: 0.5129\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6951 - acc: 0.5127 - val_loss: 0.6929 - val_acc: 0.5129\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6952 - acc: 0.5054 - val_loss: 0.6929 - val_acc: 0.5129\n",
      "Epoch 33/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6970 - acc: 0.4886 - val_loss: 0.6929 - val_acc: 0.5129\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6955 - acc: 0.5125 - val_loss: 0.6928 - val_acc: 0.5129\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6963 - acc: 0.5032 - val_loss: 0.6928 - val_acc: 0.5129\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6973 - acc: 0.4996 - val_loss: 0.6928 - val_acc: 0.5129\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6983 - acc: 0.4978 - val_loss: 0.6928 - val_acc: 0.5129\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6970 - acc: 0.5017 - val_loss: 0.6928 - val_acc: 0.5129\n",
      "Epoch 39/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6952 - acc: 0.5017 - val_loss: 0.6928 - val_acc: 0.5129\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6954 - acc: 0.5224 - val_loss: 0.6928 - val_acc: 0.5129\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6957 - acc: 0.5035 - val_loss: 0.6929 - val_acc: 0.5129\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6970 - acc: 0.4957 - val_loss: 0.6930 - val_acc: 0.5129\n",
      "Epoch 43/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6961 - acc: 0.5026 - val_loss: 0.6932 - val_acc: 0.4871\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6955 - acc: 0.5050 - val_loss: 0.6934 - val_acc: 0.4871\n",
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6960 - acc: 0.4987 - val_loss: 0.6935 - val_acc: 0.4871\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6976 - acc: 0.4875 - val_loss: 0.6935 - val_acc: 0.4871\n",
      "Epoch 47/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6961 - acc: 0.5047 - val_loss: 0.6933 - val_acc: 0.4871\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6950 - acc: 0.5015 - val_loss: 0.6931 - val_acc: 0.5129\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6958 - acc: 0.5015 - val_loss: 0.6929 - val_acc: 0.5129\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6949 - acc: 0.4987 - val_loss: 0.6928 - val_acc: 0.5129\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6957 - acc: 0.4987 - val_loss: 0.6928 - val_acc: 0.5129\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6945 - acc: 0.5108 - val_loss: 0.6928 - val_acc: 0.5129\n",
      "Epoch 53/70\n",
      "2/2 [==============================] - 0s 249ms/step - loss: 0.6960 - acc: 0.5047 - val_loss: 0.6929 - val_acc: 0.5129\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 1s 277ms/step - loss: 0.6942 - acc: 0.5028 - val_loss: 0.6929 - val_acc: 0.5129\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 1s 263ms/step - loss: 0.6964 - acc: 0.4961 - val_loss: 0.6928 - val_acc: 0.5129\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 1s 266ms/step - loss: 0.6952 - acc: 0.5024 - val_loss: 0.6928 - val_acc: 0.5129\n",
      "Epoch 57/70\n",
      "2/2 [==============================] - 1s 258ms/step - loss: 0.6946 - acc: 0.5082 - val_loss: 0.6928 - val_acc: 0.5129\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 1s 247ms/step - loss: 0.6954 - acc: 0.5047 - val_loss: 0.6928 - val_acc: 0.5129\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 1s 257ms/step - loss: 0.6932 - acc: 0.5166 - val_loss: 0.6928 - val_acc: 0.5129\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 1s 261ms/step - loss: 0.6951 - acc: 0.4961 - val_loss: 0.6928 - val_acc: 0.5129\n",
      "Epoch 61/70\n",
      "2/2 [==============================] - 0s 196ms/step - loss: 0.6942 - acc: 0.5058 - val_loss: 0.6929 - val_acc: 0.5129\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 177ms/step - loss: 0.6949 - acc: 0.5043 - val_loss: 0.6928 - val_acc: 0.5129\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 157ms/step - loss: 0.6951 - acc: 0.4942 - val_loss: 0.6928 - val_acc: 0.5129\n",
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 157ms/step - loss: 0.6944 - acc: 0.5000 - val_loss: 0.6928 - val_acc: 0.5129\n",
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 158ms/step - loss: 0.6938 - acc: 0.5039 - val_loss: 0.6927 - val_acc: 0.5129\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 162ms/step - loss: 0.6948 - acc: 0.5086 - val_loss: 0.6927 - val_acc: 0.5129\n",
      "Epoch 67/70\n",
      "2/2 [==============================] - 0s 154ms/step - loss: 0.6954 - acc: 0.4896 - val_loss: 0.6928 - val_acc: 0.5129\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6960 - acc: 0.4987 - val_loss: 0.6928 - val_acc: 0.5129\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 151ms/step - loss: 0.6937 - acc: 0.5069 - val_loss: 0.6927 - val_acc: 0.5129\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6954 - acc: 0.5052 - val_loss: 0.6927 - val_acc: 0.5129\n",
      "sample weight :  [4.70885048e-04 1.89664631e-05 2.89116911e-05 ... 1.79876872e-05\n",
      " 1.77791594e-04 5.02723440e-05]\n",
      "1293367.0\n",
      "new_x, new_y (6182, 10, 4068) (6182,)\n",
      "WARNING:tensorflow:Layer lstm_148 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_149 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_150 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_151 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/70\n",
      "2/2 [==============================] - 6s 543ms/step - loss: 0.7842 - acc: 0.4832 - val_loss: 0.7345 - val_acc: 0.5032\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.7531 - acc: 0.4830 - val_loss: 0.7115 - val_acc: 0.5032\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.7261 - acc: 0.4819 - val_loss: 0.6975 - val_acc: 0.5032\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.7066 - acc: 0.4920 - val_loss: 0.6932 - val_acc: 0.4968\n",
      "Epoch 5/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.7031 - acc: 0.4901 - val_loss: 0.6977 - val_acc: 0.4968\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6996 - acc: 0.5088 - val_loss: 0.7035 - val_acc: 0.4968\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.7047 - acc: 0.5101 - val_loss: 0.7043 - val_acc: 0.4968\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.7053 - acc: 0.5116 - val_loss: 0.7013 - val_acc: 0.4968\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.7049 - acc: 0.5106 - val_loss: 0.6977 - val_acc: 0.4968\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.7009 - acc: 0.5045 - val_loss: 0.6951 - val_acc: 0.4968\n",
      "Epoch 11/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6990 - acc: 0.5028 - val_loss: 0.6937 - val_acc: 0.4968\n",
      "Epoch 12/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6982 - acc: 0.5000 - val_loss: 0.6932 - val_acc: 0.4968\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6982 - acc: 0.5002 - val_loss: 0.6931 - val_acc: 0.5032\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6970 - acc: 0.4981 - val_loss: 0.6932 - val_acc: 0.5032\n",
      "Epoch 15/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6957 - acc: 0.5114 - val_loss: 0.6932 - val_acc: 0.5032\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6985 - acc: 0.5054 - val_loss: 0.6932 - val_acc: 0.5032\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6941 - acc: 0.5181 - val_loss: 0.6931 - val_acc: 0.5032\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6990 - acc: 0.5019 - val_loss: 0.6931 - val_acc: 0.5032\n",
      "Epoch 19/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6969 - acc: 0.5022 - val_loss: 0.6932 - val_acc: 0.4968\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6958 - acc: 0.5211 - val_loss: 0.6934 - val_acc: 0.4968\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6954 - acc: 0.5153 - val_loss: 0.6937 - val_acc: 0.4968\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.7004 - acc: 0.4847 - val_loss: 0.6940 - val_acc: 0.4968\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6981 - acc: 0.4991 - val_loss: 0.6945 - val_acc: 0.4968\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6979 - acc: 0.5039 - val_loss: 0.6949 - val_acc: 0.4968\n",
      "Epoch 25/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6967 - acc: 0.5060 - val_loss: 0.6952 - val_acc: 0.4968\n",
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6959 - acc: 0.5091 - val_loss: 0.6955 - val_acc: 0.4968\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6949 - acc: 0.5255 - val_loss: 0.6956 - val_acc: 0.4968\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6982 - acc: 0.5009 - val_loss: 0.6953 - val_acc: 0.4968\n",
      "Epoch 29/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6980 - acc: 0.5056 - val_loss: 0.6950 - val_acc: 0.4968\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6953 - acc: 0.5132 - val_loss: 0.6946 - val_acc: 0.4968\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6967 - acc: 0.5060 - val_loss: 0.6943 - val_acc: 0.4968\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6936 - acc: 0.5203 - val_loss: 0.6939 - val_acc: 0.4968\n",
      "Epoch 33/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6963 - acc: 0.5127 - val_loss: 0.6937 - val_acc: 0.4968\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6971 - acc: 0.5104 - val_loss: 0.6936 - val_acc: 0.4968\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6959 - acc: 0.5045 - val_loss: 0.6936 - val_acc: 0.4968\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6961 - acc: 0.5069 - val_loss: 0.6935 - val_acc: 0.4968\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6959 - acc: 0.5075 - val_loss: 0.6935 - val_acc: 0.4968\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6958 - acc: 0.5063 - val_loss: 0.6936 - val_acc: 0.4968\n",
      "Epoch 39/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6972 - acc: 0.5035 - val_loss: 0.6937 - val_acc: 0.4968\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6933 - acc: 0.5214 - val_loss: 0.6939 - val_acc: 0.4968\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6946 - acc: 0.5080 - val_loss: 0.6941 - val_acc: 0.4968\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6944 - acc: 0.5099 - val_loss: 0.6944 - val_acc: 0.4968\n",
      "Epoch 43/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6950 - acc: 0.5086 - val_loss: 0.6945 - val_acc: 0.4968\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6956 - acc: 0.5095 - val_loss: 0.6945 - val_acc: 0.4968\n",
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6953 - acc: 0.5110 - val_loss: 0.6943 - val_acc: 0.4968\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6948 - acc: 0.5050 - val_loss: 0.6941 - val_acc: 0.4968\n",
      "Epoch 47/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6966 - acc: 0.5058 - val_loss: 0.6938 - val_acc: 0.4968\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6967 - acc: 0.5037 - val_loss: 0.6936 - val_acc: 0.4968\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6956 - acc: 0.5024 - val_loss: 0.6935 - val_acc: 0.4968\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6967 - acc: 0.4959 - val_loss: 0.6935 - val_acc: 0.4968\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6947 - acc: 0.5028 - val_loss: 0.6937 - val_acc: 0.4968\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6960 - acc: 0.5047 - val_loss: 0.6940 - val_acc: 0.4968\n",
      "Epoch 53/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6968 - acc: 0.5041 - val_loss: 0.6944 - val_acc: 0.4968\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6966 - acc: 0.5024 - val_loss: 0.6944 - val_acc: 0.4968\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6949 - acc: 0.5114 - val_loss: 0.6943 - val_acc: 0.4968\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6952 - acc: 0.5037 - val_loss: 0.6942 - val_acc: 0.4968\n",
      "Epoch 57/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6954 - acc: 0.5063 - val_loss: 0.6941 - val_acc: 0.4968\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6962 - acc: 0.5082 - val_loss: 0.6940 - val_acc: 0.4968\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6952 - acc: 0.5037 - val_loss: 0.6938 - val_acc: 0.4968\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6958 - acc: 0.5009 - val_loss: 0.6938 - val_acc: 0.4968\n",
      "Epoch 61/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6954 - acc: 0.5063 - val_loss: 0.6937 - val_acc: 0.4968\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6946 - acc: 0.5084 - val_loss: 0.6936 - val_acc: 0.4968\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6934 - acc: 0.5116 - val_loss: 0.6935 - val_acc: 0.4968\n",
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6944 - acc: 0.5063 - val_loss: 0.6935 - val_acc: 0.4968\n",
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6933 - acc: 0.5145 - val_loss: 0.6934 - val_acc: 0.4968\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6933 - acc: 0.5164 - val_loss: 0.6934 - val_acc: 0.4968\n",
      "Epoch 67/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6947 - acc: 0.5047 - val_loss: 0.6934 - val_acc: 0.4968\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6940 - acc: 0.5151 - val_loss: 0.6934 - val_acc: 0.4968\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6953 - acc: 0.4965 - val_loss: 0.6936 - val_acc: 0.4968\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6946 - acc: 0.5069 - val_loss: 0.6938 - val_acc: 0.4968\n",
      "sample weight :  [4.54113333e-04 1.83104198e-05 2.98933710e-05 ... 1.86785514e-05\n",
      " 1.84531485e-04 5.20329084e-05]\n",
      "1311332.0\n",
      "new_x, new_y (6182, 10, 4068) (6182,)\n",
      "WARNING:tensorflow:Layer lstm_152 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_153 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_154 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_155 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/70\n",
      "2/2 [==============================] - 6s 544ms/step - loss: 0.7119 - acc: 0.4879 - val_loss: 0.6950 - val_acc: 0.4690\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.7011 - acc: 0.5009 - val_loss: 0.6912 - val_acc: 0.5310\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.7006 - acc: 0.5037 - val_loss: 0.6921 - val_acc: 0.5310\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.7009 - acc: 0.5211 - val_loss: 0.6919 - val_acc: 0.5310\n",
      "Epoch 5/70\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.7025 - acc: 0.4994 - val_loss: 0.6913 - val_acc: 0.5310\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 0.6977 - acc: 0.5151 - val_loss: 0.6915 - val_acc: 0.5310\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.7015 - acc: 0.5015 - val_loss: 0.6924 - val_acc: 0.5310\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6997 - acc: 0.4959 - val_loss: 0.6934 - val_acc: 0.4690\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6978 - acc: 0.5030 - val_loss: 0.6936 - val_acc: 0.4690\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 127ms/step - loss: 0.6989 - acc: 0.5035 - val_loss: 0.6930 - val_acc: 0.5310\n",
      "Epoch 11/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6974 - acc: 0.5104 - val_loss: 0.6923 - val_acc: 0.5310\n",
      "Epoch 12/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6977 - acc: 0.5037 - val_loss: 0.6920 - val_acc: 0.5310\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 0.6982 - acc: 0.4937 - val_loss: 0.6918 - val_acc: 0.5310\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6975 - acc: 0.4912 - val_loss: 0.6918 - val_acc: 0.5310\n",
      "Epoch 15/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6973 - acc: 0.5024 - val_loss: 0.6919 - val_acc: 0.5310\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6942 - acc: 0.5121 - val_loss: 0.6917 - val_acc: 0.5310\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 0.6965 - acc: 0.5080 - val_loss: 0.6916 - val_acc: 0.5310\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6966 - acc: 0.5099 - val_loss: 0.6915 - val_acc: 0.5310\n",
      "Epoch 19/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6968 - acc: 0.5015 - val_loss: 0.6915 - val_acc: 0.5310\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6942 - acc: 0.5112 - val_loss: 0.6914 - val_acc: 0.5310\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6939 - acc: 0.5093 - val_loss: 0.6913 - val_acc: 0.5310\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6943 - acc: 0.5071 - val_loss: 0.6913 - val_acc: 0.5310\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6948 - acc: 0.5091 - val_loss: 0.6912 - val_acc: 0.5310\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6944 - acc: 0.5155 - val_loss: 0.6912 - val_acc: 0.5310\n",
      "Epoch 25/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6945 - acc: 0.5129 - val_loss: 0.6912 - val_acc: 0.5310\n",
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.6957 - acc: 0.5045 - val_loss: 0.6912 - val_acc: 0.5310\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6964 - acc: 0.5028 - val_loss: 0.6914 - val_acc: 0.5310\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6940 - acc: 0.5104 - val_loss: 0.6918 - val_acc: 0.5310\n",
      "Epoch 29/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6930 - acc: 0.5192 - val_loss: 0.6926 - val_acc: 0.5310\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6955 - acc: 0.4946 - val_loss: 0.6932 - val_acc: 0.4690\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6950 - acc: 0.4972 - val_loss: 0.6930 - val_acc: 0.5310\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6940 - acc: 0.5097 - val_loss: 0.6925 - val_acc: 0.5310\n",
      "Epoch 33/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6935 - acc: 0.5170 - val_loss: 0.6922 - val_acc: 0.5310\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6930 - acc: 0.5075 - val_loss: 0.6919 - val_acc: 0.5310\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6943 - acc: 0.5101 - val_loss: 0.6917 - val_acc: 0.5310\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6942 - acc: 0.5078 - val_loss: 0.6915 - val_acc: 0.5310\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6950 - acc: 0.4994 - val_loss: 0.6913 - val_acc: 0.5310\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6947 - acc: 0.5069 - val_loss: 0.6912 - val_acc: 0.5310\n",
      "Epoch 39/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6928 - acc: 0.5168 - val_loss: 0.6912 - val_acc: 0.5310\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6935 - acc: 0.5192 - val_loss: 0.6912 - val_acc: 0.5310\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6936 - acc: 0.5114 - val_loss: 0.6912 - val_acc: 0.5310\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6944 - acc: 0.5121 - val_loss: 0.6913 - val_acc: 0.5310\n",
      "Epoch 43/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6918 - acc: 0.5248 - val_loss: 0.6915 - val_acc: 0.5310\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6948 - acc: 0.5030 - val_loss: 0.6920 - val_acc: 0.5310\n",
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6939 - acc: 0.5035 - val_loss: 0.6925 - val_acc: 0.5310\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6939 - acc: 0.5015 - val_loss: 0.6929 - val_acc: 0.5310\n",
      "Epoch 47/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6955 - acc: 0.4903 - val_loss: 0.6930 - val_acc: 0.5310\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6940 - acc: 0.5110 - val_loss: 0.6930 - val_acc: 0.5310\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6960 - acc: 0.4933 - val_loss: 0.6926 - val_acc: 0.5310\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6942 - acc: 0.5056 - val_loss: 0.6920 - val_acc: 0.5310\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6939 - acc: 0.5035 - val_loss: 0.6916 - val_acc: 0.5310\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6938 - acc: 0.5037 - val_loss: 0.6914 - val_acc: 0.5310\n",
      "Epoch 53/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6936 - acc: 0.5121 - val_loss: 0.6913 - val_acc: 0.5310\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6939 - acc: 0.5063 - val_loss: 0.6913 - val_acc: 0.5310\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6939 - acc: 0.5106 - val_loss: 0.6913 - val_acc: 0.5310\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6936 - acc: 0.5177 - val_loss: 0.6912 - val_acc: 0.5310\n",
      "Epoch 57/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6932 - acc: 0.5157 - val_loss: 0.6912 - val_acc: 0.5310\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6937 - acc: 0.5192 - val_loss: 0.6914 - val_acc: 0.5310\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6929 - acc: 0.5190 - val_loss: 0.6916 - val_acc: 0.5310\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6957 - acc: 0.4929 - val_loss: 0.6919 - val_acc: 0.5310\n",
      "Epoch 61/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6938 - acc: 0.5024 - val_loss: 0.6920 - val_acc: 0.5310\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6937 - acc: 0.5108 - val_loss: 0.6920 - val_acc: 0.5310\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6931 - acc: 0.5043 - val_loss: 0.6919 - val_acc: 0.5310\n",
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6938 - acc: 0.4944 - val_loss: 0.6917 - val_acc: 0.5310\n",
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6953 - acc: 0.5004 - val_loss: 0.6915 - val_acc: 0.5310\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.6935 - acc: 0.5104 - val_loss: 0.6915 - val_acc: 0.5310\n",
      "Epoch 67/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6936 - acc: 0.5067 - val_loss: 0.6913 - val_acc: 0.5310\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6926 - acc: 0.5160 - val_loss: 0.6912 - val_acc: 0.5310\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6925 - acc: 0.5075 - val_loss: 0.6912 - val_acc: 0.5310\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6931 - acc: 0.5229 - val_loss: 0.6912 - val_acc: 0.5310\n",
      "sample weight :  [4.79174815e-04 1.93290050e-05 2.83150294e-05 ... 1.77163608e-05\n",
      " 1.74842029e-04 4.93070502e-05]\n",
      "1334963.0\n",
      "new_x, new_y (6182, 10, 4068) (6182,)\n",
      "WARNING:tensorflow:Layer lstm_156 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_157 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_158 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_159 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/70\n",
      "2/2 [==============================] - 5s 556ms/step - loss: 0.7020 - acc: 0.4858 - val_loss: 0.6925 - val_acc: 0.5181\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 0.6992 - acc: 0.5119 - val_loss: 0.6936 - val_acc: 0.5181\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 0.6981 - acc: 0.5179 - val_loss: 0.6934 - val_acc: 0.5181\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6973 - acc: 0.5106 - val_loss: 0.6928 - val_acc: 0.5181\n",
      "Epoch 5/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6994 - acc: 0.5015 - val_loss: 0.6925 - val_acc: 0.5181\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6959 - acc: 0.5093 - val_loss: 0.6925 - val_acc: 0.5181\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6946 - acc: 0.5179 - val_loss: 0.6925 - val_acc: 0.5181\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6970 - acc: 0.5082 - val_loss: 0.6926 - val_acc: 0.5181\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6965 - acc: 0.5035 - val_loss: 0.6926 - val_acc: 0.5181\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6954 - acc: 0.5157 - val_loss: 0.6926 - val_acc: 0.5181\n",
      "Epoch 11/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6958 - acc: 0.5136 - val_loss: 0.6926 - val_acc: 0.5181\n",
      "Epoch 12/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6951 - acc: 0.5095 - val_loss: 0.6925 - val_acc: 0.5181\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6979 - acc: 0.4935 - val_loss: 0.6925 - val_acc: 0.5181\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6963 - acc: 0.5088 - val_loss: 0.6925 - val_acc: 0.5181\n",
      "Epoch 15/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6957 - acc: 0.5067 - val_loss: 0.6925 - val_acc: 0.5181\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6962 - acc: 0.4983 - val_loss: 0.6926 - val_acc: 0.5181\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6929 - acc: 0.5211 - val_loss: 0.6927 - val_acc: 0.5181\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6964 - acc: 0.5099 - val_loss: 0.6927 - val_acc: 0.5181\n",
      "Epoch 19/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6941 - acc: 0.5190 - val_loss: 0.6929 - val_acc: 0.5181\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6951 - acc: 0.5183 - val_loss: 0.6928 - val_acc: 0.5181\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6935 - acc: 0.5160 - val_loss: 0.6927 - val_acc: 0.5181\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6944 - acc: 0.5140 - val_loss: 0.6925 - val_acc: 0.5181\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6939 - acc: 0.5170 - val_loss: 0.6925 - val_acc: 0.5181\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.6921 - acc: 0.5136 - val_loss: 0.6925 - val_acc: 0.5181\n",
      "Epoch 25/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6930 - acc: 0.5170 - val_loss: 0.6926 - val_acc: 0.5181\n",
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6925 - acc: 0.5209 - val_loss: 0.6926 - val_acc: 0.5181\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6933 - acc: 0.5091 - val_loss: 0.6926 - val_acc: 0.5181\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6926 - acc: 0.5157 - val_loss: 0.6926 - val_acc: 0.5181\n",
      "Epoch 29/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6943 - acc: 0.5138 - val_loss: 0.6928 - val_acc: 0.5181\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6939 - acc: 0.5211 - val_loss: 0.6928 - val_acc: 0.5181\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6939 - acc: 0.5129 - val_loss: 0.6927 - val_acc: 0.5181\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6940 - acc: 0.5166 - val_loss: 0.6926 - val_acc: 0.5181\n",
      "Epoch 33/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6927 - acc: 0.5201 - val_loss: 0.6925 - val_acc: 0.5181\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6940 - acc: 0.5097 - val_loss: 0.6925 - val_acc: 0.5181\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6941 - acc: 0.5160 - val_loss: 0.6925 - val_acc: 0.5181\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6931 - acc: 0.5198 - val_loss: 0.6926 - val_acc: 0.5181\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6935 - acc: 0.5246 - val_loss: 0.6927 - val_acc: 0.5181\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6935 - acc: 0.5160 - val_loss: 0.6929 - val_acc: 0.5181\n",
      "Epoch 39/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6932 - acc: 0.5246 - val_loss: 0.6929 - val_acc: 0.5181\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6935 - acc: 0.5183 - val_loss: 0.6928 - val_acc: 0.5181\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6932 - acc: 0.5147 - val_loss: 0.6927 - val_acc: 0.5181\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6935 - acc: 0.5218 - val_loss: 0.6926 - val_acc: 0.5181\n",
      "Epoch 43/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6923 - acc: 0.5188 - val_loss: 0.6925 - val_acc: 0.5181\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6930 - acc: 0.5091 - val_loss: 0.6925 - val_acc: 0.5181\n",
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6938 - acc: 0.5075 - val_loss: 0.6925 - val_acc: 0.5181\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6933 - acc: 0.5019 - val_loss: 0.6925 - val_acc: 0.5181\n",
      "Epoch 47/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6935 - acc: 0.5073 - val_loss: 0.6925 - val_acc: 0.5181\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6934 - acc: 0.5104 - val_loss: 0.6925 - val_acc: 0.5181\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6926 - acc: 0.5205 - val_loss: 0.6925 - val_acc: 0.5181\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6930 - acc: 0.5177 - val_loss: 0.6925 - val_acc: 0.5181\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6931 - acc: 0.5140 - val_loss: 0.6925 - val_acc: 0.5181\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6923 - acc: 0.5170 - val_loss: 0.6925 - val_acc: 0.5181\n",
      "Epoch 53/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6924 - acc: 0.5147 - val_loss: 0.6925 - val_acc: 0.5181\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6926 - acc: 0.5175 - val_loss: 0.6924 - val_acc: 0.5181\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6923 - acc: 0.5160 - val_loss: 0.6925 - val_acc: 0.5181\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6928 - acc: 0.5155 - val_loss: 0.6928 - val_acc: 0.5181\n",
      "Epoch 57/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6921 - acc: 0.5214 - val_loss: 0.6930 - val_acc: 0.5181\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6929 - acc: 0.5229 - val_loss: 0.6931 - val_acc: 0.5181\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6921 - acc: 0.5293 - val_loss: 0.6929 - val_acc: 0.5181\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6927 - acc: 0.5205 - val_loss: 0.6926 - val_acc: 0.5181\n",
      "Epoch 61/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6929 - acc: 0.5175 - val_loss: 0.6925 - val_acc: 0.5181\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6927 - acc: 0.5149 - val_loss: 0.6924 - val_acc: 0.5181\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6927 - acc: 0.5155 - val_loss: 0.6924 - val_acc: 0.5181\n",
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6924 - acc: 0.5220 - val_loss: 0.6924 - val_acc: 0.5181\n",
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6933 - acc: 0.5129 - val_loss: 0.6925 - val_acc: 0.5181\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6923 - acc: 0.5270 - val_loss: 0.6925 - val_acc: 0.5181\n",
      "Epoch 67/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6919 - acc: 0.5239 - val_loss: 0.6926 - val_acc: 0.5181\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6933 - acc: 0.5183 - val_loss: 0.6927 - val_acc: 0.5181\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6936 - acc: 0.5149 - val_loss: 0.6926 - val_acc: 0.5181\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.6928 - acc: 0.5276 - val_loss: 0.6925 - val_acc: 0.5181\n",
      "sample weight :  [4.53284190e-04 1.82921265e-05 2.99118671e-05 ... 1.87282110e-05\n",
      " 1.84933629e-04 5.20259801e-05]\n",
      "1332565.0\n",
      "new_x, new_y (6182, 10, 4068) (6182,)\n",
      "WARNING:tensorflow:Layer lstm_160 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_161 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_162 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_163 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/70\n",
      "2/2 [==============================] - 5s 541ms/step - loss: 0.7200 - acc: 0.5205 - val_loss: 0.7034 - val_acc: 0.5162\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.7062 - acc: 0.5233 - val_loss: 0.6948 - val_acc: 0.5162\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.7010 - acc: 0.5075 - val_loss: 0.6927 - val_acc: 0.5162\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6981 - acc: 0.5011 - val_loss: 0.6950 - val_acc: 0.4838\n",
      "Epoch 5/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.7032 - acc: 0.4881 - val_loss: 0.6965 - val_acc: 0.4838\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.7010 - acc: 0.4944 - val_loss: 0.6954 - val_acc: 0.4838\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6997 - acc: 0.4940 - val_loss: 0.6938 - val_acc: 0.4838\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6998 - acc: 0.4892 - val_loss: 0.6929 - val_acc: 0.5162\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6988 - acc: 0.4963 - val_loss: 0.6926 - val_acc: 0.5162\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6939 - acc: 0.5203 - val_loss: 0.6930 - val_acc: 0.5162\n",
      "Epoch 11/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6954 - acc: 0.5207 - val_loss: 0.6936 - val_acc: 0.5162\n",
      "Epoch 12/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6975 - acc: 0.5084 - val_loss: 0.6940 - val_acc: 0.5162\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6981 - acc: 0.5114 - val_loss: 0.6939 - val_acc: 0.5162\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6972 - acc: 0.5116 - val_loss: 0.6934 - val_acc: 0.5162\n",
      "Epoch 15/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6980 - acc: 0.5151 - val_loss: 0.6928 - val_acc: 0.5162\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6956 - acc: 0.5058 - val_loss: 0.6926 - val_acc: 0.5162\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6941 - acc: 0.5119 - val_loss: 0.6927 - val_acc: 0.5162\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6963 - acc: 0.5000 - val_loss: 0.6927 - val_acc: 0.5162\n",
      "Epoch 19/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6945 - acc: 0.5022 - val_loss: 0.6928 - val_acc: 0.5162\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6966 - acc: 0.5015 - val_loss: 0.6928 - val_acc: 0.5162\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6952 - acc: 0.5039 - val_loss: 0.6927 - val_acc: 0.5162\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6969 - acc: 0.4946 - val_loss: 0.6926 - val_acc: 0.5162\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6956 - acc: 0.5071 - val_loss: 0.6927 - val_acc: 0.5162\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6944 - acc: 0.5142 - val_loss: 0.6929 - val_acc: 0.5162\n",
      "Epoch 25/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6945 - acc: 0.5214 - val_loss: 0.6931 - val_acc: 0.5162\n",
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6967 - acc: 0.5084 - val_loss: 0.6931 - val_acc: 0.5162\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6956 - acc: 0.5209 - val_loss: 0.6931 - val_acc: 0.5162\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6944 - acc: 0.5183 - val_loss: 0.6930 - val_acc: 0.5162\n",
      "Epoch 29/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6957 - acc: 0.5108 - val_loss: 0.6928 - val_acc: 0.5162\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6946 - acc: 0.5106 - val_loss: 0.6927 - val_acc: 0.5162\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6970 - acc: 0.5004 - val_loss: 0.6926 - val_acc: 0.5162\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6944 - acc: 0.5116 - val_loss: 0.6926 - val_acc: 0.5162\n",
      "Epoch 33/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6935 - acc: 0.5125 - val_loss: 0.6926 - val_acc: 0.5162\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6957 - acc: 0.4991 - val_loss: 0.6926 - val_acc: 0.5162\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6944 - acc: 0.5073 - val_loss: 0.6926 - val_acc: 0.5162\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6957 - acc: 0.5050 - val_loss: 0.6926 - val_acc: 0.5162\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6938 - acc: 0.5192 - val_loss: 0.6926 - val_acc: 0.5162\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6931 - acc: 0.5179 - val_loss: 0.6926 - val_acc: 0.5162\n",
      "Epoch 39/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6950 - acc: 0.5127 - val_loss: 0.6926 - val_acc: 0.5162\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6943 - acc: 0.5127 - val_loss: 0.6926 - val_acc: 0.5162\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6979 - acc: 0.4840 - val_loss: 0.6926 - val_acc: 0.5162\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6941 - acc: 0.5093 - val_loss: 0.6926 - val_acc: 0.5162\n",
      "Epoch 43/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6950 - acc: 0.5054 - val_loss: 0.6926 - val_acc: 0.5162\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6940 - acc: 0.5058 - val_loss: 0.6926 - val_acc: 0.5162\n",
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6951 - acc: 0.5011 - val_loss: 0.6926 - val_acc: 0.5162\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6938 - acc: 0.5151 - val_loss: 0.6926 - val_acc: 0.5162\n",
      "Epoch 47/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6927 - acc: 0.5151 - val_loss: 0.6927 - val_acc: 0.5162\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6948 - acc: 0.5088 - val_loss: 0.6931 - val_acc: 0.5162\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6943 - acc: 0.5214 - val_loss: 0.6936 - val_acc: 0.5162\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6949 - acc: 0.5205 - val_loss: 0.6940 - val_acc: 0.5162\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6938 - acc: 0.5198 - val_loss: 0.6940 - val_acc: 0.5162\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6949 - acc: 0.5224 - val_loss: 0.6937 - val_acc: 0.5162\n",
      "Epoch 53/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6942 - acc: 0.5278 - val_loss: 0.6933 - val_acc: 0.5162\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6942 - acc: 0.5142 - val_loss: 0.6929 - val_acc: 0.5162\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6934 - acc: 0.5209 - val_loss: 0.6926 - val_acc: 0.5162\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6938 - acc: 0.5190 - val_loss: 0.6926 - val_acc: 0.5162\n",
      "Epoch 57/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6939 - acc: 0.5138 - val_loss: 0.6927 - val_acc: 0.5162\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6942 - acc: 0.4970 - val_loss: 0.6927 - val_acc: 0.5162\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6943 - acc: 0.4942 - val_loss: 0.6926 - val_acc: 0.5162\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6931 - acc: 0.5110 - val_loss: 0.6925 - val_acc: 0.5162\n",
      "Epoch 61/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6937 - acc: 0.5121 - val_loss: 0.6925 - val_acc: 0.5162\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6937 - acc: 0.5093 - val_loss: 0.6926 - val_acc: 0.5162\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6945 - acc: 0.5097 - val_loss: 0.6929 - val_acc: 0.5162\n",
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6924 - acc: 0.5190 - val_loss: 0.6931 - val_acc: 0.5162\n",
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6947 - acc: 0.5101 - val_loss: 0.6934 - val_acc: 0.5162\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6927 - acc: 0.5201 - val_loss: 0.6935 - val_acc: 0.5162\n",
      "Epoch 67/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6947 - acc: 0.5157 - val_loss: 0.6932 - val_acc: 0.5162\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6916 - acc: 0.5291 - val_loss: 0.6928 - val_acc: 0.5162\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6925 - val_acc: 0.5162\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6917 - acc: 0.5257 - val_loss: 0.6924 - val_acc: 0.5162\n",
      "sample weight :  [4.68845099e-04 1.89219535e-05 2.89446608e-05 ... 1.81828997e-05\n",
      " 1.79269784e-04 5.03104224e-05]\n",
      "1304795.0\n",
      "new_x, new_y (6182, 10, 4068) (6182,)\n",
      "WARNING:tensorflow:Layer lstm_164 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_165 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_166 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_167 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/70\n",
      "2/2 [==============================] - 5s 537ms/step - loss: 0.7018 - acc: 0.4989 - val_loss: 0.6931 - val_acc: 0.5065\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6986 - acc: 0.5052 - val_loss: 0.6931 - val_acc: 0.5065\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.7017 - acc: 0.4907 - val_loss: 0.6931 - val_acc: 0.5065\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.7013 - acc: 0.4847 - val_loss: 0.6933 - val_acc: 0.4935\n",
      "Epoch 5/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6979 - acc: 0.5063 - val_loss: 0.6933 - val_acc: 0.4935\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.7011 - acc: 0.4955 - val_loss: 0.6932 - val_acc: 0.4935\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 0.6977 - acc: 0.5075 - val_loss: 0.6932 - val_acc: 0.4935\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6969 - acc: 0.5065 - val_loss: 0.6931 - val_acc: 0.5065\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6974 - acc: 0.4991 - val_loss: 0.6931 - val_acc: 0.5065\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6975 - acc: 0.5035 - val_loss: 0.6931 - val_acc: 0.5065\n",
      "Epoch 11/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6974 - acc: 0.5026 - val_loss: 0.6931 - val_acc: 0.5065\n",
      "Epoch 12/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6971 - acc: 0.4950 - val_loss: 0.6931 - val_acc: 0.5065\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6969 - acc: 0.5060 - val_loss: 0.6931 - val_acc: 0.5065\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6962 - acc: 0.5004 - val_loss: 0.6931 - val_acc: 0.5065\n",
      "Epoch 15/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6968 - acc: 0.4970 - val_loss: 0.6931 - val_acc: 0.5065\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6960 - acc: 0.5019 - val_loss: 0.6931 - val_acc: 0.5065\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6961 - acc: 0.5035 - val_loss: 0.6932 - val_acc: 0.4935\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6960 - acc: 0.4976 - val_loss: 0.6932 - val_acc: 0.4935\n",
      "Epoch 19/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6962 - acc: 0.4970 - val_loss: 0.6932 - val_acc: 0.4935\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6963 - acc: 0.4955 - val_loss: 0.6932 - val_acc: 0.4935\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6966 - acc: 0.4896 - val_loss: 0.6932 - val_acc: 0.4935\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6969 - acc: 0.4968 - val_loss: 0.6932 - val_acc: 0.4935\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6958 - acc: 0.4950 - val_loss: 0.6931 - val_acc: 0.4935\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6945 - acc: 0.5093 - val_loss: 0.6931 - val_acc: 0.5065\n",
      "Epoch 25/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6960 - acc: 0.4931 - val_loss: 0.6930 - val_acc: 0.5065\n",
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6962 - acc: 0.5000 - val_loss: 0.6930 - val_acc: 0.5065\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6940 - acc: 0.5114 - val_loss: 0.6931 - val_acc: 0.5065\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6952 - acc: 0.5039 - val_loss: 0.6931 - val_acc: 0.5065\n",
      "Epoch 29/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6938 - acc: 0.5067 - val_loss: 0.6930 - val_acc: 0.5065\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6936 - acc: 0.5138 - val_loss: 0.6930 - val_acc: 0.5065\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6936 - acc: 0.5084 - val_loss: 0.6931 - val_acc: 0.5065\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6940 - acc: 0.5058 - val_loss: 0.6932 - val_acc: 0.4935\n",
      "Epoch 33/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6947 - acc: 0.4965 - val_loss: 0.6933 - val_acc: 0.4935\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6946 - acc: 0.4968 - val_loss: 0.6934 - val_acc: 0.4935\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6950 - acc: 0.4935 - val_loss: 0.6933 - val_acc: 0.4935\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6951 - acc: 0.4937 - val_loss: 0.6933 - val_acc: 0.4935\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6964 - acc: 0.4843 - val_loss: 0.6932 - val_acc: 0.4935\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6944 - acc: 0.4976 - val_loss: 0.6931 - val_acc: 0.5065\n",
      "Epoch 39/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6942 - acc: 0.4912 - val_loss: 0.6930 - val_acc: 0.5065\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6939 - acc: 0.4998 - val_loss: 0.6930 - val_acc: 0.5065\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6941 - acc: 0.5075 - val_loss: 0.6930 - val_acc: 0.5065\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6943 - acc: 0.5078 - val_loss: 0.6931 - val_acc: 0.5065\n",
      "Epoch 43/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6949 - acc: 0.4970 - val_loss: 0.6931 - val_acc: 0.4935\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6946 - acc: 0.5050 - val_loss: 0.6933 - val_acc: 0.4935\n",
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6937 - acc: 0.5043 - val_loss: 0.6935 - val_acc: 0.4935\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6949 - acc: 0.4953 - val_loss: 0.6935 - val_acc: 0.4935\n",
      "Epoch 47/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6940 - acc: 0.5080 - val_loss: 0.6933 - val_acc: 0.4935\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6952 - acc: 0.4873 - val_loss: 0.6933 - val_acc: 0.4935\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6949 - acc: 0.4963 - val_loss: 0.6933 - val_acc: 0.4935\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6941 - acc: 0.5011 - val_loss: 0.6933 - val_acc: 0.4935\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6939 - acc: 0.5022 - val_loss: 0.6931 - val_acc: 0.4935\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6933 - acc: 0.5166 - val_loss: 0.6930 - val_acc: 0.5110\n",
      "Epoch 53/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6937 - acc: 0.5047 - val_loss: 0.6930 - val_acc: 0.5065\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6930 - acc: 0.5095 - val_loss: 0.6930 - val_acc: 0.5065\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6934 - acc: 0.5099 - val_loss: 0.6929 - val_acc: 0.5065\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6936 - acc: 0.5024 - val_loss: 0.6929 - val_acc: 0.5065\n",
      "Epoch 57/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6940 - acc: 0.4905 - val_loss: 0.6930 - val_acc: 0.5136\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6936 - acc: 0.5037 - val_loss: 0.6930 - val_acc: 0.5259\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6933 - acc: 0.5030 - val_loss: 0.6931 - val_acc: 0.4935\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6934 - acc: 0.5060 - val_loss: 0.6932 - val_acc: 0.4935\n",
      "Epoch 61/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6931 - acc: 0.5000 - val_loss: 0.6932 - val_acc: 0.4935\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6936 - acc: 0.5045 - val_loss: 0.6930 - val_acc: 0.4929\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6940 - acc: 0.4978 - val_loss: 0.6928 - val_acc: 0.5084\n",
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6934 - acc: 0.5093 - val_loss: 0.6928 - val_acc: 0.5065\n",
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6941 - acc: 0.4922 - val_loss: 0.6928 - val_acc: 0.5065\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.6928 - acc: 0.5119 - val_loss: 0.6927 - val_acc: 0.5065\n",
      "Epoch 67/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6921 - acc: 0.5119 - val_loss: 0.6928 - val_acc: 0.5142\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6933 - acc: 0.5006 - val_loss: 0.6928 - val_acc: 0.5252\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6931 - acc: 0.5132 - val_loss: 0.6927 - val_acc: 0.5440\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6921 - acc: 0.5233 - val_loss: 0.6925 - val_acc: 0.5168\n",
      "sample weight :  [4.62578288e-04 1.87939303e-05 2.90043563e-05 ... 1.83527171e-05\n",
      " 1.79618482e-04 5.05584594e-05]\n",
      "1309516.0\n",
      "new_x, new_y (6182, 10, 4068) (6182,)\n",
      "WARNING:tensorflow:Layer lstm_168 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_169 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_170 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_171 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/70\n",
      "2/2 [==============================] - 5s 540ms/step - loss: 0.7052 - acc: 0.4937 - val_loss: 0.6925 - val_acc: 0.5194\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 158ms/step - loss: 0.7020 - acc: 0.4983 - val_loss: 0.6939 - val_acc: 0.5194\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 158ms/step - loss: 0.7027 - acc: 0.5028 - val_loss: 0.6941 - val_acc: 0.5194\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 0s 156ms/step - loss: 0.7011 - acc: 0.5091 - val_loss: 0.6928 - val_acc: 0.5194\n",
      "Epoch 5/70\n",
      "2/2 [==============================] - 0s 156ms/step - loss: 0.6984 - acc: 0.5173 - val_loss: 0.6924 - val_acc: 0.5194\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 157ms/step - loss: 0.6983 - acc: 0.5052 - val_loss: 0.6925 - val_acc: 0.5194\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6982 - acc: 0.5060 - val_loss: 0.6927 - val_acc: 0.5194\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6999 - acc: 0.5028 - val_loss: 0.6930 - val_acc: 0.5194\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6993 - acc: 0.4991 - val_loss: 0.6933 - val_acc: 0.4806\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6991 - acc: 0.4935 - val_loss: 0.6933 - val_acc: 0.4806\n",
      "Epoch 11/70\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.6979 - acc: 0.5073 - val_loss: 0.6930 - val_acc: 0.5194\n",
      "Epoch 12/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6990 - acc: 0.4981 - val_loss: 0.6927 - val_acc: 0.5194\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6977 - acc: 0.5002 - val_loss: 0.6925 - val_acc: 0.5194\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6974 - acc: 0.5017 - val_loss: 0.6924 - val_acc: 0.5194\n",
      "Epoch 15/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6978 - acc: 0.5002 - val_loss: 0.6924 - val_acc: 0.5194\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.6981 - acc: 0.4985 - val_loss: 0.6925 - val_acc: 0.5194\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6963 - acc: 0.5145 - val_loss: 0.6926 - val_acc: 0.5194\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6929 - acc: 0.5194 - val_loss: 0.6925 - val_acc: 0.5194\n",
      "Epoch 19/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6974 - acc: 0.5110 - val_loss: 0.6924 - val_acc: 0.5194\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6976 - acc: 0.5052 - val_loss: 0.6924 - val_acc: 0.5194\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6972 - acc: 0.4968 - val_loss: 0.6924 - val_acc: 0.5194\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6952 - acc: 0.5242 - val_loss: 0.6924 - val_acc: 0.5194\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6969 - acc: 0.5104 - val_loss: 0.6924 - val_acc: 0.5194\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6963 - acc: 0.5106 - val_loss: 0.6925 - val_acc: 0.5194\n",
      "Epoch 25/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6962 - acc: 0.5112 - val_loss: 0.6925 - val_acc: 0.5194\n",
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6952 - acc: 0.5119 - val_loss: 0.6924 - val_acc: 0.5194\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6971 - acc: 0.4985 - val_loss: 0.6924 - val_acc: 0.5194\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6950 - acc: 0.5047 - val_loss: 0.6926 - val_acc: 0.5194\n",
      "Epoch 29/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6954 - acc: 0.5084 - val_loss: 0.6929 - val_acc: 0.5194\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6958 - acc: 0.5026 - val_loss: 0.6928 - val_acc: 0.5194\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6963 - acc: 0.5019 - val_loss: 0.6926 - val_acc: 0.5194\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6960 - acc: 0.4942 - val_loss: 0.6924 - val_acc: 0.5194\n",
      "Epoch 33/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6946 - acc: 0.5052 - val_loss: 0.6923 - val_acc: 0.5194\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6935 - acc: 0.5134 - val_loss: 0.6925 - val_acc: 0.5194\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6961 - acc: 0.5084 - val_loss: 0.6925 - val_acc: 0.5194\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6942 - acc: 0.5188 - val_loss: 0.6924 - val_acc: 0.5194\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6954 - acc: 0.5030 - val_loss: 0.6924 - val_acc: 0.5194\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6956 - acc: 0.5013 - val_loss: 0.6923 - val_acc: 0.5194\n",
      "Epoch 39/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6958 - acc: 0.5006 - val_loss: 0.6923 - val_acc: 0.5194\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6928 - acc: 0.5145 - val_loss: 0.6924 - val_acc: 0.5194\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6947 - acc: 0.5019 - val_loss: 0.6924 - val_acc: 0.5194\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6945 - acc: 0.5127 - val_loss: 0.6924 - val_acc: 0.5194\n",
      "Epoch 43/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6959 - acc: 0.4950 - val_loss: 0.6923 - val_acc: 0.5194\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6950 - acc: 0.4925 - val_loss: 0.6923 - val_acc: 0.5194\n",
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6934 - acc: 0.5168 - val_loss: 0.6923 - val_acc: 0.5194\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6947 - acc: 0.5073 - val_loss: 0.6923 - val_acc: 0.5194\n",
      "Epoch 47/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6933 - acc: 0.5052 - val_loss: 0.6923 - val_acc: 0.5194\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6932 - acc: 0.5194 - val_loss: 0.6924 - val_acc: 0.5194\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6935 - acc: 0.5166 - val_loss: 0.6923 - val_acc: 0.5194\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6933 - acc: 0.5121 - val_loss: 0.6922 - val_acc: 0.5194\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6932 - acc: 0.5093 - val_loss: 0.6922 - val_acc: 0.5194\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6926 - acc: 0.5224 - val_loss: 0.6923 - val_acc: 0.5194\n",
      "Epoch 53/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6951 - acc: 0.4996 - val_loss: 0.6922 - val_acc: 0.5194\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6931 - acc: 0.5125 - val_loss: 0.6920 - val_acc: 0.5194\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6936 - acc: 0.5106 - val_loss: 0.6919 - val_acc: 0.5194\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6935 - acc: 0.5078 - val_loss: 0.6919 - val_acc: 0.5194\n",
      "Epoch 57/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6934 - acc: 0.5106 - val_loss: 0.6919 - val_acc: 0.5194\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6923 - acc: 0.5108 - val_loss: 0.6919 - val_acc: 0.5233\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6917 - acc: 0.5153 - val_loss: 0.6920 - val_acc: 0.5530\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6907 - acc: 0.5201 - val_loss: 0.6917 - val_acc: 0.5608\n",
      "Epoch 61/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6912 - acc: 0.5205 - val_loss: 0.6912 - val_acc: 0.5246\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6901 - acc: 0.5306 - val_loss: 0.6910 - val_acc: 0.5194\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6897 - acc: 0.5362 - val_loss: 0.6905 - val_acc: 0.5194\n",
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6896 - acc: 0.5272 - val_loss: 0.6902 - val_acc: 0.5194\n",
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6889 - acc: 0.5371 - val_loss: 0.6896 - val_acc: 0.5233\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6856 - acc: 0.5466 - val_loss: 0.6890 - val_acc: 0.5278\n",
      "Epoch 67/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6856 - acc: 0.5470 - val_loss: 0.6876 - val_acc: 0.5589\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6800 - acc: 0.5761 - val_loss: 0.6868 - val_acc: 0.5757\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6762 - acc: 0.6063 - val_loss: 0.6860 - val_acc: 0.5563\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6747 - acc: 0.5938 - val_loss: 0.6829 - val_acc: 0.5776\n",
      "sample weight :  [4.75017558e-04 1.88660351e-05 2.56586720e-05 ... 2.06126904e-05\n",
      " 1.55541461e-04 4.82615848e-05]\n",
      "1301579.0\n",
      "new_x, new_y (6182, 10, 4068) (6182,)\n",
      "WARNING:tensorflow:Layer lstm_172 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_173 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_174 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_175 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/70\n",
      "2/2 [==============================] - 5s 597ms/step - loss: 0.6984 - acc: 0.4991 - val_loss: 0.6954 - val_acc: 0.5097\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6944 - acc: 0.5289 - val_loss: 0.6958 - val_acc: 0.5097\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6979 - acc: 0.5101 - val_loss: 0.6958 - val_acc: 0.5097\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6959 - acc: 0.5345 - val_loss: 0.6947 - val_acc: 0.5097\n",
      "Epoch 5/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6971 - acc: 0.5194 - val_loss: 0.6934 - val_acc: 0.5097\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6977 - acc: 0.5004 - val_loss: 0.6930 - val_acc: 0.5097\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6960 - acc: 0.5112 - val_loss: 0.6932 - val_acc: 0.4903\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6998 - acc: 0.4827 - val_loss: 0.6934 - val_acc: 0.4903\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.6962 - acc: 0.5015 - val_loss: 0.6932 - val_acc: 0.4903\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6980 - acc: 0.5006 - val_loss: 0.6930 - val_acc: 0.5097\n",
      "Epoch 11/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6948 - acc: 0.5177 - val_loss: 0.6932 - val_acc: 0.5097\n",
      "Epoch 12/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6964 - acc: 0.5108 - val_loss: 0.6940 - val_acc: 0.5097\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6961 - acc: 0.5106 - val_loss: 0.6946 - val_acc: 0.5097\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6968 - acc: 0.5160 - val_loss: 0.6948 - val_acc: 0.5097\n",
      "Epoch 15/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6960 - acc: 0.5151 - val_loss: 0.6944 - val_acc: 0.5097\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6953 - acc: 0.5229 - val_loss: 0.6938 - val_acc: 0.5097\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6967 - acc: 0.5041 - val_loss: 0.6934 - val_acc: 0.5097\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6960 - acc: 0.5170 - val_loss: 0.6931 - val_acc: 0.5097\n",
      "Epoch 19/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6963 - acc: 0.5000 - val_loss: 0.6930 - val_acc: 0.5097\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6953 - acc: 0.5119 - val_loss: 0.6930 - val_acc: 0.5097\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6931 - acc: 0.5211 - val_loss: 0.6930 - val_acc: 0.5097\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6951 - acc: 0.5050 - val_loss: 0.6932 - val_acc: 0.5097\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6948 - acc: 0.5125 - val_loss: 0.6934 - val_acc: 0.5097\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6948 - acc: 0.5045 - val_loss: 0.6937 - val_acc: 0.5097\n",
      "Epoch 25/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6927 - acc: 0.5157 - val_loss: 0.6940 - val_acc: 0.5097\n",
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6943 - acc: 0.5095 - val_loss: 0.6943 - val_acc: 0.5097\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6949 - acc: 0.5110 - val_loss: 0.6943 - val_acc: 0.5097\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6941 - acc: 0.5175 - val_loss: 0.6943 - val_acc: 0.5097\n",
      "Epoch 29/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6930 - acc: 0.5280 - val_loss: 0.6941 - val_acc: 0.5097\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6926 - acc: 0.5181 - val_loss: 0.6940 - val_acc: 0.5097\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6908 - acc: 0.5377 - val_loss: 0.6940 - val_acc: 0.5097\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6962 - acc: 0.5065 - val_loss: 0.6938 - val_acc: 0.5097\n",
      "Epoch 33/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6926 - acc: 0.5224 - val_loss: 0.6936 - val_acc: 0.5097\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6919 - acc: 0.5203 - val_loss: 0.6933 - val_acc: 0.5097\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 159ms/step - loss: 0.6926 - acc: 0.5205 - val_loss: 0.6931 - val_acc: 0.5097\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 170ms/step - loss: 0.6937 - acc: 0.5084 - val_loss: 0.6930 - val_acc: 0.5097\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 171ms/step - loss: 0.6947 - acc: 0.5039 - val_loss: 0.6929 - val_acc: 0.5097\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 169ms/step - loss: 0.6956 - acc: 0.4912 - val_loss: 0.6929 - val_acc: 0.5097\n",
      "Epoch 39/70\n",
      "2/2 [==============================] - 0s 163ms/step - loss: 0.6934 - acc: 0.5073 - val_loss: 0.6930 - val_acc: 0.5097\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6934 - acc: 0.5073 - val_loss: 0.6931 - val_acc: 0.5097\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6928 - acc: 0.5121 - val_loss: 0.6936 - val_acc: 0.5097\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6950 - acc: 0.5084 - val_loss: 0.6939 - val_acc: 0.5097\n",
      "Epoch 43/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6927 - acc: 0.5198 - val_loss: 0.6940 - val_acc: 0.5097\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6944 - acc: 0.5097 - val_loss: 0.6940 - val_acc: 0.5097\n",
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6933 - acc: 0.5267 - val_loss: 0.6939 - val_acc: 0.5097\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6948 - acc: 0.5127 - val_loss: 0.6936 - val_acc: 0.5097\n",
      "Epoch 47/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6914 - acc: 0.5237 - val_loss: 0.6936 - val_acc: 0.5097\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6923 - acc: 0.5237 - val_loss: 0.6936 - val_acc: 0.5097\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6925 - acc: 0.5237 - val_loss: 0.6934 - val_acc: 0.5097\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6931 - acc: 0.5203 - val_loss: 0.6934 - val_acc: 0.5097\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6922 - acc: 0.5196 - val_loss: 0.6934 - val_acc: 0.5097\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.6944 - acc: 0.5069 - val_loss: 0.6934 - val_acc: 0.5097\n",
      "Epoch 53/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6942 - acc: 0.5097 - val_loss: 0.6935 - val_acc: 0.5097\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6937 - acc: 0.5140 - val_loss: 0.6937 - val_acc: 0.5097\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6938 - acc: 0.5222 - val_loss: 0.6939 - val_acc: 0.5097\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6925 - acc: 0.5248 - val_loss: 0.6939 - val_acc: 0.5097\n",
      "Epoch 57/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6927 - acc: 0.5194 - val_loss: 0.6936 - val_acc: 0.5097\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6928 - acc: 0.5151 - val_loss: 0.6934 - val_acc: 0.5097\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6939 - acc: 0.5211 - val_loss: 0.6932 - val_acc: 0.5097\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6920 - acc: 0.5214 - val_loss: 0.6934 - val_acc: 0.5097\n",
      "Epoch 61/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6926 - acc: 0.5168 - val_loss: 0.6936 - val_acc: 0.5097\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6930 - acc: 0.5181 - val_loss: 0.6938 - val_acc: 0.5097\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6925 - acc: 0.5233 - val_loss: 0.6938 - val_acc: 0.5097\n",
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6929 - acc: 0.5246 - val_loss: 0.6939 - val_acc: 0.5097\n",
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6915 - acc: 0.5291 - val_loss: 0.6938 - val_acc: 0.5097\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6930 - acc: 0.5244 - val_loss: 0.6936 - val_acc: 0.5097\n",
      "Epoch 67/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6914 - acc: 0.5270 - val_loss: 0.6934 - val_acc: 0.5097\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6924 - acc: 0.5235 - val_loss: 0.6933 - val_acc: 0.5097\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6924 - acc: 0.5263 - val_loss: 0.6933 - val_acc: 0.5097\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6919 - acc: 0.5244 - val_loss: 0.6932 - val_acc: 0.5097\n",
      "sample weight :  [4.52542326e-04 1.79804475e-05 2.69911784e-05 ... 2.15724263e-05\n",
      " 1.63398440e-04 5.05692643e-05]\n",
      "1306324.0\n",
      "new_x, new_y (6182, 10, 4068) (6182,)\n",
      "WARNING:tensorflow:Layer lstm_176 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_177 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_178 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_179 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/70\n",
      "2/2 [==============================] - 6s 542ms/step - loss: 0.6976 - acc: 0.5069 - val_loss: 0.6946 - val_acc: 0.4942\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.7000 - acc: 0.5006 - val_loss: 0.6952 - val_acc: 0.4942\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6990 - acc: 0.5063 - val_loss: 0.6944 - val_acc: 0.4942\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6994 - acc: 0.5009 - val_loss: 0.6936 - val_acc: 0.4942\n",
      "Epoch 5/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6998 - acc: 0.4903 - val_loss: 0.6933 - val_acc: 0.4942\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6982 - acc: 0.4972 - val_loss: 0.6934 - val_acc: 0.4942\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 0.6978 - acc: 0.5015 - val_loss: 0.6934 - val_acc: 0.4942\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6974 - acc: 0.4978 - val_loss: 0.6934 - val_acc: 0.4942\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 151ms/step - loss: 0.6964 - acc: 0.5047 - val_loss: 0.6936 - val_acc: 0.4942\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6964 - acc: 0.5024 - val_loss: 0.6935 - val_acc: 0.4942\n",
      "Epoch 11/70\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.6952 - acc: 0.5039 - val_loss: 0.6933 - val_acc: 0.4942\n",
      "Epoch 12/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6946 - acc: 0.5101 - val_loss: 0.6932 - val_acc: 0.4942\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6968 - acc: 0.4944 - val_loss: 0.6932 - val_acc: 0.4942\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6949 - acc: 0.5093 - val_loss: 0.6932 - val_acc: 0.4942\n",
      "Epoch 15/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6965 - acc: 0.4916 - val_loss: 0.6933 - val_acc: 0.4942\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6946 - acc: 0.5026 - val_loss: 0.6937 - val_acc: 0.4942\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6951 - acc: 0.4987 - val_loss: 0.6938 - val_acc: 0.4942\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6960 - acc: 0.4991 - val_loss: 0.6936 - val_acc: 0.4942\n",
      "Epoch 19/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6967 - acc: 0.4996 - val_loss: 0.6933 - val_acc: 0.4942\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6943 - acc: 0.5084 - val_loss: 0.6931 - val_acc: 0.5058\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6964 - acc: 0.4853 - val_loss: 0.6931 - val_acc: 0.5058\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6966 - acc: 0.4860 - val_loss: 0.6932 - val_acc: 0.4942\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6942 - acc: 0.5063 - val_loss: 0.6934 - val_acc: 0.4942\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6944 - acc: 0.4961 - val_loss: 0.6940 - val_acc: 0.4942\n",
      "Epoch 25/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6943 - acc: 0.5088 - val_loss: 0.6943 - val_acc: 0.4942\n",
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6951 - acc: 0.5041 - val_loss: 0.6941 - val_acc: 0.4942\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6944 - acc: 0.5153 - val_loss: 0.6938 - val_acc: 0.4942\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6943 - acc: 0.5138 - val_loss: 0.6935 - val_acc: 0.4942\n",
      "Epoch 29/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6946 - acc: 0.5060 - val_loss: 0.6933 - val_acc: 0.4942\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6947 - acc: 0.4970 - val_loss: 0.6932 - val_acc: 0.4942\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6949 - acc: 0.4991 - val_loss: 0.6933 - val_acc: 0.4942\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6939 - acc: 0.5045 - val_loss: 0.6934 - val_acc: 0.4942\n",
      "Epoch 33/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6934 - acc: 0.5030 - val_loss: 0.6935 - val_acc: 0.4942\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6937 - acc: 0.5078 - val_loss: 0.6937 - val_acc: 0.4942\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6943 - acc: 0.5056 - val_loss: 0.6937 - val_acc: 0.4942\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6948 - acc: 0.4968 - val_loss: 0.6934 - val_acc: 0.4942\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6949 - acc: 0.4894 - val_loss: 0.6932 - val_acc: 0.4942\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6952 - acc: 0.4929 - val_loss: 0.6931 - val_acc: 0.5058\n",
      "Epoch 39/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6937 - acc: 0.5015 - val_loss: 0.6931 - val_acc: 0.5058\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6934 - acc: 0.5080 - val_loss: 0.6932 - val_acc: 0.4942\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6937 - acc: 0.5080 - val_loss: 0.6935 - val_acc: 0.4942\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6937 - acc: 0.5082 - val_loss: 0.6939 - val_acc: 0.4942\n",
      "Epoch 43/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6934 - acc: 0.5108 - val_loss: 0.6945 - val_acc: 0.4942\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6950 - acc: 0.5099 - val_loss: 0.6947 - val_acc: 0.4942\n",
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6932 - acc: 0.5097 - val_loss: 0.6946 - val_acc: 0.4942\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6938 - acc: 0.5179 - val_loss: 0.6943 - val_acc: 0.4942\n",
      "Epoch 47/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6936 - acc: 0.5110 - val_loss: 0.6940 - val_acc: 0.4942\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6942 - acc: 0.5082 - val_loss: 0.6937 - val_acc: 0.4942\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6936 - acc: 0.5019 - val_loss: 0.6937 - val_acc: 0.4942\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6941 - acc: 0.5002 - val_loss: 0.6936 - val_acc: 0.4942\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6938 - acc: 0.5037 - val_loss: 0.6934 - val_acc: 0.4942\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6939 - acc: 0.5013 - val_loss: 0.6934 - val_acc: 0.4942\n",
      "Epoch 53/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6932 - acc: 0.5037 - val_loss: 0.6935 - val_acc: 0.4942\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6933 - acc: 0.5123 - val_loss: 0.6936 - val_acc: 0.4942\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6934 - acc: 0.5030 - val_loss: 0.6939 - val_acc: 0.4942\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6930 - acc: 0.5164 - val_loss: 0.6939 - val_acc: 0.4942\n",
      "Epoch 57/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6932 - acc: 0.5136 - val_loss: 0.6938 - val_acc: 0.4942\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6930 - acc: 0.5108 - val_loss: 0.6938 - val_acc: 0.4942\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6934 - acc: 0.5116 - val_loss: 0.6937 - val_acc: 0.4942\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6937 - acc: 0.5091 - val_loss: 0.6935 - val_acc: 0.4942\n",
      "Epoch 61/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6933 - acc: 0.5091 - val_loss: 0.6933 - val_acc: 0.4942\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6934 - acc: 0.5009 - val_loss: 0.6931 - val_acc: 0.4942\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6939 - acc: 0.4959 - val_loss: 0.6930 - val_acc: 0.5058\n",
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6937 - acc: 0.5028 - val_loss: 0.6930 - val_acc: 0.5058\n",
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6927 - acc: 0.5190 - val_loss: 0.6930 - val_acc: 0.5142\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.6935 - acc: 0.5060 - val_loss: 0.6930 - val_acc: 0.5058\n",
      "Epoch 67/70\n",
      "2/2 [==============================] - 0s 127ms/step - loss: 0.6938 - acc: 0.4976 - val_loss: 0.6931 - val_acc: 0.4942\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6926 - acc: 0.5123 - val_loss: 0.6932 - val_acc: 0.4942\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6935 - acc: 0.5030 - val_loss: 0.6936 - val_acc: 0.4942\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6919 - acc: 0.5170 - val_loss: 0.6940 - val_acc: 0.4942\n",
      "sample weight :  [4.67993640e-04 1.86536233e-05 2.61181758e-05 ... 2.07622798e-05\n",
      " 1.58246130e-04 4.87793050e-05]\n",
      "1295788.0\n",
      "new_x, new_y (6182, 10, 4068) (6182,)\n",
      "WARNING:tensorflow:Layer lstm_180 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_181 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_182 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_183 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/70\n",
      "2/2 [==============================] - 5s 538ms/step - loss: 0.7063 - acc: 0.4974 - val_loss: 0.6923 - val_acc: 0.5330\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6996 - acc: 0.5125 - val_loss: 0.6981 - val_acc: 0.4670\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.7036 - acc: 0.4935 - val_loss: 0.6984 - val_acc: 0.4670\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.7020 - acc: 0.5078 - val_loss: 0.6953 - val_acc: 0.4670\n",
      "Epoch 5/70\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 0.7000 - acc: 0.5035 - val_loss: 0.6930 - val_acc: 0.5330\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6975 - acc: 0.5093 - val_loss: 0.6922 - val_acc: 0.5330\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.7030 - acc: 0.4981 - val_loss: 0.6926 - val_acc: 0.5330\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6974 - acc: 0.5119 - val_loss: 0.6936 - val_acc: 0.4670\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6982 - acc: 0.5006 - val_loss: 0.6947 - val_acc: 0.4670\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6996 - acc: 0.5035 - val_loss: 0.6956 - val_acc: 0.4670\n",
      "Epoch 11/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6994 - acc: 0.5041 - val_loss: 0.6960 - val_acc: 0.4670\n",
      "Epoch 12/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6995 - acc: 0.5017 - val_loss: 0.6954 - val_acc: 0.4670\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 0.6962 - acc: 0.5198 - val_loss: 0.6944 - val_acc: 0.4670\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.7003 - acc: 0.4871 - val_loss: 0.6932 - val_acc: 0.4670\n",
      "Epoch 15/70\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.6957 - acc: 0.5097 - val_loss: 0.6924 - val_acc: 0.5330\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6970 - acc: 0.5054 - val_loss: 0.6922 - val_acc: 0.5330\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6971 - acc: 0.5080 - val_loss: 0.6924 - val_acc: 0.5330\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6988 - acc: 0.4892 - val_loss: 0.6930 - val_acc: 0.5330\n",
      "Epoch 19/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6967 - acc: 0.5058 - val_loss: 0.6934 - val_acc: 0.4670\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6976 - acc: 0.4957 - val_loss: 0.6933 - val_acc: 0.4670\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6930 - acc: 0.5272 - val_loss: 0.6930 - val_acc: 0.5330\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6954 - acc: 0.5045 - val_loss: 0.6931 - val_acc: 0.5550\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6985 - acc: 0.4896 - val_loss: 0.6937 - val_acc: 0.4670\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6969 - acc: 0.5028 - val_loss: 0.6942 - val_acc: 0.4670\n",
      "Epoch 25/70\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.6981 - acc: 0.4950 - val_loss: 0.6945 - val_acc: 0.4670\n",
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6949 - acc: 0.5101 - val_loss: 0.6947 - val_acc: 0.4670\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6949 - acc: 0.5093 - val_loss: 0.6950 - val_acc: 0.4670\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6940 - acc: 0.5106 - val_loss: 0.6947 - val_acc: 0.4670\n",
      "Epoch 29/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6962 - acc: 0.4933 - val_loss: 0.6940 - val_acc: 0.4670\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6963 - acc: 0.5022 - val_loss: 0.6933 - val_acc: 0.4670\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6965 - acc: 0.4937 - val_loss: 0.6931 - val_acc: 0.5569\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6953 - acc: 0.5024 - val_loss: 0.6929 - val_acc: 0.5330\n",
      "Epoch 33/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6939 - acc: 0.5099 - val_loss: 0.6925 - val_acc: 0.5330\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6958 - acc: 0.4987 - val_loss: 0.6921 - val_acc: 0.5330\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6958 - acc: 0.4978 - val_loss: 0.6919 - val_acc: 0.5330\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6960 - acc: 0.4950 - val_loss: 0.6920 - val_acc: 0.5330\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6940 - acc: 0.5088 - val_loss: 0.6926 - val_acc: 0.5330\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6940 - acc: 0.5041 - val_loss: 0.6936 - val_acc: 0.4670\n",
      "Epoch 39/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6955 - acc: 0.4965 - val_loss: 0.6950 - val_acc: 0.4670\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6965 - acc: 0.4965 - val_loss: 0.6970 - val_acc: 0.4670\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6963 - acc: 0.4978 - val_loss: 0.6979 - val_acc: 0.4670\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6959 - acc: 0.5006 - val_loss: 0.6966 - val_acc: 0.4670\n",
      "Epoch 43/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6951 - acc: 0.5047 - val_loss: 0.6948 - val_acc: 0.4670\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6954 - acc: 0.4963 - val_loss: 0.6934 - val_acc: 0.4670\n",
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6958 - acc: 0.4884 - val_loss: 0.6926 - val_acc: 0.5330\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6940 - acc: 0.5095 - val_loss: 0.6922 - val_acc: 0.5330\n",
      "Epoch 47/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6938 - acc: 0.5110 - val_loss: 0.6917 - val_acc: 0.5330\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6940 - acc: 0.5013 - val_loss: 0.6917 - val_acc: 0.5330\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6948 - acc: 0.5052 - val_loss: 0.6922 - val_acc: 0.5330\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6938 - acc: 0.5045 - val_loss: 0.6928 - val_acc: 0.5401\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6947 - acc: 0.4983 - val_loss: 0.6937 - val_acc: 0.4670\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6958 - acc: 0.4929 - val_loss: 0.6947 - val_acc: 0.4670\n",
      "Epoch 53/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6949 - acc: 0.4935 - val_loss: 0.6955 - val_acc: 0.4670\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6952 - acc: 0.4909 - val_loss: 0.6956 - val_acc: 0.4670\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6935 - acc: 0.5069 - val_loss: 0.6952 - val_acc: 0.4670\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6933 - acc: 0.5080 - val_loss: 0.6941 - val_acc: 0.4670\n",
      "Epoch 57/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6940 - acc: 0.5000 - val_loss: 0.6932 - val_acc: 0.4670\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6932 - acc: 0.5088 - val_loss: 0.6928 - val_acc: 0.5614\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6938 - acc: 0.5028 - val_loss: 0.6928 - val_acc: 0.5679\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6939 - acc: 0.5030 - val_loss: 0.6931 - val_acc: 0.4825\n",
      "Epoch 61/70\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 0.6939 - acc: 0.5032 - val_loss: 0.6934 - val_acc: 0.4670\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6948 - acc: 0.4927 - val_loss: 0.6935 - val_acc: 0.4670\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6925 - acc: 0.5142 - val_loss: 0.6932 - val_acc: 0.4683\n",
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6931 - acc: 0.5086 - val_loss: 0.6929 - val_acc: 0.4845\n",
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6930 - acc: 0.5108 - val_loss: 0.6925 - val_acc: 0.5576\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6927 - acc: 0.5110 - val_loss: 0.6923 - val_acc: 0.5602\n",
      "Epoch 67/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6928 - acc: 0.5147 - val_loss: 0.6921 - val_acc: 0.5602\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6930 - acc: 0.5132 - val_loss: 0.6923 - val_acc: 0.5414\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6920 - acc: 0.5106 - val_loss: 0.6926 - val_acc: 0.5006\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6906 - acc: 0.5347 - val_loss: 0.6923 - val_acc: 0.5116\n",
      "sample weight :  [4.66171070e-04 1.87555334e-05 2.63255159e-05 ... 2.05257865e-05\n",
      " 1.60578234e-04 4.86972924e-05]\n",
      "1313146.0\n",
      "new_x, new_y (6182, 10, 4068) (6182,)\n",
      "WARNING:tensorflow:Layer lstm_184 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_185 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_186 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_187 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/70\n",
      "2/2 [==============================] - 5s 533ms/step - loss: 0.7000 - acc: 0.5084 - val_loss: 0.6919 - val_acc: 0.5246\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 0.7009 - acc: 0.4985 - val_loss: 0.6923 - val_acc: 0.5246\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.7006 - acc: 0.4987 - val_loss: 0.6922 - val_acc: 0.5246\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6983 - acc: 0.5112 - val_loss: 0.6920 - val_acc: 0.5246\n",
      "Epoch 5/70\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 0.6993 - acc: 0.5045 - val_loss: 0.6920 - val_acc: 0.5246\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.7006 - acc: 0.5045 - val_loss: 0.6920 - val_acc: 0.5246\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.7003 - acc: 0.4963 - val_loss: 0.6919 - val_acc: 0.5246\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 158ms/step - loss: 0.6979 - acc: 0.5054 - val_loss: 0.6920 - val_acc: 0.5246\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 158ms/step - loss: 0.6991 - acc: 0.5091 - val_loss: 0.6920 - val_acc: 0.5246\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 158ms/step - loss: 0.6988 - acc: 0.4953 - val_loss: 0.6920 - val_acc: 0.5246\n",
      "Epoch 11/70\n",
      "2/2 [==============================] - 0s 155ms/step - loss: 0.6982 - acc: 0.5045 - val_loss: 0.6919 - val_acc: 0.5246\n",
      "Epoch 12/70\n",
      "2/2 [==============================] - 0s 160ms/step - loss: 0.6973 - acc: 0.5037 - val_loss: 0.6920 - val_acc: 0.5246\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6972 - acc: 0.4946 - val_loss: 0.6920 - val_acc: 0.5246\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6973 - acc: 0.5127 - val_loss: 0.6921 - val_acc: 0.5246\n",
      "Epoch 15/70\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 0.6974 - acc: 0.5095 - val_loss: 0.6921 - val_acc: 0.5246\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6983 - acc: 0.5073 - val_loss: 0.6920 - val_acc: 0.5246\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6954 - acc: 0.5108 - val_loss: 0.6919 - val_acc: 0.5246\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.6963 - acc: 0.5037 - val_loss: 0.6922 - val_acc: 0.5246\n",
      "Epoch 19/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6958 - acc: 0.5078 - val_loss: 0.6926 - val_acc: 0.5246\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6941 - acc: 0.5095 - val_loss: 0.6926 - val_acc: 0.5246\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6940 - acc: 0.5134 - val_loss: 0.6922 - val_acc: 0.5246\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6946 - acc: 0.5091 - val_loss: 0.6919 - val_acc: 0.5246\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6951 - acc: 0.5091 - val_loss: 0.6922 - val_acc: 0.5246\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6949 - acc: 0.5173 - val_loss: 0.6925 - val_acc: 0.5246\n",
      "Epoch 25/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6934 - acc: 0.5222 - val_loss: 0.6927 - val_acc: 0.5246\n",
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6955 - acc: 0.5166 - val_loss: 0.6924 - val_acc: 0.5246\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6946 - acc: 0.5188 - val_loss: 0.6921 - val_acc: 0.5246\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6939 - acc: 0.5125 - val_loss: 0.6919 - val_acc: 0.5246\n",
      "Epoch 29/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6945 - acc: 0.5179 - val_loss: 0.6920 - val_acc: 0.5246\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6963 - acc: 0.4970 - val_loss: 0.6920 - val_acc: 0.5246\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6935 - acc: 0.5110 - val_loss: 0.6920 - val_acc: 0.5246\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6934 - acc: 0.5153 - val_loss: 0.6919 - val_acc: 0.5246\n",
      "Epoch 33/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6930 - acc: 0.5162 - val_loss: 0.6919 - val_acc: 0.5246\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6944 - acc: 0.5145 - val_loss: 0.6919 - val_acc: 0.5246\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6957 - acc: 0.5106 - val_loss: 0.6919 - val_acc: 0.5246\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6926 - acc: 0.5218 - val_loss: 0.6919 - val_acc: 0.5246\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6940 - acc: 0.5022 - val_loss: 0.6921 - val_acc: 0.5246\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6927 - acc: 0.5175 - val_loss: 0.6924 - val_acc: 0.5246\n",
      "Epoch 39/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6948 - acc: 0.5004 - val_loss: 0.6926 - val_acc: 0.5246\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6946 - acc: 0.5006 - val_loss: 0.6926 - val_acc: 0.5246\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6933 - acc: 0.5082 - val_loss: 0.6923 - val_acc: 0.5246\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6938 - acc: 0.5024 - val_loss: 0.6920 - val_acc: 0.5246\n",
      "Epoch 43/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6936 - acc: 0.5054 - val_loss: 0.6919 - val_acc: 0.5246\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6939 - acc: 0.5073 - val_loss: 0.6920 - val_acc: 0.5246\n",
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6940 - acc: 0.5164 - val_loss: 0.6922 - val_acc: 0.5246\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6931 - acc: 0.5250 - val_loss: 0.6923 - val_acc: 0.5246\n",
      "Epoch 47/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6941 - acc: 0.5274 - val_loss: 0.6922 - val_acc: 0.5246\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6942 - acc: 0.5170 - val_loss: 0.6921 - val_acc: 0.5246\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6941 - acc: 0.5181 - val_loss: 0.6919 - val_acc: 0.5246\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6929 - acc: 0.5140 - val_loss: 0.6919 - val_acc: 0.5246\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6942 - acc: 0.5069 - val_loss: 0.6921 - val_acc: 0.5246\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6923 - acc: 0.5216 - val_loss: 0.6922 - val_acc: 0.5246\n",
      "Epoch 53/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6934 - acc: 0.5086 - val_loss: 0.6922 - val_acc: 0.5246\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6939 - acc: 0.5004 - val_loss: 0.6921 - val_acc: 0.5246\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6930 - acc: 0.5155 - val_loss: 0.6920 - val_acc: 0.5246\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6932 - acc: 0.5153 - val_loss: 0.6919 - val_acc: 0.5246\n",
      "Epoch 57/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6926 - acc: 0.5140 - val_loss: 0.6919 - val_acc: 0.5246\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6925 - acc: 0.5203 - val_loss: 0.6919 - val_acc: 0.5246\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6928 - acc: 0.5201 - val_loss: 0.6919 - val_acc: 0.5246\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6934 - acc: 0.5170 - val_loss: 0.6919 - val_acc: 0.5246\n",
      "Epoch 61/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6929 - acc: 0.5207 - val_loss: 0.6919 - val_acc: 0.5246\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6928 - acc: 0.5160 - val_loss: 0.6919 - val_acc: 0.5246\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6923 - acc: 0.5218 - val_loss: 0.6920 - val_acc: 0.5246\n",
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6938 - acc: 0.5030 - val_loss: 0.6923 - val_acc: 0.5246\n",
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6937 - acc: 0.5024 - val_loss: 0.6925 - val_acc: 0.5246\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6933 - acc: 0.5024 - val_loss: 0.6926 - val_acc: 0.5246\n",
      "Epoch 67/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6946 - acc: 0.4925 - val_loss: 0.6925 - val_acc: 0.5246\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6941 - acc: 0.5106 - val_loss: 0.6922 - val_acc: 0.5246\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 0.6938 - acc: 0.5028 - val_loss: 0.6920 - val_acc: 0.5246\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6932 - acc: 0.5050 - val_loss: 0.6918 - val_acc: 0.5246\n",
      "sample weight :  [4.47679798e-04 1.80069591e-05 2.74690817e-05 ... 2.13741459e-05\n",
      " 1.67232463e-04 5.06469432e-05]\n",
      "1294215.0\n",
      "new_x, new_y (6182, 10, 4068) (6182,)\n",
      "WARNING:tensorflow:Layer lstm_188 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_189 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_190 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_191 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/70\n",
      "2/2 [==============================] - 5s 540ms/step - loss: 0.7838 - acc: 0.4873 - val_loss: 0.7348 - val_acc: 0.4922\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.7424 - acc: 0.4868 - val_loss: 0.7087 - val_acc: 0.4922\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.7196 - acc: 0.4853 - val_loss: 0.6954 - val_acc: 0.4922\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.7015 - acc: 0.5091 - val_loss: 0.6934 - val_acc: 0.5078\n",
      "Epoch 5/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.7016 - acc: 0.5037 - val_loss: 0.6982 - val_acc: 0.5078\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.7082 - acc: 0.5041 - val_loss: 0.7017 - val_acc: 0.5078\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.7075 - acc: 0.5080 - val_loss: 0.7004 - val_acc: 0.5078\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.7085 - acc: 0.5006 - val_loss: 0.6969 - val_acc: 0.5078\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.7031 - acc: 0.5065 - val_loss: 0.6943 - val_acc: 0.5078\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.7021 - acc: 0.5069 - val_loss: 0.6932 - val_acc: 0.5078\n",
      "Epoch 11/70\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.6996 - acc: 0.5060 - val_loss: 0.6931 - val_acc: 0.5078\n",
      "Epoch 12/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.7010 - acc: 0.4948 - val_loss: 0.6935 - val_acc: 0.4922\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.7008 - acc: 0.5000 - val_loss: 0.6940 - val_acc: 0.4922\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.7029 - acc: 0.4901 - val_loss: 0.6943 - val_acc: 0.4922\n",
      "Epoch 15/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.7021 - acc: 0.4994 - val_loss: 0.6944 - val_acc: 0.4922\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 0.6977 - acc: 0.4970 - val_loss: 0.6942 - val_acc: 0.4922\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6999 - acc: 0.4959 - val_loss: 0.6938 - val_acc: 0.4922\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.7000 - acc: 0.4950 - val_loss: 0.6934 - val_acc: 0.4922\n",
      "Epoch 19/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6983 - acc: 0.5054 - val_loss: 0.6931 - val_acc: 0.5078\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6997 - acc: 0.4918 - val_loss: 0.6930 - val_acc: 0.5078\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.7008 - acc: 0.4940 - val_loss: 0.6931 - val_acc: 0.5078\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.7002 - acc: 0.4981 - val_loss: 0.6934 - val_acc: 0.5078\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.7016 - acc: 0.4994 - val_loss: 0.6935 - val_acc: 0.5078\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6984 - acc: 0.5054 - val_loss: 0.6936 - val_acc: 0.5078\n",
      "Epoch 25/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.7006 - acc: 0.4978 - val_loss: 0.6935 - val_acc: 0.5078\n",
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.7008 - acc: 0.4994 - val_loss: 0.6934 - val_acc: 0.5078\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6989 - acc: 0.5041 - val_loss: 0.6933 - val_acc: 0.5078\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.7027 - acc: 0.4866 - val_loss: 0.6932 - val_acc: 0.5078\n",
      "Epoch 29/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6966 - acc: 0.5108 - val_loss: 0.6931 - val_acc: 0.5078\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6990 - acc: 0.5006 - val_loss: 0.6931 - val_acc: 0.5078\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6979 - acc: 0.5067 - val_loss: 0.6930 - val_acc: 0.5078\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6994 - acc: 0.4978 - val_loss: 0.6930 - val_acc: 0.5078\n",
      "Epoch 33/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6983 - acc: 0.5136 - val_loss: 0.6930 - val_acc: 0.5078\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6958 - acc: 0.5110 - val_loss: 0.6930 - val_acc: 0.5078\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6999 - acc: 0.4879 - val_loss: 0.6931 - val_acc: 0.5078\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.7012 - acc: 0.4890 - val_loss: 0.6931 - val_acc: 0.5078\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6965 - acc: 0.5082 - val_loss: 0.6931 - val_acc: 0.5078\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.7002 - acc: 0.4916 - val_loss: 0.6931 - val_acc: 0.5078\n",
      "Epoch 39/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6993 - acc: 0.4981 - val_loss: 0.6930 - val_acc: 0.5078\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6954 - acc: 0.5080 - val_loss: 0.6930 - val_acc: 0.5078\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6994 - acc: 0.5000 - val_loss: 0.6930 - val_acc: 0.5078\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6980 - acc: 0.4963 - val_loss: 0.6930 - val_acc: 0.5078\n",
      "Epoch 43/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6962 - acc: 0.5114 - val_loss: 0.6931 - val_acc: 0.5078\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6963 - acc: 0.5119 - val_loss: 0.6932 - val_acc: 0.5078\n",
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6961 - acc: 0.5088 - val_loss: 0.6933 - val_acc: 0.5078\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6987 - acc: 0.4963 - val_loss: 0.6933 - val_acc: 0.5078\n",
      "Epoch 47/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6968 - acc: 0.5024 - val_loss: 0.6934 - val_acc: 0.5078\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6971 - acc: 0.5069 - val_loss: 0.6935 - val_acc: 0.5078\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6949 - acc: 0.5108 - val_loss: 0.6934 - val_acc: 0.5078\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6959 - acc: 0.5162 - val_loss: 0.6933 - val_acc: 0.5078\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6962 - acc: 0.5153 - val_loss: 0.6931 - val_acc: 0.5078\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6960 - acc: 0.5026 - val_loss: 0.6930 - val_acc: 0.5078\n",
      "Epoch 53/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6953 - acc: 0.5000 - val_loss: 0.6929 - val_acc: 0.5078\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6971 - acc: 0.5052 - val_loss: 0.6929 - val_acc: 0.5078\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6954 - acc: 0.5041 - val_loss: 0.6930 - val_acc: 0.5078\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6962 - acc: 0.5015 - val_loss: 0.6929 - val_acc: 0.5078\n",
      "Epoch 57/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6981 - acc: 0.4884 - val_loss: 0.6929 - val_acc: 0.5078\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6964 - acc: 0.5050 - val_loss: 0.6929 - val_acc: 0.5078\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6944 - acc: 0.5071 - val_loss: 0.6930 - val_acc: 0.5078\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6958 - acc: 0.5065 - val_loss: 0.6930 - val_acc: 0.5078\n",
      "Epoch 61/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6940 - acc: 0.5050 - val_loss: 0.6929 - val_acc: 0.5078\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6946 - acc: 0.5101 - val_loss: 0.6929 - val_acc: 0.5078\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6935 - acc: 0.5127 - val_loss: 0.6929 - val_acc: 0.5078\n",
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6935 - acc: 0.5149 - val_loss: 0.6929 - val_acc: 0.5078\n",
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6945 - acc: 0.5060 - val_loss: 0.6929 - val_acc: 0.5078\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6928 - acc: 0.5186 - val_loss: 0.6929 - val_acc: 0.5078\n",
      "Epoch 67/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6959 - acc: 0.5026 - val_loss: 0.6929 - val_acc: 0.5078\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6945 - acc: 0.5045 - val_loss: 0.6929 - val_acc: 0.5078\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6974 - acc: 0.4959 - val_loss: 0.6929 - val_acc: 0.5078\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6943 - acc: 0.5138 - val_loss: 0.6929 - val_acc: 0.5078\n",
      "sample weight :  [4.62055251e-04 1.86298574e-05 2.68050125e-05 ... 2.06446342e-05\n",
      " 1.62486253e-04 4.88698834e-05]\n",
      "1298959.0\n",
      "new_x, new_y (6182, 10, 4068) (6182,)\n",
      "WARNING:tensorflow:Layer lstm_192 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_193 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_194 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_195 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/70\n",
      "2/2 [==============================] - 5s 551ms/step - loss: 0.7022 - acc: 0.4996 - val_loss: 0.6931 - val_acc: 0.5032\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6970 - acc: 0.5052 - val_loss: 0.6950 - val_acc: 0.4968\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.7031 - acc: 0.4968 - val_loss: 0.6958 - val_acc: 0.4968\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.7018 - acc: 0.4933 - val_loss: 0.6946 - val_acc: 0.4968\n",
      "Epoch 5/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6997 - acc: 0.4978 - val_loss: 0.6934 - val_acc: 0.4968\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6989 - acc: 0.4974 - val_loss: 0.6931 - val_acc: 0.5032\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6980 - acc: 0.5026 - val_loss: 0.6932 - val_acc: 0.5032\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6981 - acc: 0.5035 - val_loss: 0.6933 - val_acc: 0.5032\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6983 - acc: 0.4978 - val_loss: 0.6934 - val_acc: 0.5032\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6991 - acc: 0.4890 - val_loss: 0.6934 - val_acc: 0.5032\n",
      "Epoch 11/70\n",
      "2/2 [==============================] - 0s 157ms/step - loss: 0.6953 - acc: 0.5075 - val_loss: 0.6933 - val_acc: 0.5032\n",
      "Epoch 12/70\n",
      "2/2 [==============================] - 0s 155ms/step - loss: 0.6992 - acc: 0.4933 - val_loss: 0.6932 - val_acc: 0.5032\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 155ms/step - loss: 0.6975 - acc: 0.4972 - val_loss: 0.6931 - val_acc: 0.5032\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 156ms/step - loss: 0.6957 - acc: 0.5060 - val_loss: 0.6933 - val_acc: 0.4968\n",
      "Epoch 15/70\n",
      "2/2 [==============================] - 0s 153ms/step - loss: 0.6965 - acc: 0.5009 - val_loss: 0.6934 - val_acc: 0.4968\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.6976 - acc: 0.4933 - val_loss: 0.6933 - val_acc: 0.4968\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 0.6961 - acc: 0.4996 - val_loss: 0.6932 - val_acc: 0.4968\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6957 - acc: 0.4963 - val_loss: 0.6931 - val_acc: 0.4968\n",
      "Epoch 19/70\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.6961 - acc: 0.5017 - val_loss: 0.6931 - val_acc: 0.5032\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6964 - acc: 0.5006 - val_loss: 0.6931 - val_acc: 0.5032\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6967 - acc: 0.4931 - val_loss: 0.6932 - val_acc: 0.5032\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6953 - acc: 0.5065 - val_loss: 0.6932 - val_acc: 0.5032\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6943 - acc: 0.5108 - val_loss: 0.6931 - val_acc: 0.5032\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.6957 - acc: 0.4972 - val_loss: 0.6934 - val_acc: 0.4968\n",
      "Epoch 25/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6970 - acc: 0.4860 - val_loss: 0.6936 - val_acc: 0.4968\n",
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6934 - acc: 0.5145 - val_loss: 0.6936 - val_acc: 0.4968\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6944 - acc: 0.5006 - val_loss: 0.6934 - val_acc: 0.4968\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6954 - acc: 0.4950 - val_loss: 0.6932 - val_acc: 0.4968\n",
      "Epoch 29/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6957 - acc: 0.4965 - val_loss: 0.6931 - val_acc: 0.5032\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6952 - acc: 0.4918 - val_loss: 0.6931 - val_acc: 0.5032\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6930 - acc: 0.5134 - val_loss: 0.6931 - val_acc: 0.5032\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6939 - acc: 0.5071 - val_loss: 0.6931 - val_acc: 0.5032\n",
      "Epoch 33/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6939 - acc: 0.4940 - val_loss: 0.6932 - val_acc: 0.4968\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6941 - acc: 0.5043 - val_loss: 0.6932 - val_acc: 0.4968\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6952 - acc: 0.4983 - val_loss: 0.6933 - val_acc: 0.4968\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6944 - acc: 0.4974 - val_loss: 0.6933 - val_acc: 0.4968\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6943 - acc: 0.5019 - val_loss: 0.6932 - val_acc: 0.4968\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6932 - acc: 0.5097 - val_loss: 0.6931 - val_acc: 0.5032\n",
      "Epoch 39/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6941 - acc: 0.5043 - val_loss: 0.6931 - val_acc: 0.5032\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6939 - acc: 0.5009 - val_loss: 0.6932 - val_acc: 0.5032\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6935 - acc: 0.5069 - val_loss: 0.6933 - val_acc: 0.5032\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6948 - acc: 0.4965 - val_loss: 0.6933 - val_acc: 0.5032\n",
      "Epoch 43/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6943 - acc: 0.4937 - val_loss: 0.6932 - val_acc: 0.5032\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6941 - acc: 0.5039 - val_loss: 0.6931 - val_acc: 0.5032\n",
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6945 - acc: 0.5071 - val_loss: 0.6933 - val_acc: 0.4968\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6934 - acc: 0.5127 - val_loss: 0.6935 - val_acc: 0.4968\n",
      "Epoch 47/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6949 - acc: 0.5002 - val_loss: 0.6935 - val_acc: 0.4968\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.6946 - acc: 0.4888 - val_loss: 0.6933 - val_acc: 0.4968\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6950 - acc: 0.4955 - val_loss: 0.6932 - val_acc: 0.4968\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6960 - acc: 0.4834 - val_loss: 0.6931 - val_acc: 0.5032\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6937 - acc: 0.5091 - val_loss: 0.6931 - val_acc: 0.5032\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6953 - acc: 0.4873 - val_loss: 0.6932 - val_acc: 0.5032\n",
      "Epoch 53/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6942 - acc: 0.4974 - val_loss: 0.6932 - val_acc: 0.5032\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6952 - acc: 0.4916 - val_loss: 0.6933 - val_acc: 0.5032\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6944 - acc: 0.5035 - val_loss: 0.6932 - val_acc: 0.5032\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6948 - acc: 0.4886 - val_loss: 0.6931 - val_acc: 0.5032\n",
      "Epoch 57/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6926 - acc: 0.5134 - val_loss: 0.6932 - val_acc: 0.4968\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6943 - acc: 0.4931 - val_loss: 0.6934 - val_acc: 0.4968\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6936 - acc: 0.4948 - val_loss: 0.6935 - val_acc: 0.4968\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6940 - acc: 0.4983 - val_loss: 0.6935 - val_acc: 0.4968\n",
      "Epoch 61/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6940 - acc: 0.4972 - val_loss: 0.6933 - val_acc: 0.4968\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6946 - acc: 0.4974 - val_loss: 0.6931 - val_acc: 0.4968\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6929 - acc: 0.5153 - val_loss: 0.6931 - val_acc: 0.5032\n",
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6937 - acc: 0.5058 - val_loss: 0.6932 - val_acc: 0.5032\n",
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6938 - acc: 0.4970 - val_loss: 0.6932 - val_acc: 0.5032\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6936 - acc: 0.4970 - val_loss: 0.6931 - val_acc: 0.5032\n",
      "Epoch 67/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6932 - acc: 0.5082 - val_loss: 0.6931 - val_acc: 0.5032\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6933 - acc: 0.5047 - val_loss: 0.6931 - val_acc: 0.5032\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6949 - acc: 0.4862 - val_loss: 0.6931 - val_acc: 0.5032\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6930 - acc: 0.5091 - val_loss: 0.6931 - val_acc: 0.5032\n",
      "sample weight :  [4.59342313e-04 1.85167680e-05 2.69666129e-05 ... 2.07649857e-05\n",
      " 1.63375167e-04 4.91626904e-05]\n",
      "1305677.0\n",
      "new_x, new_y (6182, 10, 4068) (6182,)\n",
      "WARNING:tensorflow:Layer lstm_196 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_197 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_198 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_199 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/70\n",
      "2/2 [==============================] - 8s 3s/step - loss: 0.7142 - acc: 0.4998 - val_loss: 0.6943 - val_acc: 0.5116\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.7029 - acc: 0.5136 - val_loss: 0.6933 - val_acc: 0.4884\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.7007 - acc: 0.5002 - val_loss: 0.6969 - val_acc: 0.4884\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.7053 - acc: 0.4942 - val_loss: 0.6983 - val_acc: 0.4884\n",
      "Epoch 5/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.7016 - acc: 0.5125 - val_loss: 0.6967 - val_acc: 0.4884\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.7006 - acc: 0.5078 - val_loss: 0.6944 - val_acc: 0.4884\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.7015 - acc: 0.5071 - val_loss: 0.6930 - val_acc: 0.5116\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6981 - acc: 0.5050 - val_loss: 0.6930 - val_acc: 0.5116\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.7016 - acc: 0.4985 - val_loss: 0.6934 - val_acc: 0.5116\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6996 - acc: 0.5030 - val_loss: 0.6934 - val_acc: 0.5116\n",
      "Epoch 11/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.7010 - acc: 0.5022 - val_loss: 0.6931 - val_acc: 0.5116\n",
      "Epoch 12/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.7000 - acc: 0.4987 - val_loss: 0.6929 - val_acc: 0.5116\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6966 - acc: 0.5114 - val_loss: 0.6930 - val_acc: 0.5116\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6979 - acc: 0.4976 - val_loss: 0.6931 - val_acc: 0.5116\n",
      "Epoch 15/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6980 - acc: 0.4965 - val_loss: 0.6933 - val_acc: 0.4884\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.7016 - acc: 0.4903 - val_loss: 0.6933 - val_acc: 0.4884\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6978 - acc: 0.5032 - val_loss: 0.6931 - val_acc: 0.5116\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6969 - acc: 0.5028 - val_loss: 0.6930 - val_acc: 0.5116\n",
      "Epoch 19/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6956 - acc: 0.5114 - val_loss: 0.6929 - val_acc: 0.5116\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6953 - acc: 0.5114 - val_loss: 0.6929 - val_acc: 0.5116\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6972 - acc: 0.4994 - val_loss: 0.6929 - val_acc: 0.5116\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6957 - acc: 0.5084 - val_loss: 0.6929 - val_acc: 0.5116\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6972 - acc: 0.5119 - val_loss: 0.6929 - val_acc: 0.5116\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6977 - acc: 0.4994 - val_loss: 0.6931 - val_acc: 0.5116\n",
      "Epoch 25/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6962 - acc: 0.5110 - val_loss: 0.6933 - val_acc: 0.4884\n",
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 0.6955 - acc: 0.5022 - val_loss: 0.6934 - val_acc: 0.4884\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6961 - acc: 0.5050 - val_loss: 0.6933 - val_acc: 0.4884\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6958 - acc: 0.4978 - val_loss: 0.6932 - val_acc: 0.4884\n",
      "Epoch 29/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6957 - acc: 0.5067 - val_loss: 0.6931 - val_acc: 0.5116\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6949 - acc: 0.4998 - val_loss: 0.6929 - val_acc: 0.5116\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6967 - acc: 0.4970 - val_loss: 0.6929 - val_acc: 0.5116\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6971 - acc: 0.4987 - val_loss: 0.6929 - val_acc: 0.5116\n",
      "Epoch 33/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6970 - acc: 0.4981 - val_loss: 0.6929 - val_acc: 0.5116\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6951 - acc: 0.5026 - val_loss: 0.6929 - val_acc: 0.5116\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6968 - acc: 0.4978 - val_loss: 0.6930 - val_acc: 0.5116\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6959 - acc: 0.5004 - val_loss: 0.6931 - val_acc: 0.5116\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6949 - acc: 0.5013 - val_loss: 0.6931 - val_acc: 0.5116\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6952 - acc: 0.5015 - val_loss: 0.6931 - val_acc: 0.5149\n",
      "Epoch 39/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6957 - acc: 0.5032 - val_loss: 0.6932 - val_acc: 0.4884\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6963 - acc: 0.4946 - val_loss: 0.6931 - val_acc: 0.5401\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6946 - acc: 0.5039 - val_loss: 0.6930 - val_acc: 0.5116\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6957 - acc: 0.4983 - val_loss: 0.6931 - val_acc: 0.5116\n",
      "Epoch 43/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6944 - acc: 0.5073 - val_loss: 0.6931 - val_acc: 0.4884\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6939 - acc: 0.5067 - val_loss: 0.6931 - val_acc: 0.5116\n",
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6961 - acc: 0.5013 - val_loss: 0.6930 - val_acc: 0.5116\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6955 - acc: 0.4929 - val_loss: 0.6929 - val_acc: 0.5116\n",
      "Epoch 47/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6960 - acc: 0.4925 - val_loss: 0.6929 - val_acc: 0.5116\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6960 - acc: 0.4959 - val_loss: 0.6929 - val_acc: 0.5116\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6944 - acc: 0.5080 - val_loss: 0.6929 - val_acc: 0.5116\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6955 - acc: 0.5004 - val_loss: 0.6932 - val_acc: 0.4884\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6943 - acc: 0.5017 - val_loss: 0.6936 - val_acc: 0.4884\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6955 - acc: 0.4953 - val_loss: 0.6939 - val_acc: 0.4884\n",
      "Epoch 53/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6949 - acc: 0.4985 - val_loss: 0.6939 - val_acc: 0.4884\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6959 - acc: 0.4879 - val_loss: 0.6938 - val_acc: 0.4884\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6947 - acc: 0.4922 - val_loss: 0.6937 - val_acc: 0.4884\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6956 - acc: 0.4957 - val_loss: 0.6935 - val_acc: 0.4884\n",
      "Epoch 57/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6946 - acc: 0.5039 - val_loss: 0.6931 - val_acc: 0.4884\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6942 - acc: 0.4998 - val_loss: 0.6929 - val_acc: 0.5116\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6957 - acc: 0.4953 - val_loss: 0.6928 - val_acc: 0.5116\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6942 - acc: 0.5060 - val_loss: 0.6928 - val_acc: 0.5116\n",
      "Epoch 61/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6939 - acc: 0.5004 - val_loss: 0.6928 - val_acc: 0.5116\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6935 - acc: 0.5084 - val_loss: 0.6930 - val_acc: 0.5116\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6941 - acc: 0.5022 - val_loss: 0.6933 - val_acc: 0.4884\n",
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6937 - acc: 0.5127 - val_loss: 0.6936 - val_acc: 0.4884\n",
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6947 - acc: 0.4985 - val_loss: 0.6936 - val_acc: 0.4884\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6939 - acc: 0.5015 - val_loss: 0.6937 - val_acc: 0.4884\n",
      "Epoch 67/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6954 - acc: 0.4953 - val_loss: 0.6935 - val_acc: 0.4884\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6949 - acc: 0.4963 - val_loss: 0.6933 - val_acc: 0.4884\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6936 - acc: 0.4994 - val_loss: 0.6931 - val_acc: 0.4916\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6930 - acc: 0.5166 - val_loss: 0.6930 - val_acc: 0.5019\n",
      "sample weight :  [4.60706914e-04 1.85439488e-05 2.68889286e-05 ... 2.06916450e-05\n",
      " 1.62523577e-04 4.87663527e-05]\n",
      "1316488.0\n",
      "new_x, new_y (6182, 10, 4068) (6182,)\n",
      "WARNING:tensorflow:Layer lstm_200 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_201 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_202 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_203 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/70\n",
      "2/2 [==============================] - 6s 541ms/step - loss: 0.7037 - acc: 0.4957 - val_loss: 0.6926 - val_acc: 0.5323\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6999 - acc: 0.4968 - val_loss: 0.6911 - val_acc: 0.5323\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.7002 - acc: 0.5065 - val_loss: 0.6911 - val_acc: 0.5323\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6979 - acc: 0.5075 - val_loss: 0.6923 - val_acc: 0.5323\n",
      "Epoch 5/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6997 - acc: 0.4974 - val_loss: 0.6944 - val_acc: 0.4677\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.7002 - acc: 0.5045 - val_loss: 0.6962 - val_acc: 0.4677\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6996 - acc: 0.5017 - val_loss: 0.6973 - val_acc: 0.4677\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.7012 - acc: 0.5028 - val_loss: 0.6970 - val_acc: 0.4677\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.7008 - acc: 0.4881 - val_loss: 0.6959 - val_acc: 0.4677\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6979 - acc: 0.5030 - val_loss: 0.6945 - val_acc: 0.4677\n",
      "Epoch 11/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6966 - acc: 0.5088 - val_loss: 0.6938 - val_acc: 0.4677\n",
      "Epoch 12/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6974 - acc: 0.4955 - val_loss: 0.6938 - val_acc: 0.4677\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6961 - acc: 0.4959 - val_loss: 0.6940 - val_acc: 0.4677\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6957 - acc: 0.5063 - val_loss: 0.6941 - val_acc: 0.4677\n",
      "Epoch 15/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6978 - acc: 0.5004 - val_loss: 0.6941 - val_acc: 0.4677\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6977 - acc: 0.4987 - val_loss: 0.6944 - val_acc: 0.4677\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6956 - acc: 0.5032 - val_loss: 0.6944 - val_acc: 0.4677\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6946 - acc: 0.5086 - val_loss: 0.6940 - val_acc: 0.4677\n",
      "Epoch 19/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6962 - acc: 0.4987 - val_loss: 0.6932 - val_acc: 0.4677\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6964 - acc: 0.4905 - val_loss: 0.6926 - val_acc: 0.5323\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6961 - acc: 0.4963 - val_loss: 0.6924 - val_acc: 0.5323\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6978 - acc: 0.4927 - val_loss: 0.6924 - val_acc: 0.5323\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6970 - acc: 0.4864 - val_loss: 0.6927 - val_acc: 0.5323\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6959 - acc: 0.4965 - val_loss: 0.6933 - val_acc: 0.4677\n",
      "Epoch 25/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6962 - acc: 0.4959 - val_loss: 0.6938 - val_acc: 0.4677\n",
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6949 - acc: 0.4981 - val_loss: 0.6938 - val_acc: 0.4677\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6949 - acc: 0.5091 - val_loss: 0.6936 - val_acc: 0.4677\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6948 - acc: 0.5022 - val_loss: 0.6932 - val_acc: 0.4677\n",
      "Epoch 29/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6938 - acc: 0.5080 - val_loss: 0.6931 - val_acc: 0.5323\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6958 - acc: 0.4944 - val_loss: 0.6932 - val_acc: 0.4677\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6949 - acc: 0.4948 - val_loss: 0.6936 - val_acc: 0.4677\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6934 - acc: 0.5127 - val_loss: 0.6942 - val_acc: 0.4677\n",
      "Epoch 33/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6950 - acc: 0.4953 - val_loss: 0.6947 - val_acc: 0.4677\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6948 - acc: 0.4981 - val_loss: 0.6946 - val_acc: 0.4677\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6948 - acc: 0.5002 - val_loss: 0.6941 - val_acc: 0.4677\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6954 - acc: 0.4933 - val_loss: 0.6936 - val_acc: 0.4677\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6935 - acc: 0.5145 - val_loss: 0.6934 - val_acc: 0.4677\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6938 - acc: 0.5050 - val_loss: 0.6934 - val_acc: 0.4677\n",
      "Epoch 39/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6935 - acc: 0.5069 - val_loss: 0.6934 - val_acc: 0.4677\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6931 - acc: 0.5067 - val_loss: 0.6933 - val_acc: 0.4677\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6949 - acc: 0.4925 - val_loss: 0.6934 - val_acc: 0.4677\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6936 - acc: 0.5009 - val_loss: 0.6937 - val_acc: 0.4677\n",
      "Epoch 43/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6953 - acc: 0.4920 - val_loss: 0.6942 - val_acc: 0.4677\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6949 - acc: 0.5013 - val_loss: 0.6942 - val_acc: 0.4677\n",
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6935 - acc: 0.5114 - val_loss: 0.6936 - val_acc: 0.4677\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6939 - acc: 0.5099 - val_loss: 0.6928 - val_acc: 0.5323\n",
      "Epoch 47/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6944 - acc: 0.5009 - val_loss: 0.6923 - val_acc: 0.5323\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6947 - acc: 0.4914 - val_loss: 0.6922 - val_acc: 0.5323\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6956 - acc: 0.4862 - val_loss: 0.6925 - val_acc: 0.5323\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6951 - acc: 0.4899 - val_loss: 0.6930 - val_acc: 0.5356\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6935 - acc: 0.5106 - val_loss: 0.6934 - val_acc: 0.4677\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6932 - acc: 0.5104 - val_loss: 0.6938 - val_acc: 0.4677\n",
      "Epoch 53/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6943 - acc: 0.4892 - val_loss: 0.6943 - val_acc: 0.4677\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6944 - acc: 0.4959 - val_loss: 0.6947 - val_acc: 0.4677\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6940 - acc: 0.5073 - val_loss: 0.6948 - val_acc: 0.4677\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6937 - acc: 0.5019 - val_loss: 0.6942 - val_acc: 0.4677\n",
      "Epoch 57/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6948 - acc: 0.5011 - val_loss: 0.6935 - val_acc: 0.4677\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6947 - acc: 0.4933 - val_loss: 0.6930 - val_acc: 0.5427\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6935 - acc: 0.5019 - val_loss: 0.6925 - val_acc: 0.5323\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6938 - acc: 0.4937 - val_loss: 0.6921 - val_acc: 0.5323\n",
      "Epoch 61/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6949 - acc: 0.4970 - val_loss: 0.6920 - val_acc: 0.5323\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6937 - acc: 0.5000 - val_loss: 0.6921 - val_acc: 0.5323\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6939 - acc: 0.5069 - val_loss: 0.6922 - val_acc: 0.5323\n",
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6938 - acc: 0.5071 - val_loss: 0.6924 - val_acc: 0.5323\n",
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6945 - acc: 0.4918 - val_loss: 0.6928 - val_acc: 0.5382\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6933 - acc: 0.5151 - val_loss: 0.6934 - val_acc: 0.4677\n",
      "Epoch 67/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6935 - acc: 0.5054 - val_loss: 0.6938 - val_acc: 0.4677\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6938 - acc: 0.5015 - val_loss: 0.6943 - val_acc: 0.4677\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6937 - acc: 0.5063 - val_loss: 0.6949 - val_acc: 0.4677\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6937 - acc: 0.4948 - val_loss: 0.6955 - val_acc: 0.4677\n",
      "sample weight :  [4.73833745e-04 1.91423903e-05 2.60787040e-05 ... 1.99926198e-05\n",
      " 1.57516365e-04 4.71437253e-05]\n",
      "1317315.0\n",
      "new_x, new_y (6182, 10, 4068) (6182,)\n",
      "WARNING:tensorflow:Layer lstm_204 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_205 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_206 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_207 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/70\n",
      "2/2 [==============================] - 5s 536ms/step - loss: 0.6965 - acc: 0.5252 - val_loss: 0.6917 - val_acc: 0.5446\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6984 - acc: 0.5037 - val_loss: 0.6909 - val_acc: 0.5446\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6984 - acc: 0.5000 - val_loss: 0.6895 - val_acc: 0.5446\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6960 - acc: 0.5091 - val_loss: 0.6893 - val_acc: 0.5446\n",
      "Epoch 5/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6936 - acc: 0.5300 - val_loss: 0.6899 - val_acc: 0.5446\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6965 - acc: 0.5071 - val_loss: 0.6904 - val_acc: 0.5446\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6951 - acc: 0.5110 - val_loss: 0.6907 - val_acc: 0.5446\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 0.6967 - acc: 0.5052 - val_loss: 0.6908 - val_acc: 0.5446\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6959 - acc: 0.5032 - val_loss: 0.6905 - val_acc: 0.5446\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6978 - acc: 0.5037 - val_loss: 0.6902 - val_acc: 0.5446\n",
      "Epoch 11/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6929 - acc: 0.5153 - val_loss: 0.6899 - val_acc: 0.5446\n",
      "Epoch 12/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6949 - acc: 0.5132 - val_loss: 0.6897 - val_acc: 0.5446\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6943 - acc: 0.5151 - val_loss: 0.6895 - val_acc: 0.5446\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6956 - acc: 0.5121 - val_loss: 0.6894 - val_acc: 0.5446\n",
      "Epoch 15/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6951 - acc: 0.5091 - val_loss: 0.6894 - val_acc: 0.5446\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6955 - acc: 0.5142 - val_loss: 0.6897 - val_acc: 0.5446\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6939 - acc: 0.5123 - val_loss: 0.6899 - val_acc: 0.5446\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6947 - acc: 0.5097 - val_loss: 0.6900 - val_acc: 0.5446\n",
      "Epoch 19/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6935 - acc: 0.5155 - val_loss: 0.6903 - val_acc: 0.5446\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6952 - acc: 0.5093 - val_loss: 0.6906 - val_acc: 0.5446\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6954 - acc: 0.5041 - val_loss: 0.6903 - val_acc: 0.5446\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6947 - acc: 0.5041 - val_loss: 0.6896 - val_acc: 0.5446\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6957 - acc: 0.5054 - val_loss: 0.6893 - val_acc: 0.5446\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6949 - acc: 0.5183 - val_loss: 0.6893 - val_acc: 0.5446\n",
      "Epoch 25/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6941 - acc: 0.5252 - val_loss: 0.6895 - val_acc: 0.5446\n",
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6950 - acc: 0.5162 - val_loss: 0.6901 - val_acc: 0.5446\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6949 - acc: 0.5132 - val_loss: 0.6910 - val_acc: 0.5446\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6934 - acc: 0.5060 - val_loss: 0.6919 - val_acc: 0.5446\n",
      "Epoch 29/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6961 - acc: 0.4890 - val_loss: 0.6923 - val_acc: 0.5446\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6928 - acc: 0.5080 - val_loss: 0.6919 - val_acc: 0.5446\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6940 - acc: 0.5088 - val_loss: 0.6909 - val_acc: 0.5446\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6950 - acc: 0.5013 - val_loss: 0.6900 - val_acc: 0.5446\n",
      "Epoch 33/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6932 - acc: 0.5214 - val_loss: 0.6895 - val_acc: 0.5446\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6933 - acc: 0.5173 - val_loss: 0.6893 - val_acc: 0.5446\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6936 - acc: 0.5173 - val_loss: 0.6892 - val_acc: 0.5446\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6937 - acc: 0.5155 - val_loss: 0.6893 - val_acc: 0.5446\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6937 - acc: 0.5194 - val_loss: 0.6897 - val_acc: 0.5446\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6923 - acc: 0.5321 - val_loss: 0.6901 - val_acc: 0.5446\n",
      "Epoch 39/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6932 - acc: 0.5151 - val_loss: 0.6904 - val_acc: 0.5446\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6932 - acc: 0.5116 - val_loss: 0.6906 - val_acc: 0.5446\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6942 - acc: 0.5112 - val_loss: 0.6906 - val_acc: 0.5446\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6936 - acc: 0.5166 - val_loss: 0.6904 - val_acc: 0.5446\n",
      "Epoch 43/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6922 - acc: 0.5192 - val_loss: 0.6901 - val_acc: 0.5446\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6929 - acc: 0.5151 - val_loss: 0.6898 - val_acc: 0.5446\n",
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6931 - acc: 0.5211 - val_loss: 0.6895 - val_acc: 0.5446\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6934 - acc: 0.5116 - val_loss: 0.6894 - val_acc: 0.5446\n",
      "Epoch 47/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6917 - acc: 0.5235 - val_loss: 0.6895 - val_acc: 0.5446\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6930 - acc: 0.5188 - val_loss: 0.6896 - val_acc: 0.5446\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6930 - acc: 0.5183 - val_loss: 0.6897 - val_acc: 0.5446\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6927 - acc: 0.5203 - val_loss: 0.6897 - val_acc: 0.5446\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6934 - acc: 0.5183 - val_loss: 0.6899 - val_acc: 0.5446\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6923 - acc: 0.5188 - val_loss: 0.6900 - val_acc: 0.5446\n",
      "Epoch 53/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6931 - acc: 0.5099 - val_loss: 0.6899 - val_acc: 0.5446\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6920 - acc: 0.5250 - val_loss: 0.6897 - val_acc: 0.5446\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6923 - acc: 0.5231 - val_loss: 0.6896 - val_acc: 0.5446\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6929 - acc: 0.5203 - val_loss: 0.6896 - val_acc: 0.5446\n",
      "Epoch 57/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6920 - acc: 0.5205 - val_loss: 0.6898 - val_acc: 0.5446\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6928 - acc: 0.5194 - val_loss: 0.6899 - val_acc: 0.5446\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6932 - acc: 0.5181 - val_loss: 0.6900 - val_acc: 0.5446\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6919 - acc: 0.5248 - val_loss: 0.6901 - val_acc: 0.5446\n",
      "Epoch 61/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6929 - acc: 0.5207 - val_loss: 0.6901 - val_acc: 0.5446\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6930 - acc: 0.5164 - val_loss: 0.6903 - val_acc: 0.5446\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6929 - acc: 0.5151 - val_loss: 0.6906 - val_acc: 0.5446\n",
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6931 - acc: 0.5110 - val_loss: 0.6904 - val_acc: 0.5446\n",
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6923 - acc: 0.5194 - val_loss: 0.6900 - val_acc: 0.5446\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6925 - acc: 0.5170 - val_loss: 0.6898 - val_acc: 0.5446\n",
      "Epoch 67/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6919 - acc: 0.5149 - val_loss: 0.6897 - val_acc: 0.5446\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6924 - acc: 0.5179 - val_loss: 0.6899 - val_acc: 0.5446\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6920 - acc: 0.5263 - val_loss: 0.6902 - val_acc: 0.5446\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6920 - acc: 0.5186 - val_loss: 0.6902 - val_acc: 0.5446\n",
      "sample weight :  [4.56236754e-04 1.84293778e-05 2.72319663e-05 ... 2.07837516e-05\n",
      " 1.63758806e-04 4.90036716e-05]\n",
      "1310580.0\n",
      "new_x, new_y (6182, 10, 4068) (6182,)\n",
      "WARNING:tensorflow:Layer lstm_208 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_209 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_210 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_211 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/70\n",
      "2/2 [==============================] - 5s 543ms/step - loss: 0.7300 - acc: 0.5145 - val_loss: 0.7128 - val_acc: 0.4981\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.7136 - acc: 0.5101 - val_loss: 0.7002 - val_acc: 0.4981\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.7010 - acc: 0.5145 - val_loss: 0.6941 - val_acc: 0.4981\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 0.6977 - acc: 0.5052 - val_loss: 0.6932 - val_acc: 0.5019\n",
      "Epoch 5/70\n",
      "2/2 [==============================] - 0s 156ms/step - loss: 0.6976 - acc: 0.5037 - val_loss: 0.6939 - val_acc: 0.5019\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 154ms/step - loss: 0.7006 - acc: 0.4989 - val_loss: 0.6939 - val_acc: 0.5019\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 154ms/step - loss: 0.7034 - acc: 0.4920 - val_loss: 0.6933 - val_acc: 0.5019\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 155ms/step - loss: 0.6977 - acc: 0.4974 - val_loss: 0.6931 - val_acc: 0.5019\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6983 - acc: 0.4894 - val_loss: 0.6934 - val_acc: 0.4981\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6971 - acc: 0.4929 - val_loss: 0.6940 - val_acc: 0.4981\n",
      "Epoch 11/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 136ms/step - loss: 0.7004 - acc: 0.4920 - val_loss: 0.6946 - val_acc: 0.4981\n",
      "Epoch 12/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6987 - acc: 0.4985 - val_loss: 0.6951 - val_acc: 0.4981\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6969 - acc: 0.5054 - val_loss: 0.6953 - val_acc: 0.4981\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6974 - acc: 0.5069 - val_loss: 0.6950 - val_acc: 0.4981\n",
      "Epoch 15/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6944 - acc: 0.5157 - val_loss: 0.6946 - val_acc: 0.4981\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6958 - acc: 0.4998 - val_loss: 0.6942 - val_acc: 0.4981\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6964 - acc: 0.5000 - val_loss: 0.6939 - val_acc: 0.4981\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6963 - acc: 0.4994 - val_loss: 0.6936 - val_acc: 0.4981\n",
      "Epoch 19/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6962 - acc: 0.5017 - val_loss: 0.6935 - val_acc: 0.4981\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6935 - acc: 0.5099 - val_loss: 0.6935 - val_acc: 0.4981\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6960 - acc: 0.4970 - val_loss: 0.6935 - val_acc: 0.4981\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6975 - acc: 0.4955 - val_loss: 0.6935 - val_acc: 0.4981\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6961 - acc: 0.5009 - val_loss: 0.6937 - val_acc: 0.4981\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6962 - acc: 0.5015 - val_loss: 0.6938 - val_acc: 0.4981\n",
      "Epoch 25/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6959 - acc: 0.5037 - val_loss: 0.6939 - val_acc: 0.4981\n",
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6950 - acc: 0.5123 - val_loss: 0.6939 - val_acc: 0.4981\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6964 - acc: 0.5052 - val_loss: 0.6939 - val_acc: 0.4981\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6958 - acc: 0.5000 - val_loss: 0.6939 - val_acc: 0.4981\n",
      "Epoch 29/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6962 - acc: 0.5002 - val_loss: 0.6936 - val_acc: 0.4981\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6952 - acc: 0.5041 - val_loss: 0.6933 - val_acc: 0.4981\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6946 - acc: 0.5106 - val_loss: 0.6932 - val_acc: 0.4981\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6949 - acc: 0.4937 - val_loss: 0.6931 - val_acc: 0.5019\n",
      "Epoch 33/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6944 - acc: 0.5065 - val_loss: 0.6931 - val_acc: 0.4981\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6945 - acc: 0.5013 - val_loss: 0.6933 - val_acc: 0.4981\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6936 - acc: 0.5011 - val_loss: 0.6936 - val_acc: 0.4981\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6957 - acc: 0.5017 - val_loss: 0.6939 - val_acc: 0.4981\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6948 - acc: 0.5065 - val_loss: 0.6941 - val_acc: 0.4981\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6955 - acc: 0.5004 - val_loss: 0.6944 - val_acc: 0.4981\n",
      "Epoch 39/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6937 - acc: 0.5155 - val_loss: 0.6944 - val_acc: 0.4981\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6940 - acc: 0.5147 - val_loss: 0.6943 - val_acc: 0.4981\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6948 - acc: 0.5086 - val_loss: 0.6941 - val_acc: 0.4981\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6947 - acc: 0.5058 - val_loss: 0.6937 - val_acc: 0.4981\n",
      "Epoch 43/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6960 - acc: 0.4978 - val_loss: 0.6934 - val_acc: 0.4981\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6950 - acc: 0.5073 - val_loss: 0.6933 - val_acc: 0.4981\n",
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6937 - acc: 0.5149 - val_loss: 0.6932 - val_acc: 0.4981\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6937 - acc: 0.4931 - val_loss: 0.6931 - val_acc: 0.4981\n",
      "Epoch 47/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6937 - acc: 0.5047 - val_loss: 0.6931 - val_acc: 0.4981\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6955 - acc: 0.4983 - val_loss: 0.6932 - val_acc: 0.4981\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6943 - acc: 0.4970 - val_loss: 0.6933 - val_acc: 0.4981\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6939 - acc: 0.5065 - val_loss: 0.6934 - val_acc: 0.4981\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6954 - acc: 0.4998 - val_loss: 0.6935 - val_acc: 0.4981\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6929 - acc: 0.5129 - val_loss: 0.6935 - val_acc: 0.4981\n",
      "Epoch 53/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6933 - acc: 0.5157 - val_loss: 0.6935 - val_acc: 0.4981\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6936 - acc: 0.5110 - val_loss: 0.6935 - val_acc: 0.4981\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6938 - acc: 0.5035 - val_loss: 0.6937 - val_acc: 0.4981\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6940 - acc: 0.5047 - val_loss: 0.6941 - val_acc: 0.4981\n",
      "Epoch 57/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6939 - acc: 0.5136 - val_loss: 0.6945 - val_acc: 0.4981\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6938 - acc: 0.5138 - val_loss: 0.6949 - val_acc: 0.4981\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6939 - acc: 0.5132 - val_loss: 0.6951 - val_acc: 0.4981\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6937 - acc: 0.5058 - val_loss: 0.6950 - val_acc: 0.4981\n",
      "Epoch 61/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6948 - acc: 0.5056 - val_loss: 0.6947 - val_acc: 0.4981\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6944 - acc: 0.5123 - val_loss: 0.6941 - val_acc: 0.4981\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6926 - acc: 0.5216 - val_loss: 0.6935 - val_acc: 0.4981\n",
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6928 - acc: 0.5035 - val_loss: 0.6932 - val_acc: 0.4981\n",
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6940 - acc: 0.5093 - val_loss: 0.6931 - val_acc: 0.4981\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.6929 - acc: 0.5134 - val_loss: 0.6931 - val_acc: 0.4981\n",
      "Epoch 67/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6937 - acc: 0.5019 - val_loss: 0.6931 - val_acc: 0.4981\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6940 - acc: 0.4976 - val_loss: 0.6933 - val_acc: 0.4981\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6939 - acc: 0.5140 - val_loss: 0.6934 - val_acc: 0.4981\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6939 - acc: 0.5043 - val_loss: 0.6936 - val_acc: 0.4981\n",
      "sample weight :  [4.41807811e-04 1.78676615e-05 2.81140959e-05 ... 2.14171833e-05\n",
      " 1.69094953e-04 5.05754356e-05]\n",
      "1321528.0\n",
      "new_x, new_y (6182, 10, 4068) (6182,)\n",
      "WARNING:tensorflow:Layer lstm_212 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_213 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_214 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_215 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/70\n",
      "2/2 [==============================] - 5s 535ms/step - loss: 0.7442 - acc: 0.4802 - val_loss: 0.7132 - val_acc: 0.4787\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.7221 - acc: 0.4845 - val_loss: 0.6980 - val_acc: 0.4787\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.7026 - acc: 0.5019 - val_loss: 0.6923 - val_acc: 0.5213\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.7001 - acc: 0.5091 - val_loss: 0.6941 - val_acc: 0.5213\n",
      "Epoch 5/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6993 - acc: 0.5123 - val_loss: 0.6976 - val_acc: 0.5213\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.7026 - acc: 0.5216 - val_loss: 0.6981 - val_acc: 0.5213\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.7033 - acc: 0.5188 - val_loss: 0.6964 - val_acc: 0.5213\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.7020 - acc: 0.5173 - val_loss: 0.6943 - val_acc: 0.5213\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.7022 - acc: 0.5106 - val_loss: 0.6927 - val_acc: 0.5213\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6983 - acc: 0.5196 - val_loss: 0.6922 - val_acc: 0.5213\n",
      "Epoch 11/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6986 - acc: 0.5030 - val_loss: 0.6925 - val_acc: 0.5213\n",
      "Epoch 12/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6985 - acc: 0.5026 - val_loss: 0.6929 - val_acc: 0.5213\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.7021 - acc: 0.4855 - val_loss: 0.6932 - val_acc: 0.4787\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6978 - acc: 0.5097 - val_loss: 0.6933 - val_acc: 0.4787\n",
      "Epoch 15/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6966 - acc: 0.5095 - val_loss: 0.6932 - val_acc: 0.4787\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6993 - acc: 0.4970 - val_loss: 0.6930 - val_acc: 0.5213\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.7006 - acc: 0.4890 - val_loss: 0.6927 - val_acc: 0.5213\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6983 - acc: 0.5052 - val_loss: 0.6924 - val_acc: 0.5213\n",
      "Epoch 19/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6997 - acc: 0.5108 - val_loss: 0.6922 - val_acc: 0.5213\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6997 - acc: 0.4892 - val_loss: 0.6922 - val_acc: 0.5213\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6955 - acc: 0.5177 - val_loss: 0.6923 - val_acc: 0.5213\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6969 - acc: 0.5153 - val_loss: 0.6923 - val_acc: 0.5213\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.7009 - acc: 0.5035 - val_loss: 0.6923 - val_acc: 0.5213\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6973 - acc: 0.5173 - val_loss: 0.6922 - val_acc: 0.5213\n",
      "Epoch 25/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6959 - acc: 0.5121 - val_loss: 0.6922 - val_acc: 0.5213\n",
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6973 - acc: 0.5067 - val_loss: 0.6922 - val_acc: 0.5213\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6986 - acc: 0.5026 - val_loss: 0.6922 - val_acc: 0.5213\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6954 - acc: 0.5086 - val_loss: 0.6922 - val_acc: 0.5213\n",
      "Epoch 29/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6959 - acc: 0.5037 - val_loss: 0.6922 - val_acc: 0.5213\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6950 - acc: 0.5183 - val_loss: 0.6922 - val_acc: 0.5213\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6958 - acc: 0.5097 - val_loss: 0.6923 - val_acc: 0.5213\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6990 - acc: 0.4987 - val_loss: 0.6923 - val_acc: 0.5213\n",
      "Epoch 33/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6967 - acc: 0.4996 - val_loss: 0.6923 - val_acc: 0.5213\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6959 - acc: 0.5060 - val_loss: 0.6924 - val_acc: 0.5213\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6952 - acc: 0.5078 - val_loss: 0.6924 - val_acc: 0.5213\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6945 - acc: 0.5093 - val_loss: 0.6924 - val_acc: 0.5213\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6945 - acc: 0.5134 - val_loss: 0.6923 - val_acc: 0.5213\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6937 - acc: 0.5205 - val_loss: 0.6923 - val_acc: 0.5213\n",
      "Epoch 39/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6940 - acc: 0.5132 - val_loss: 0.6923 - val_acc: 0.5213\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6972 - acc: 0.4968 - val_loss: 0.6923 - val_acc: 0.5213\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6950 - acc: 0.5091 - val_loss: 0.6922 - val_acc: 0.5213\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6957 - acc: 0.5073 - val_loss: 0.6922 - val_acc: 0.5213\n",
      "Epoch 43/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6955 - acc: 0.5035 - val_loss: 0.6922 - val_acc: 0.5213\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6948 - acc: 0.5194 - val_loss: 0.6922 - val_acc: 0.5213\n",
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6988 - acc: 0.4970 - val_loss: 0.6922 - val_acc: 0.5213\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6966 - acc: 0.5123 - val_loss: 0.6922 - val_acc: 0.5213\n",
      "Epoch 47/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6948 - acc: 0.5069 - val_loss: 0.6923 - val_acc: 0.5213\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6936 - acc: 0.5231 - val_loss: 0.6922 - val_acc: 0.5213\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6961 - acc: 0.5136 - val_loss: 0.6922 - val_acc: 0.5213\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6936 - acc: 0.5162 - val_loss: 0.6922 - val_acc: 0.5213\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6960 - acc: 0.5039 - val_loss: 0.6922 - val_acc: 0.5213\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6967 - acc: 0.4957 - val_loss: 0.6923 - val_acc: 0.5213\n",
      "Epoch 53/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6943 - acc: 0.5116 - val_loss: 0.6923 - val_acc: 0.5213\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6970 - acc: 0.4965 - val_loss: 0.6922 - val_acc: 0.5213\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6943 - acc: 0.5140 - val_loss: 0.6922 - val_acc: 0.5213\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6967 - acc: 0.4942 - val_loss: 0.6922 - val_acc: 0.5213\n",
      "Epoch 57/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6943 - acc: 0.5151 - val_loss: 0.6922 - val_acc: 0.5213\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6954 - acc: 0.5084 - val_loss: 0.6922 - val_acc: 0.5213\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6953 - acc: 0.5050 - val_loss: 0.6921 - val_acc: 0.5213\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6956 - acc: 0.5056 - val_loss: 0.6921 - val_acc: 0.5213\n",
      "Epoch 61/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6943 - acc: 0.5123 - val_loss: 0.6921 - val_acc: 0.5213\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6946 - acc: 0.5136 - val_loss: 0.6921 - val_acc: 0.5213\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6958 - acc: 0.5045 - val_loss: 0.6922 - val_acc: 0.5213\n",
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6966 - acc: 0.4903 - val_loss: 0.6923 - val_acc: 0.5213\n",
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6953 - acc: 0.5019 - val_loss: 0.6924 - val_acc: 0.5213\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6943 - acc: 0.5078 - val_loss: 0.6923 - val_acc: 0.5213\n",
      "Epoch 67/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6936 - acc: 0.5041 - val_loss: 0.6921 - val_acc: 0.5213\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.6935 - acc: 0.5153 - val_loss: 0.6921 - val_acc: 0.5213\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6944 - acc: 0.5108 - val_loss: 0.6921 - val_acc: 0.5213\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6947 - acc: 0.5058 - val_loss: 0.6921 - val_acc: 0.5213\n",
      "sample weight :  [4.66966543e-04 1.88904107e-05 2.67105440e-05 ... 2.02473541e-05\n",
      " 1.59926668e-04 4.78037237e-05]\n",
      "1323574.0\n",
      "new_x, new_y (6182, 10, 4068) (6182,)\n",
      "WARNING:tensorflow:Layer lstm_216 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_217 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_218 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_219 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/70\n",
      "2/2 [==============================] - 5s 532ms/step - loss: 0.7061 - acc: 0.5075 - val_loss: 0.6929 - val_acc: 0.5239\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6983 - acc: 0.5125 - val_loss: 0.6924 - val_acc: 0.5239\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.6985 - acc: 0.4912 - val_loss: 0.6950 - val_acc: 0.4761\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.7007 - acc: 0.4862 - val_loss: 0.6965 - val_acc: 0.4761\n",
      "Epoch 5/70\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 0.7005 - acc: 0.4974 - val_loss: 0.6957 - val_acc: 0.4761\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6972 - acc: 0.5063 - val_loss: 0.6941 - val_acc: 0.4761\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 156ms/step - loss: 0.6974 - acc: 0.5063 - val_loss: 0.6928 - val_acc: 0.5239\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 158ms/step - loss: 0.6965 - acc: 0.5009 - val_loss: 0.6921 - val_acc: 0.5239\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 159ms/step - loss: 0.6994 - acc: 0.4961 - val_loss: 0.6920 - val_acc: 0.5239\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6975 - acc: 0.4970 - val_loss: 0.6920 - val_acc: 0.5239\n",
      "Epoch 11/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6977 - acc: 0.5099 - val_loss: 0.6920 - val_acc: 0.5239\n",
      "Epoch 12/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6980 - acc: 0.5006 - val_loss: 0.6920 - val_acc: 0.5239\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.6966 - acc: 0.5039 - val_loss: 0.6920 - val_acc: 0.5239\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 0.6963 - acc: 0.5026 - val_loss: 0.6921 - val_acc: 0.5239\n",
      "Epoch 15/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6978 - acc: 0.4963 - val_loss: 0.6922 - val_acc: 0.5239\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6970 - acc: 0.5011 - val_loss: 0.6923 - val_acc: 0.5239\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6944 - acc: 0.5112 - val_loss: 0.6925 - val_acc: 0.5239\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 0.6955 - acc: 0.5065 - val_loss: 0.6928 - val_acc: 0.5239\n",
      "Epoch 19/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6946 - acc: 0.5082 - val_loss: 0.6930 - val_acc: 0.5239\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6971 - acc: 0.4955 - val_loss: 0.6931 - val_acc: 0.5239\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6947 - acc: 0.5052 - val_loss: 0.6931 - val_acc: 0.5239\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6955 - acc: 0.5069 - val_loss: 0.6930 - val_acc: 0.5239\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6967 - acc: 0.4920 - val_loss: 0.6929 - val_acc: 0.5239\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6946 - acc: 0.5063 - val_loss: 0.6927 - val_acc: 0.5239\n",
      "Epoch 25/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6950 - acc: 0.5056 - val_loss: 0.6925 - val_acc: 0.5239\n",
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6947 - acc: 0.5069 - val_loss: 0.6923 - val_acc: 0.5239\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6961 - acc: 0.5019 - val_loss: 0.6922 - val_acc: 0.5239\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6965 - acc: 0.4985 - val_loss: 0.6921 - val_acc: 0.5239\n",
      "Epoch 29/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6970 - acc: 0.4974 - val_loss: 0.6921 - val_acc: 0.5239\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6962 - acc: 0.5024 - val_loss: 0.6922 - val_acc: 0.5239\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6955 - acc: 0.5043 - val_loss: 0.6924 - val_acc: 0.5239\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6971 - acc: 0.4925 - val_loss: 0.6924 - val_acc: 0.5239\n",
      "Epoch 33/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6956 - acc: 0.5073 - val_loss: 0.6924 - val_acc: 0.5239\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6966 - acc: 0.4948 - val_loss: 0.6924 - val_acc: 0.5239\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6957 - acc: 0.5002 - val_loss: 0.6925 - val_acc: 0.5239\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6959 - acc: 0.4968 - val_loss: 0.6927 - val_acc: 0.5239\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6950 - acc: 0.5037 - val_loss: 0.6926 - val_acc: 0.5239\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6943 - acc: 0.5030 - val_loss: 0.6925 - val_acc: 0.5239\n",
      "Epoch 39/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6944 - acc: 0.5054 - val_loss: 0.6923 - val_acc: 0.5239\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6938 - acc: 0.5037 - val_loss: 0.6921 - val_acc: 0.5239\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6933 - acc: 0.5104 - val_loss: 0.6920 - val_acc: 0.5239\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6943 - acc: 0.5071 - val_loss: 0.6920 - val_acc: 0.5239\n",
      "Epoch 43/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6939 - acc: 0.5069 - val_loss: 0.6922 - val_acc: 0.5239\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6950 - acc: 0.5067 - val_loss: 0.6925 - val_acc: 0.5239\n",
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6936 - acc: 0.5000 - val_loss: 0.6929 - val_acc: 0.5239\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6951 - acc: 0.4959 - val_loss: 0.6932 - val_acc: 0.4761\n",
      "Epoch 47/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6945 - acc: 0.4959 - val_loss: 0.6933 - val_acc: 0.4761\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6937 - acc: 0.5054 - val_loss: 0.6933 - val_acc: 0.4761\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6949 - acc: 0.5043 - val_loss: 0.6933 - val_acc: 0.4761\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6947 - acc: 0.4948 - val_loss: 0.6933 - val_acc: 0.4761\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6938 - acc: 0.4981 - val_loss: 0.6933 - val_acc: 0.4761\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6943 - acc: 0.4877 - val_loss: 0.6932 - val_acc: 0.4761\n",
      "Epoch 53/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6941 - acc: 0.4927 - val_loss: 0.6930 - val_acc: 0.5239\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6936 - acc: 0.5050 - val_loss: 0.6927 - val_acc: 0.5239\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6937 - acc: 0.5039 - val_loss: 0.6923 - val_acc: 0.5239\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6931 - acc: 0.5045 - val_loss: 0.6922 - val_acc: 0.5239\n",
      "Epoch 57/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6939 - acc: 0.5017 - val_loss: 0.6921 - val_acc: 0.5239\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6946 - acc: 0.4959 - val_loss: 0.6921 - val_acc: 0.5239\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6939 - acc: 0.5101 - val_loss: 0.6922 - val_acc: 0.5239\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6939 - acc: 0.5039 - val_loss: 0.6923 - val_acc: 0.5239\n",
      "Epoch 61/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6934 - acc: 0.5073 - val_loss: 0.6925 - val_acc: 0.5239\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6935 - acc: 0.4987 - val_loss: 0.6926 - val_acc: 0.5239\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6937 - acc: 0.5065 - val_loss: 0.6926 - val_acc: 0.5239\n",
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6932 - acc: 0.5054 - val_loss: 0.6927 - val_acc: 0.5239\n",
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6947 - acc: 0.4922 - val_loss: 0.6927 - val_acc: 0.5239\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 0.6937 - acc: 0.4965 - val_loss: 0.6928 - val_acc: 0.5239\n",
      "Epoch 67/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6933 - acc: 0.5067 - val_loss: 0.6927 - val_acc: 0.5239\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6930 - acc: 0.5091 - val_loss: 0.6925 - val_acc: 0.5239\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.6927 - acc: 0.5093 - val_loss: 0.6924 - val_acc: 0.5239\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6940 - acc: 0.5082 - val_loss: 0.6925 - val_acc: 0.5239\n",
      "sample weight :  [4.61401054e-04 1.86772734e-05 2.71319783e-05 ... 2.04847812e-05\n",
      " 1.62540995e-04 4.83880471e-05]\n",
      "1306377.0\n",
      "new_x, new_y (6182, 10, 4068) (6182,)\n",
      "WARNING:tensorflow:Layer lstm_220 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_221 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_222 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_223 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/70\n",
      "2/2 [==============================] - 5s 681ms/step - loss: 0.7713 - acc: 0.4903 - val_loss: 0.7277 - val_acc: 0.4981\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 0.7326 - acc: 0.4963 - val_loss: 0.7063 - val_acc: 0.4981\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.7149 - acc: 0.4920 - val_loss: 0.6955 - val_acc: 0.4981\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 0s 325ms/step - loss: 0.7033 - acc: 0.4948 - val_loss: 0.6933 - val_acc: 0.5019\n",
      "Epoch 5/70\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 0.7014 - acc: 0.4961 - val_loss: 0.6973 - val_acc: 0.5019\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.7015 - acc: 0.5073 - val_loss: 0.7008 - val_acc: 0.5019\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.7058 - acc: 0.5022 - val_loss: 0.7008 - val_acc: 0.5019\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.7068 - acc: 0.5024 - val_loss: 0.6988 - val_acc: 0.5019\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 156ms/step - loss: 0.7050 - acc: 0.5032 - val_loss: 0.6963 - val_acc: 0.5019\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 159ms/step - loss: 0.7000 - acc: 0.5134 - val_loss: 0.6944 - val_acc: 0.5019\n",
      "Epoch 11/70\n",
      "2/2 [==============================] - 0s 157ms/step - loss: 0.7024 - acc: 0.4905 - val_loss: 0.6934 - val_acc: 0.5019\n",
      "Epoch 12/70\n",
      "2/2 [==============================] - 0s 161ms/step - loss: 0.6998 - acc: 0.5015 - val_loss: 0.6931 - val_acc: 0.5239\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6984 - acc: 0.5039 - val_loss: 0.6935 - val_acc: 0.4981\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.7020 - acc: 0.4925 - val_loss: 0.6939 - val_acc: 0.4981\n",
      "Epoch 15/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6988 - acc: 0.4998 - val_loss: 0.6940 - val_acc: 0.4981\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.7004 - acc: 0.4940 - val_loss: 0.6938 - val_acc: 0.4981\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.7011 - acc: 0.4940 - val_loss: 0.6935 - val_acc: 0.4981\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 0.6984 - acc: 0.5082 - val_loss: 0.6932 - val_acc: 0.4981\n",
      "Epoch 19/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6995 - acc: 0.5032 - val_loss: 0.6931 - val_acc: 0.5019\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6990 - acc: 0.5041 - val_loss: 0.6933 - val_acc: 0.5019\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6984 - acc: 0.5009 - val_loss: 0.6935 - val_acc: 0.5019\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6976 - acc: 0.4996 - val_loss: 0.6936 - val_acc: 0.5019\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6998 - acc: 0.4970 - val_loss: 0.6937 - val_acc: 0.5019\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.7001 - acc: 0.4929 - val_loss: 0.6937 - val_acc: 0.5019\n",
      "Epoch 25/70\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.6983 - acc: 0.5009 - val_loss: 0.6937 - val_acc: 0.5019\n",
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.6963 - acc: 0.5084 - val_loss: 0.6937 - val_acc: 0.5019\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.6974 - acc: 0.5015 - val_loss: 0.6936 - val_acc: 0.5019\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6981 - acc: 0.5045 - val_loss: 0.6936 - val_acc: 0.5019\n",
      "Epoch 29/70\n",
      "2/2 [==============================] - 0s 347ms/step - loss: 0.6983 - acc: 0.4972 - val_loss: 0.6936 - val_acc: 0.5019\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6967 - acc: 0.5030 - val_loss: 0.6934 - val_acc: 0.5019\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6971 - acc: 0.5078 - val_loss: 0.6933 - val_acc: 0.5019\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6961 - acc: 0.5091 - val_loss: 0.6932 - val_acc: 0.5019\n",
      "Epoch 33/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6970 - acc: 0.5030 - val_loss: 0.6932 - val_acc: 0.5019\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6970 - acc: 0.5073 - val_loss: 0.6932 - val_acc: 0.5019\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6984 - acc: 0.4899 - val_loss: 0.6932 - val_acc: 0.5019\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6947 - acc: 0.5123 - val_loss: 0.6933 - val_acc: 0.5019\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6987 - acc: 0.4981 - val_loss: 0.6933 - val_acc: 0.5019\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6960 - acc: 0.5035 - val_loss: 0.6933 - val_acc: 0.5019\n",
      "Epoch 39/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6978 - acc: 0.4903 - val_loss: 0.6932 - val_acc: 0.5019\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6957 - acc: 0.5041 - val_loss: 0.6932 - val_acc: 0.5019\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6975 - acc: 0.4959 - val_loss: 0.6932 - val_acc: 0.5019\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6961 - acc: 0.4996 - val_loss: 0.6932 - val_acc: 0.5019\n",
      "Epoch 43/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6965 - acc: 0.4989 - val_loss: 0.6932 - val_acc: 0.5019\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6950 - acc: 0.5093 - val_loss: 0.6932 - val_acc: 0.5019\n",
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6969 - acc: 0.4989 - val_loss: 0.6932 - val_acc: 0.5019\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6975 - acc: 0.4888 - val_loss: 0.6932 - val_acc: 0.5019\n",
      "Epoch 47/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6958 - acc: 0.5004 - val_loss: 0.6932 - val_acc: 0.5019\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6985 - acc: 0.4912 - val_loss: 0.6932 - val_acc: 0.5019\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6966 - acc: 0.5082 - val_loss: 0.6931 - val_acc: 0.5019\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6947 - acc: 0.5043 - val_loss: 0.6931 - val_acc: 0.5019\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6948 - acc: 0.5101 - val_loss: 0.6931 - val_acc: 0.4981\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6962 - acc: 0.4985 - val_loss: 0.6931 - val_acc: 0.4981\n",
      "Epoch 53/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6971 - acc: 0.4981 - val_loss: 0.6931 - val_acc: 0.4981\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 0.6961 - acc: 0.4948 - val_loss: 0.6931 - val_acc: 0.4981\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6968 - acc: 0.4944 - val_loss: 0.6931 - val_acc: 0.5019\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6967 - acc: 0.5022 - val_loss: 0.6931 - val_acc: 0.5019\n",
      "Epoch 57/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6963 - acc: 0.4953 - val_loss: 0.6933 - val_acc: 0.5019\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6955 - acc: 0.4994 - val_loss: 0.6934 - val_acc: 0.5019\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6943 - acc: 0.5104 - val_loss: 0.6934 - val_acc: 0.5019\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6950 - acc: 0.5101 - val_loss: 0.6934 - val_acc: 0.5019\n",
      "Epoch 61/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6959 - acc: 0.5080 - val_loss: 0.6933 - val_acc: 0.5019\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6947 - acc: 0.5132 - val_loss: 0.6932 - val_acc: 0.5019\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6960 - acc: 0.4978 - val_loss: 0.6931 - val_acc: 0.5019\n",
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6955 - acc: 0.4985 - val_loss: 0.6931 - val_acc: 0.5019\n",
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 154ms/step - loss: 0.6957 - acc: 0.4989 - val_loss: 0.6931 - val_acc: 0.5019\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6967 - acc: 0.4959 - val_loss: 0.6932 - val_acc: 0.5019\n",
      "Epoch 67/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.6947 - acc: 0.5060 - val_loss: 0.6932 - val_acc: 0.5019\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6949 - acc: 0.4985 - val_loss: 0.6931 - val_acc: 0.5019\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6940 - acc: 0.5093 - val_loss: 0.6930 - val_acc: 0.5019\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6950 - acc: 0.5011 - val_loss: 0.6930 - val_acc: 0.5032\n",
      "sample weight :  [4.59446119e-04 1.86336816e-05 2.72705351e-05 ... 2.05805874e-05\n",
      " 1.63409621e-04 4.85550122e-05]\n",
      "1316482.0\n",
      "new_x, new_y (6182, 10, 4068) (6182,)\n",
      "WARNING:tensorflow:Layer lstm_224 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_225 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_226 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_227 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/70\n",
      "2/2 [==============================] - 5s 560ms/step - loss: 0.7020 - acc: 0.5054 - val_loss: 0.6927 - val_acc: 0.5285\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.7009 - acc: 0.4974 - val_loss: 0.6966 - val_acc: 0.4715\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.7008 - acc: 0.5030 - val_loss: 0.6970 - val_acc: 0.4715\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.7029 - acc: 0.4961 - val_loss: 0.6953 - val_acc: 0.4715\n",
      "Epoch 5/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6992 - acc: 0.5047 - val_loss: 0.6934 - val_acc: 0.4715\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6988 - acc: 0.5017 - val_loss: 0.6921 - val_acc: 0.5285\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6961 - acc: 0.5181 - val_loss: 0.6917 - val_acc: 0.5285\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6968 - acc: 0.5153 - val_loss: 0.6917 - val_acc: 0.5285\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6974 - acc: 0.5028 - val_loss: 0.6920 - val_acc: 0.5285\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6992 - acc: 0.4998 - val_loss: 0.6925 - val_acc: 0.5285\n",
      "Epoch 11/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6997 - acc: 0.4912 - val_loss: 0.6931 - val_acc: 0.5285\n",
      "Epoch 12/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6983 - acc: 0.4970 - val_loss: 0.6934 - val_acc: 0.4715\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6958 - acc: 0.5095 - val_loss: 0.6932 - val_acc: 0.4715\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6968 - acc: 0.5093 - val_loss: 0.6930 - val_acc: 0.5285\n",
      "Epoch 15/70\n",
      "2/2 [==============================] - 1s 144ms/step - loss: 0.6977 - acc: 0.5004 - val_loss: 0.6929 - val_acc: 0.5285\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6966 - acc: 0.4996 - val_loss: 0.6932 - val_acc: 0.4715\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 171ms/step - loss: 0.6968 - acc: 0.4937 - val_loss: 0.6934 - val_acc: 0.4715\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 168ms/step - loss: 0.6969 - acc: 0.4978 - val_loss: 0.6934 - val_acc: 0.4715\n",
      "Epoch 19/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 172ms/step - loss: 0.6951 - acc: 0.4981 - val_loss: 0.6933 - val_acc: 0.4715\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 154ms/step - loss: 0.6973 - acc: 0.4920 - val_loss: 0.6932 - val_acc: 0.4715\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 157ms/step - loss: 0.6963 - acc: 0.4929 - val_loss: 0.6928 - val_acc: 0.5285\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6945 - acc: 0.5043 - val_loss: 0.6925 - val_acc: 0.5285\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6966 - acc: 0.4957 - val_loss: 0.6924 - val_acc: 0.5285\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.6947 - acc: 0.5078 - val_loss: 0.6925 - val_acc: 0.5285\n",
      "Epoch 25/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6951 - acc: 0.5067 - val_loss: 0.6925 - val_acc: 0.5285\n",
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6948 - acc: 0.5093 - val_loss: 0.6924 - val_acc: 0.5285\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6940 - acc: 0.5065 - val_loss: 0.6922 - val_acc: 0.5285\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6955 - acc: 0.4957 - val_loss: 0.6922 - val_acc: 0.5285\n",
      "Epoch 29/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6937 - acc: 0.5006 - val_loss: 0.6926 - val_acc: 0.5285\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6958 - acc: 0.4935 - val_loss: 0.6933 - val_acc: 0.4715\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6942 - acc: 0.5039 - val_loss: 0.6940 - val_acc: 0.4715\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6942 - acc: 0.5091 - val_loss: 0.6945 - val_acc: 0.4715\n",
      "Epoch 33/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6948 - acc: 0.5006 - val_loss: 0.6944 - val_acc: 0.4715\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6955 - acc: 0.5004 - val_loss: 0.6937 - val_acc: 0.4715\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6932 - acc: 0.5155 - val_loss: 0.6929 - val_acc: 0.5285\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6939 - acc: 0.5050 - val_loss: 0.6926 - val_acc: 0.5285\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6936 - acc: 0.5162 - val_loss: 0.6927 - val_acc: 0.5285\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6943 - acc: 0.5056 - val_loss: 0.6932 - val_acc: 0.4715\n",
      "Epoch 39/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6949 - acc: 0.5073 - val_loss: 0.6939 - val_acc: 0.4715\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6940 - acc: 0.5065 - val_loss: 0.6945 - val_acc: 0.4715\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6954 - acc: 0.4896 - val_loss: 0.6948 - val_acc: 0.4715\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6949 - acc: 0.4987 - val_loss: 0.6945 - val_acc: 0.4715\n",
      "Epoch 43/70\n",
      "2/2 [==============================] - 0s 151ms/step - loss: 0.6945 - acc: 0.5065 - val_loss: 0.6935 - val_acc: 0.4715\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6947 - acc: 0.4996 - val_loss: 0.6924 - val_acc: 0.5285\n",
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6938 - acc: 0.5088 - val_loss: 0.6919 - val_acc: 0.5285\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6938 - acc: 0.5041 - val_loss: 0.6920 - val_acc: 0.5285\n",
      "Epoch 47/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6936 - acc: 0.5026 - val_loss: 0.6923 - val_acc: 0.5285\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 153ms/step - loss: 0.6937 - acc: 0.5002 - val_loss: 0.6930 - val_acc: 0.5310\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6935 - acc: 0.5009 - val_loss: 0.6938 - val_acc: 0.4715\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6947 - acc: 0.5002 - val_loss: 0.6943 - val_acc: 0.4715\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6942 - acc: 0.4978 - val_loss: 0.6943 - val_acc: 0.4715\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6954 - acc: 0.4832 - val_loss: 0.6938 - val_acc: 0.4715\n",
      "Epoch 53/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6934 - acc: 0.5041 - val_loss: 0.6932 - val_acc: 0.4715\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6934 - acc: 0.5065 - val_loss: 0.6926 - val_acc: 0.5285\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6926 - acc: 0.5147 - val_loss: 0.6920 - val_acc: 0.5285\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6941 - acc: 0.5060 - val_loss: 0.6916 - val_acc: 0.5285\n",
      "Epoch 57/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6948 - acc: 0.4987 - val_loss: 0.6916 - val_acc: 0.5285\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6945 - acc: 0.5065 - val_loss: 0.6917 - val_acc: 0.5285\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6931 - acc: 0.5125 - val_loss: 0.6921 - val_acc: 0.5285\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6951 - acc: 0.4909 - val_loss: 0.6926 - val_acc: 0.5285\n",
      "Epoch 61/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6933 - acc: 0.5013 - val_loss: 0.6927 - val_acc: 0.5369\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6928 - acc: 0.5162 - val_loss: 0.6926 - val_acc: 0.5291\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.6939 - acc: 0.5084 - val_loss: 0.6925 - val_acc: 0.5304\n",
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6930 - acc: 0.5082 - val_loss: 0.6927 - val_acc: 0.5576\n",
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6941 - acc: 0.5032 - val_loss: 0.6926 - val_acc: 0.5537\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6932 - acc: 0.5080 - val_loss: 0.6924 - val_acc: 0.5530\n",
      "Epoch 67/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6920 - acc: 0.5231 - val_loss: 0.6922 - val_acc: 0.5505\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6924 - acc: 0.5250 - val_loss: 0.6919 - val_acc: 0.5472\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6919 - acc: 0.5317 - val_loss: 0.6918 - val_acc: 0.5511\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6905 - acc: 0.5302 - val_loss: 0.6916 - val_acc: 0.5485\n",
      "sample weight :  [4.55451498e-04 1.84446349e-05 2.85031009e-05 ... 2.06318621e-05\n",
      " 1.66583823e-04 4.89208462e-05]\n",
      "1300609.0\n",
      "new_x, new_y (6182, 10, 4068) (6182,)\n",
      "WARNING:tensorflow:Layer lstm_228 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_229 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_230 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_231 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/70\n",
      "2/2 [==============================] - 6s 596ms/step - loss: 0.7225 - acc: 0.4983 - val_loss: 0.7045 - val_acc: 0.4845\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.7058 - acc: 0.4983 - val_loss: 0.6949 - val_acc: 0.4845\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.7016 - acc: 0.4965 - val_loss: 0.6927 - val_acc: 0.5155\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6992 - acc: 0.5071 - val_loss: 0.6932 - val_acc: 0.5155\n",
      "Epoch 5/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 138ms/step - loss: 0.7003 - acc: 0.5101 - val_loss: 0.6930 - val_acc: 0.5155\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.7001 - acc: 0.5032 - val_loss: 0.6927 - val_acc: 0.5155\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6995 - acc: 0.4948 - val_loss: 0.6929 - val_acc: 0.5155\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6986 - acc: 0.4965 - val_loss: 0.6937 - val_acc: 0.4845\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6989 - acc: 0.5056 - val_loss: 0.6947 - val_acc: 0.4845\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6995 - acc: 0.4974 - val_loss: 0.6953 - val_acc: 0.4845\n",
      "Epoch 11/70\n",
      "2/2 [==============================] - 0s 153ms/step - loss: 0.6992 - acc: 0.5019 - val_loss: 0.6955 - val_acc: 0.4845\n",
      "Epoch 12/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6967 - acc: 0.5039 - val_loss: 0.6952 - val_acc: 0.4845\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6978 - acc: 0.4994 - val_loss: 0.6947 - val_acc: 0.4845\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6961 - acc: 0.5054 - val_loss: 0.6942 - val_acc: 0.4845\n",
      "Epoch 15/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6966 - acc: 0.5058 - val_loss: 0.6936 - val_acc: 0.4845\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6958 - acc: 0.5084 - val_loss: 0.6932 - val_acc: 0.4845\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6986 - acc: 0.4925 - val_loss: 0.6929 - val_acc: 0.5155\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6969 - acc: 0.5017 - val_loss: 0.6928 - val_acc: 0.5155\n",
      "Epoch 19/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6964 - acc: 0.4976 - val_loss: 0.6929 - val_acc: 0.5155\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6981 - acc: 0.4959 - val_loss: 0.6932 - val_acc: 0.4845\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6975 - acc: 0.4970 - val_loss: 0.6935 - val_acc: 0.4845\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6945 - acc: 0.5075 - val_loss: 0.6941 - val_acc: 0.4845\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6963 - acc: 0.5104 - val_loss: 0.6947 - val_acc: 0.4845\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6931 - acc: 0.5209 - val_loss: 0.6950 - val_acc: 0.4845\n",
      "Epoch 25/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6971 - acc: 0.4959 - val_loss: 0.6947 - val_acc: 0.4845\n",
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 152ms/step - loss: 0.6964 - acc: 0.5011 - val_loss: 0.6942 - val_acc: 0.4845\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 153ms/step - loss: 0.6942 - acc: 0.5097 - val_loss: 0.6936 - val_acc: 0.4845\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6964 - acc: 0.5032 - val_loss: 0.6931 - val_acc: 0.4845\n",
      "Epoch 29/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6951 - acc: 0.5194 - val_loss: 0.6929 - val_acc: 0.5155\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6959 - acc: 0.5028 - val_loss: 0.6928 - val_acc: 0.5155\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6960 - acc: 0.5000 - val_loss: 0.6929 - val_acc: 0.5155\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6954 - acc: 0.5080 - val_loss: 0.6930 - val_acc: 0.5155\n",
      "Epoch 33/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6949 - acc: 0.5050 - val_loss: 0.6930 - val_acc: 0.5155\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6962 - acc: 0.4935 - val_loss: 0.6932 - val_acc: 0.4845\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6929 - acc: 0.5162 - val_loss: 0.6934 - val_acc: 0.4845\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6940 - acc: 0.5017 - val_loss: 0.6934 - val_acc: 0.4845\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6949 - acc: 0.5039 - val_loss: 0.6936 - val_acc: 0.4845\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6941 - acc: 0.5129 - val_loss: 0.6940 - val_acc: 0.4845\n",
      "Epoch 39/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6956 - acc: 0.5011 - val_loss: 0.6940 - val_acc: 0.4845\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6945 - acc: 0.5067 - val_loss: 0.6936 - val_acc: 0.4845\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6948 - acc: 0.5019 - val_loss: 0.6934 - val_acc: 0.4845\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6956 - acc: 0.4996 - val_loss: 0.6932 - val_acc: 0.4845\n",
      "Epoch 43/70\n",
      "2/2 [==============================] - 0s 194ms/step - loss: 0.6950 - acc: 0.4965 - val_loss: 0.6933 - val_acc: 0.4845\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6944 - acc: 0.5058 - val_loss: 0.6935 - val_acc: 0.4845\n",
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6958 - acc: 0.4940 - val_loss: 0.6936 - val_acc: 0.4845\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6948 - acc: 0.5035 - val_loss: 0.6937 - val_acc: 0.4845\n",
      "Epoch 47/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6950 - acc: 0.5000 - val_loss: 0.6935 - val_acc: 0.4845\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6959 - acc: 0.4905 - val_loss: 0.6934 - val_acc: 0.4845\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 0.6943 - acc: 0.5065 - val_loss: 0.6932 - val_acc: 0.4845\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6962 - acc: 0.4849 - val_loss: 0.6932 - val_acc: 0.4845\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6946 - acc: 0.5045 - val_loss: 0.6933 - val_acc: 0.4845\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6949 - acc: 0.4886 - val_loss: 0.6935 - val_acc: 0.4845\n",
      "Epoch 53/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6946 - acc: 0.4970 - val_loss: 0.6936 - val_acc: 0.4845\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6945 - acc: 0.5106 - val_loss: 0.6937 - val_acc: 0.4845\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6937 - acc: 0.5073 - val_loss: 0.6939 - val_acc: 0.4845\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6946 - acc: 0.5000 - val_loss: 0.6939 - val_acc: 0.4845\n",
      "Epoch 57/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6949 - acc: 0.4985 - val_loss: 0.6937 - val_acc: 0.4845\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6950 - acc: 0.4942 - val_loss: 0.6935 - val_acc: 0.4845\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6940 - acc: 0.5084 - val_loss: 0.6933 - val_acc: 0.4845\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6938 - acc: 0.5022 - val_loss: 0.6932 - val_acc: 0.4845\n",
      "Epoch 61/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6939 - acc: 0.4963 - val_loss: 0.6933 - val_acc: 0.4845\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6938 - acc: 0.4955 - val_loss: 0.6934 - val_acc: 0.4845\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6951 - acc: 0.4983 - val_loss: 0.6934 - val_acc: 0.4845\n",
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6938 - acc: 0.5073 - val_loss: 0.6932 - val_acc: 0.4845\n",
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6943 - acc: 0.4996 - val_loss: 0.6930 - val_acc: 0.5155\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6946 - acc: 0.4959 - val_loss: 0.6928 - val_acc: 0.5155\n",
      "Epoch 67/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6949 - acc: 0.4989 - val_loss: 0.6928 - val_acc: 0.5155\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6945 - acc: 0.4987 - val_loss: 0.6928 - val_acc: 0.5155\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6938 - acc: 0.4996 - val_loss: 0.6929 - val_acc: 0.5155\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6942 - acc: 0.4950 - val_loss: 0.6929 - val_acc: 0.5155\n",
      "sample weight :  [4.57816835e-04 1.85308373e-05 2.83643489e-05 ... 2.05333606e-05\n",
      " 1.65788927e-04 4.86534078e-05]\n",
      "1319798.0\n",
      "new_x, new_y (6182, 10, 4068) (6182,)\n",
      "WARNING:tensorflow:Layer lstm_232 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_233 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_234 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_235 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/70\n",
      "2/2 [==============================] - 6s 564ms/step - loss: 0.7005 - acc: 0.5026 - val_loss: 0.6932 - val_acc: 0.5058\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6971 - acc: 0.5164 - val_loss: 0.6933 - val_acc: 0.5058\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6993 - acc: 0.5116 - val_loss: 0.6931 - val_acc: 0.5058\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.7019 - acc: 0.4916 - val_loss: 0.6933 - val_acc: 0.4942\n",
      "Epoch 5/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6978 - acc: 0.5052 - val_loss: 0.6938 - val_acc: 0.4942\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6982 - acc: 0.5015 - val_loss: 0.6938 - val_acc: 0.4942\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.7020 - acc: 0.4814 - val_loss: 0.6933 - val_acc: 0.4942\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6980 - acc: 0.4985 - val_loss: 0.6931 - val_acc: 0.5058\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6985 - acc: 0.4957 - val_loss: 0.6931 - val_acc: 0.5058\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6994 - acc: 0.4942 - val_loss: 0.6931 - val_acc: 0.5058\n",
      "Epoch 11/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.6973 - acc: 0.5002 - val_loss: 0.6933 - val_acc: 0.4942\n",
      "Epoch 12/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6942 - acc: 0.5136 - val_loss: 0.6935 - val_acc: 0.4942\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6969 - acc: 0.4957 - val_loss: 0.6934 - val_acc: 0.4942\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6970 - acc: 0.5006 - val_loss: 0.6932 - val_acc: 0.4942\n",
      "Epoch 15/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6963 - acc: 0.4968 - val_loss: 0.6931 - val_acc: 0.5058\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6946 - acc: 0.5114 - val_loss: 0.6931 - val_acc: 0.5058\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6990 - acc: 0.4849 - val_loss: 0.6931 - val_acc: 0.5058\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6957 - acc: 0.5047 - val_loss: 0.6931 - val_acc: 0.5058\n",
      "Epoch 19/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6971 - acc: 0.4981 - val_loss: 0.6932 - val_acc: 0.4942\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 153ms/step - loss: 0.6950 - acc: 0.5022 - val_loss: 0.6936 - val_acc: 0.4942\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6942 - acc: 0.5024 - val_loss: 0.6940 - val_acc: 0.4942\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6956 - acc: 0.5045 - val_loss: 0.6937 - val_acc: 0.4942\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6939 - acc: 0.5198 - val_loss: 0.6932 - val_acc: 0.4942\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6963 - acc: 0.4944 - val_loss: 0.6931 - val_acc: 0.5058\n",
      "Epoch 25/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6968 - acc: 0.4849 - val_loss: 0.6933 - val_acc: 0.5058\n",
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6964 - acc: 0.4933 - val_loss: 0.6935 - val_acc: 0.5058\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6982 - acc: 0.4914 - val_loss: 0.6935 - val_acc: 0.5058\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6957 - acc: 0.4983 - val_loss: 0.6933 - val_acc: 0.5058\n",
      "Epoch 29/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6956 - acc: 0.4968 - val_loss: 0.6931 - val_acc: 0.5058\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6938 - acc: 0.5017 - val_loss: 0.6933 - val_acc: 0.4942\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6937 - acc: 0.5104 - val_loss: 0.6939 - val_acc: 0.4942\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6950 - acc: 0.4985 - val_loss: 0.6946 - val_acc: 0.4942\n",
      "Epoch 33/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6961 - acc: 0.4957 - val_loss: 0.6949 - val_acc: 0.4942\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6961 - acc: 0.4959 - val_loss: 0.6945 - val_acc: 0.4942\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6955 - acc: 0.4989 - val_loss: 0.6939 - val_acc: 0.4942\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6946 - acc: 0.5037 - val_loss: 0.6934 - val_acc: 0.4942\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6938 - acc: 0.5054 - val_loss: 0.6932 - val_acc: 0.4942\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 0.6959 - acc: 0.4888 - val_loss: 0.6931 - val_acc: 0.5058\n",
      "Epoch 39/70\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 0.6935 - acc: 0.5086 - val_loss: 0.6931 - val_acc: 0.5058\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6949 - acc: 0.4972 - val_loss: 0.6931 - val_acc: 0.5058\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6938 - acc: 0.5013 - val_loss: 0.6932 - val_acc: 0.4942\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6941 - acc: 0.5006 - val_loss: 0.6933 - val_acc: 0.4942\n",
      "Epoch 43/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6939 - acc: 0.5024 - val_loss: 0.6933 - val_acc: 0.4942\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6953 - acc: 0.4914 - val_loss: 0.6932 - val_acc: 0.4942\n",
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6949 - acc: 0.4963 - val_loss: 0.6931 - val_acc: 0.5058\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6936 - acc: 0.5043 - val_loss: 0.6931 - val_acc: 0.5058\n",
      "Epoch 47/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6942 - acc: 0.5039 - val_loss: 0.6930 - val_acc: 0.5058\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6943 - acc: 0.5000 - val_loss: 0.6931 - val_acc: 0.5058\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6950 - acc: 0.5019 - val_loss: 0.6931 - val_acc: 0.5058\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 0.6923 - acc: 0.5192 - val_loss: 0.6931 - val_acc: 0.5188\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6929 - acc: 0.5095 - val_loss: 0.6931 - val_acc: 0.4942\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 354ms/step - loss: 0.6937 - acc: 0.5075 - val_loss: 0.6932 - val_acc: 0.4942\n",
      "Epoch 53/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 148ms/step - loss: 0.6927 - acc: 0.5073 - val_loss: 0.6933 - val_acc: 0.4942\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6941 - acc: 0.5032 - val_loss: 0.6935 - val_acc: 0.4942\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6935 - acc: 0.5035 - val_loss: 0.6936 - val_acc: 0.4942\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6958 - acc: 0.4881 - val_loss: 0.6938 - val_acc: 0.4942\n",
      "Epoch 57/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6947 - acc: 0.5011 - val_loss: 0.6939 - val_acc: 0.4942\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6942 - acc: 0.5017 - val_loss: 0.6939 - val_acc: 0.4942\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6933 - acc: 0.5047 - val_loss: 0.6938 - val_acc: 0.4942\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6939 - acc: 0.5101 - val_loss: 0.6934 - val_acc: 0.4942\n",
      "Epoch 61/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6939 - acc: 0.5095 - val_loss: 0.6931 - val_acc: 0.4942\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6930 - acc: 0.5183 - val_loss: 0.6930 - val_acc: 0.5058\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6939 - acc: 0.5078 - val_loss: 0.6930 - val_acc: 0.5058\n",
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6925 - acc: 0.5082 - val_loss: 0.6931 - val_acc: 0.5058\n",
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6951 - acc: 0.4916 - val_loss: 0.6930 - val_acc: 0.5058\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6925 - acc: 0.5080 - val_loss: 0.6930 - val_acc: 0.5058\n",
      "Epoch 67/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6942 - acc: 0.4899 - val_loss: 0.6932 - val_acc: 0.4942\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6928 - acc: 0.4987 - val_loss: 0.6936 - val_acc: 0.4942\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6944 - acc: 0.4918 - val_loss: 0.6940 - val_acc: 0.4942\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6947 - acc: 0.5032 - val_loss: 0.6943 - val_acc: 0.4942\n",
      "sample weight :  [4.38602210e-04 1.77649935e-05 2.95758103e-05 ... 2.13432302e-05\n",
      " 1.72636238e-04 5.04666757e-05]\n",
      "1310055.0\n",
      "new_x, new_y (6182, 10, 4068) (6182,)\n",
      "WARNING:tensorflow:Layer lstm_236 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_237 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_238 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_239 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/70\n",
      "2/2 [==============================] - 6s 563ms/step - loss: 0.7007 - acc: 0.5147 - val_loss: 0.6920 - val_acc: 0.5239\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6990 - acc: 0.5043 - val_loss: 0.6921 - val_acc: 0.5239\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6938 - acc: 0.5237 - val_loss: 0.6921 - val_acc: 0.5239\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6985 - acc: 0.4985 - val_loss: 0.6920 - val_acc: 0.5239\n",
      "Epoch 5/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6973 - acc: 0.5153 - val_loss: 0.6921 - val_acc: 0.5239\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6945 - acc: 0.5203 - val_loss: 0.6925 - val_acc: 0.5239\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6983 - acc: 0.5088 - val_loss: 0.6928 - val_acc: 0.5239\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6976 - acc: 0.5179 - val_loss: 0.6925 - val_acc: 0.5239\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6982 - acc: 0.5255 - val_loss: 0.6921 - val_acc: 0.5239\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6963 - acc: 0.5173 - val_loss: 0.6920 - val_acc: 0.5239\n",
      "Epoch 11/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6984 - acc: 0.4976 - val_loss: 0.6921 - val_acc: 0.5239\n",
      "Epoch 12/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6944 - acc: 0.5088 - val_loss: 0.6924 - val_acc: 0.5239\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 0.6967 - acc: 0.5032 - val_loss: 0.6925 - val_acc: 0.5239\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6932 - acc: 0.5224 - val_loss: 0.6925 - val_acc: 0.5239\n",
      "Epoch 15/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6954 - acc: 0.5037 - val_loss: 0.6922 - val_acc: 0.5239\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6941 - acc: 0.5179 - val_loss: 0.6921 - val_acc: 0.5239\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6958 - acc: 0.5080 - val_loss: 0.6920 - val_acc: 0.5239\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6959 - acc: 0.5140 - val_loss: 0.6921 - val_acc: 0.5239\n",
      "Epoch 19/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6948 - acc: 0.5125 - val_loss: 0.6923 - val_acc: 0.5239\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6967 - acc: 0.5099 - val_loss: 0.6923 - val_acc: 0.5239\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6956 - acc: 0.5153 - val_loss: 0.6921 - val_acc: 0.5239\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6955 - acc: 0.5211 - val_loss: 0.6920 - val_acc: 0.5239\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6952 - acc: 0.5129 - val_loss: 0.6920 - val_acc: 0.5239\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6944 - acc: 0.5119 - val_loss: 0.6921 - val_acc: 0.5239\n",
      "Epoch 25/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6939 - acc: 0.5065 - val_loss: 0.6922 - val_acc: 0.5239\n",
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6949 - acc: 0.5097 - val_loss: 0.6922 - val_acc: 0.5239\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6933 - acc: 0.5088 - val_loss: 0.6921 - val_acc: 0.5239\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6939 - acc: 0.5132 - val_loss: 0.6920 - val_acc: 0.5239\n",
      "Epoch 29/70\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6954 - acc: 0.4974 - val_loss: 0.6920 - val_acc: 0.5239\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6942 - acc: 0.5125 - val_loss: 0.6920 - val_acc: 0.5239\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6952 - acc: 0.5056 - val_loss: 0.6921 - val_acc: 0.5239\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 0.6926 - acc: 0.5192 - val_loss: 0.6921 - val_acc: 0.5239\n",
      "Epoch 33/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6934 - acc: 0.5155 - val_loss: 0.6920 - val_acc: 0.5239\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6951 - acc: 0.5147 - val_loss: 0.6920 - val_acc: 0.5239\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6943 - acc: 0.5047 - val_loss: 0.6921 - val_acc: 0.5239\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6935 - acc: 0.5203 - val_loss: 0.6923 - val_acc: 0.5239\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6937 - acc: 0.5110 - val_loss: 0.6923 - val_acc: 0.5239\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.6944 - acc: 0.5002 - val_loss: 0.6921 - val_acc: 0.5239\n",
      "Epoch 39/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6938 - acc: 0.5078 - val_loss: 0.6920 - val_acc: 0.5239\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6934 - acc: 0.5091 - val_loss: 0.6920 - val_acc: 0.5239\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6938 - acc: 0.5173 - val_loss: 0.6921 - val_acc: 0.5239\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6929 - acc: 0.5272 - val_loss: 0.6921 - val_acc: 0.5239\n",
      "Epoch 43/70\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6936 - acc: 0.5162 - val_loss: 0.6921 - val_acc: 0.5239\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6940 - acc: 0.5147 - val_loss: 0.6920 - val_acc: 0.5239\n",
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6927 - acc: 0.5188 - val_loss: 0.6920 - val_acc: 0.5239\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6948 - acc: 0.5166 - val_loss: 0.6920 - val_acc: 0.5239\n",
      "Epoch 47/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6940 - acc: 0.5168 - val_loss: 0.6920 - val_acc: 0.5239\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6940 - acc: 0.5142 - val_loss: 0.6920 - val_acc: 0.5239\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.6941 - acc: 0.5147 - val_loss: 0.6920 - val_acc: 0.5239\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6946 - acc: 0.5037 - val_loss: 0.6921 - val_acc: 0.5239\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6943 - acc: 0.5067 - val_loss: 0.6920 - val_acc: 0.5239\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6939 - acc: 0.5037 - val_loss: 0.6920 - val_acc: 0.5239\n",
      "Epoch 53/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6936 - acc: 0.5056 - val_loss: 0.6920 - val_acc: 0.5239\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6930 - acc: 0.5162 - val_loss: 0.6920 - val_acc: 0.5239\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 0s 153ms/step - loss: 0.6944 - acc: 0.5136 - val_loss: 0.6919 - val_acc: 0.5239\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6931 - acc: 0.5162 - val_loss: 0.6919 - val_acc: 0.5239\n",
      "Epoch 57/70\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.6931 - acc: 0.5162 - val_loss: 0.6920 - val_acc: 0.5239\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6935 - acc: 0.5082 - val_loss: 0.6920 - val_acc: 0.5239\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6924 - acc: 0.5162 - val_loss: 0.6919 - val_acc: 0.5239\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6928 - acc: 0.5155 - val_loss: 0.6920 - val_acc: 0.5239\n",
      "Epoch 61/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6926 - acc: 0.5153 - val_loss: 0.6920 - val_acc: 0.5239\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6936 - acc: 0.5147 - val_loss: 0.6920 - val_acc: 0.5239\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6934 - acc: 0.5078 - val_loss: 0.6920 - val_acc: 0.5239\n",
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6921 - acc: 0.5237 - val_loss: 0.6920 - val_acc: 0.5239\n",
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6916 - acc: 0.5205 - val_loss: 0.6918 - val_acc: 0.5239\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6909 - acc: 0.5252 - val_loss: 0.6918 - val_acc: 0.5239\n",
      "Epoch 67/70\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.6920 - acc: 0.5203 - val_loss: 0.6920 - val_acc: 0.5239\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.6933 - acc: 0.5205 - val_loss: 0.6922 - val_acc: 0.5239\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6918 - acc: 0.5233 - val_loss: 0.6919 - val_acc: 0.5239\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6917 - val_acc: 0.5239\n",
      "Adaboost accuracy : 0.7404530744336569, precision : 0.7747653806047967, recall : 0.8006465517241379, f1 : 0.78749337572867, roc_auc : 0.7252827572234952\n",
      "CPU times: user 1h 9min 11s, sys: 6h 59s, total: 7h 10min 11s\n",
      "Wall time: 36min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "with tf.device('/device:GPU:1'):\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=30, verbose=1, restore_best_weights=True)\n",
    "#     cb_checkpoint = ModelCheckpoint(filepath='./models/adaboost_lstm1.h5', monitor='val_acc',\n",
    "#                                     verbose=1, save_best_only=True)\n",
    "#     base_estimator = MyKerasClassifier(build_fn=get_model, epochs=50, batch_size=128, validation_split=0.25, callbacks=[early_stop])\n",
    "    base_estimator = MyKerasClassifier(build_fn=get_model, epochs=70, batch_size=4096, validation_split=0.25)\n",
    "\n",
    "    boosted_classifier = AdaBoostClassifier(base_estimator=base_estimator, n_estimators=60, random_state=42, learning_rate=1)\n",
    "    \n",
    "    print(\"Adaboost LSTM Start\")\n",
    "    boosted_classifier.fit(X_train, y_train)\n",
    "    preds = boosted_classifier.predict(X_test)\n",
    "\n",
    "    precision = precision_score(y_test, preds)\n",
    "    recall = recall_score(y_test, preds)\n",
    "    f1 = f1_score(y_test, preds)\n",
    "    roc_auc = roc_auc_score(y_test, preds)\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "\n",
    "    print(f'Adaboost accuracy : {acc}, precision : {precision}, recall : {recall}, f1 : {f1}, roc_auc : {roc_auc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Adaboost accuracy : 0.7320388349514563, precision : 0.7601214574898786, recall : 0.8092672413793104, f1 : 0.7839248434237996, roc_auc : 0.7125752738501091"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reload_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-35da49911c3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreload_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my_pred_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_pred_test\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_pred_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_pred_test\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'reload_model' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred_test = reload_model.predict(X_test)\n",
    "\n",
    "y_pred_test[y_pred_test>0.5]=1\n",
    "y_pred_test[y_pred_test<=0.5]=0\n",
    "precision = precision_score(y_test, y_pred_test)\n",
    "recall = recall_score(y_test, y_pred_test)\n",
    "f1 = f1_score(y_test, y_pred_test)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_test)\n",
    "acc = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(f'accuracy : {acc}, precision : {precision}, recall : {recall}, f1 : {f1}, roc_auc : {roc_auc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://4fc3a096-2c5e-42aa-8ed7-0d35dd718c6c/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1f891f3f10> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1f881728e0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1f803f70a0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1f803f0760> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://3f4d161d-ae84-45e1-b949-b9e25640463a/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://3f4d161d-ae84-45e1-b949-b9e25640463a/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1df849c0a0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1df83455e0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1dd4262df0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1dd4264610> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://aea38500-abca-43c1-9f03-3d652beb35d0/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://aea38500-abca-43c1-9f03-3d652beb35d0/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1d987c3f10> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1d7809a6a0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1dd425cf10> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1d9871ba60> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://d8af8a47-726a-41d3-be0d-0593228ced6d/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://d8af8a47-726a-41d3-be0d-0593228ced6d/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1d147e3ca0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1d146eb7c0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1d14692280> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1d1469e8e0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://f103218e-4d08-43a8-b7ad-52bd21b3ac7f/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://f103218e-4d08-43a8-b7ad-52bd21b3ac7f/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1ccc6d7070> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1ccc6d7820> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1ccc69d2e0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1ccc662af0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://d990fedd-2b25-49b4-969d-be78df877613/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://d990fedd-2b25-49b4-969d-be78df877613/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1c9878d880> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1c98698340> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1c98665220> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1c98629310> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://e6789a93-f575-469c-93b0-687ba122780c/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://e6789a93-f575-469c-93b0-687ba122780c/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1c6867d940> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1c685861c0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1c6855b520> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1c68518370> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://a414dbcf-accd-4e11-a6f7-61ffa027a2fe/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://a414dbcf-accd-4e11-a6f7-61ffa027a2fe/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1c1071c9a0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1c10624520> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1c105ca190> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1c105cffa0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://878dedc5-d1ab-447a-9325-76d09d8e4c61/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://878dedc5-d1ab-447a-9325-76d09d8e4c61/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1bd060d940> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1bd05191c0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1bd04bab20> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1bd05195e0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://9d9a03cd-e0f8-48d3-83b3-5dc29a0c49b3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://9d9a03cd-e0f8-48d3-83b3-5dc29a0c49b3/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1bd0556b80> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1b76216700> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1b7617c850> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1b7612b430> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://7f4b6df1-1710-4b94-b145-4df49eb2c1df/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://7f4b6df1-1710-4b94-b145-4df49eb2c1df/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1b4421a9d0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1b44123370> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1b4412ae50> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1b440b53d0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://6882bb5a-c30f-45d5-a61f-6f5376d4b070/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://6882bb5a-c30f-45d5-a61f-6f5376d4b070/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1b0410a220> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1b0410ab80> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1ae47dd460> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1b0415ec10> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://928e318e-066e-448b-b9e4-fa49130710d1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://928e318e-066e-448b-b9e4-fa49130710d1/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1adc21b340> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1adc21b160> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1adc12ea00> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1adc0b82e0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://2dec7e68-30b3-412b-8dc2-4baace97d62b/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://2dec7e68-30b3-412b-8dc2-4baace97d62b/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1a9c10daf0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1a6c7d82e0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1a6c7d8b20> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1a6c741ca0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://d6187611-ec92-4789-a080-27933b012dcc/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://d6187611-ec92-4789-a080-27933b012dcc/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1a4403ef40> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1a9c0a1820> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1a9c081940> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1a24730ee0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://499bb4db-f428-4561-bc88-a98f49c11e17/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://499bb4db-f428-4561-bc88-a98f49c11e17/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1a08115400> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1a08115190> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f19e47e5e50> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f19e476b490> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://12e1fa1c-0401-497a-90da-48a9b4f0b715/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://12e1fa1c-0401-497a-90da-48a9b4f0b715/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f19ac7c1bb0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f19ac6c9550> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f19ac6f2cd0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f19ac694130> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://94288a7f-24f6-4f72-9a74-76d071b01a68/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://94288a7f-24f6-4f72-9a74-76d071b01a68/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f19ac70b370> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f19ac70b100> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f19ac7783a0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f197c53eeb0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://53037386-12b4-4385-b5cb-98f69aed3181/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://53037386-12b4-4385-b5cb-98f69aed3181/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f19ac724fa0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1924f33ca0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f197c68e9a0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1924eb8a30> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://56c8a39b-d555-4124-a103-af0a24f06b89/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://56c8a39b-d555-4124-a103-af0a24f06b89/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1924f74430> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1924f741c0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1915f02eb0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1915e80040> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_15 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_15 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_16 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_16 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_17 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_17 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_18 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_18 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_19 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_19 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_20 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_20 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_22 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_22 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_23 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_23 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_24 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_24 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_25 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_25 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_26 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_26 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_27 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_27 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_28 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_28 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_29 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_29 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_30 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_30 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_31 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_31 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_32 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_32 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_33 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_33 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_34 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_34 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_35 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_35 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_36 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_36 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_37 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_37 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_38 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_38 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_39 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_39 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_40 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_40 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_41 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_41 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_42 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_42 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_43 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_43 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_44 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_44 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_45 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_45 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_46 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_46 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_47 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_47 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_48 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_48 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_49 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_49 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_50 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_50 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_51 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_51 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_52 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_52 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_53 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_53 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_54 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_54 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_55 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_55 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_56 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_56 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_57 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_57 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_58 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_58 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_59 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_59 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_60 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_60 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_61 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_61 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_62 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_62 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_63 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_63 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_64 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_64 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_65 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_65 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_66 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_66 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_67 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_67 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_68 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_68 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_69 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_69 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_70 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_70 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_71 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_71 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_72 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_72 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_73 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_73 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_74 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_74 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_75 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_75 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_76 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_76 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_77 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_77 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_78 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_78 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_79 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_79 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import joblib\n",
    "\n",
    "joblib.dump(boosted_classifier, './models/adaboost_lstm.pkl') \n",
    "reload_model = joblib.load('./models/adaboost_lstm.pkl') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.21709043, 0.7829096 ],\n",
       "        [0.1775779 , 0.8224221 ],\n",
       "        [0.12006653, 0.8799335 ],\n",
       "        ...,\n",
       "        [0.18669136, 0.81330866],\n",
       "        [0.10798669, 0.8920133 ],\n",
       "        [0.28414887, 0.7158511 ]], dtype=float32),\n",
       " array([[0.3096022 , 0.6903978 ],\n",
       "        [0.24463809, 0.7553619 ],\n",
       "        [0.12932858, 0.8706714 ],\n",
       "        ...,\n",
       "        [0.41682047, 0.58317953],\n",
       "        [0.08030998, 0.9196901 ],\n",
       "        [0.14576423, 0.85423577]], dtype=float32),\n",
       " array([[0.49377504, 0.50622493],\n",
       "        [0.58474076, 0.41525927],\n",
       "        [0.33875763, 0.66124237],\n",
       "        ...,\n",
       "        [0.45829618, 0.54170376],\n",
       "        [0.06277613, 0.93722385],\n",
       "        [0.28231055, 0.7176894 ]], dtype=float32),\n",
       " array([[0.6876445 , 0.31235552],\n",
       "        [0.3763796 , 0.6236204 ],\n",
       "        [0.208996  , 0.791004  ],\n",
       "        ...,\n",
       "        [0.28230792, 0.71769214],\n",
       "        [0.23227866, 0.76772135],\n",
       "        [0.30269474, 0.69730526]], dtype=float32),\n",
       " array([[0.49829653, 0.50170344],\n",
       "        [0.26000848, 0.73999155],\n",
       "        [0.3973388 , 0.60266113],\n",
       "        ...,\n",
       "        [0.34852   , 0.65147996],\n",
       "        [0.3023709 , 0.6976291 ],\n",
       "        [0.31941816, 0.6805818 ]], dtype=float32),\n",
       " array([[0.5392873 , 0.46071267],\n",
       "        [0.3972464 , 0.6027536 ],\n",
       "        [0.2862208 , 0.7137792 ],\n",
       "        ...,\n",
       "        [0.37388724, 0.6261127 ],\n",
       "        [0.43855244, 0.56144756],\n",
       "        [0.34109214, 0.65890783]], dtype=float32),\n",
       " array([[0.58762056, 0.41237947],\n",
       "        [0.32428414, 0.67571586],\n",
       "        [0.29800615, 0.7019939 ],\n",
       "        ...,\n",
       "        [0.40338236, 0.59661764],\n",
       "        [0.3535327 , 0.64646727],\n",
       "        [0.36878416, 0.6312158 ]], dtype=float32),\n",
       " array([[0.67959875, 0.32040125],\n",
       "        [0.44220376, 0.5577963 ],\n",
       "        [0.23554575, 0.76445425],\n",
       "        ...,\n",
       "        [0.50974274, 0.4902573 ],\n",
       "        [0.47609597, 0.523904  ],\n",
       "        [0.28592792, 0.71407205]], dtype=float32),\n",
       " array([[0.59042466, 0.40957537],\n",
       "        [0.3481807 , 0.6518193 ],\n",
       "        [0.21365432, 0.78634566],\n",
       "        ...,\n",
       "        [0.43507037, 0.56492966],\n",
       "        [0.5528668 , 0.44713324],\n",
       "        [0.2876912 , 0.71230876]], dtype=float32),\n",
       " array([[0.52117157, 0.47882837],\n",
       "        [0.42324227, 0.5767577 ],\n",
       "        [0.28833416, 0.71166587],\n",
       "        ...,\n",
       "        [0.45563588, 0.54436415],\n",
       "        [0.46786553, 0.5321345 ],\n",
       "        [0.29077026, 0.70922977]], dtype=float32),\n",
       " array([[0.5551268 , 0.4448732 ],\n",
       "        [0.5021234 , 0.49787664],\n",
       "        [0.24324764, 0.7567523 ],\n",
       "        ...,\n",
       "        [0.43748724, 0.5625127 ],\n",
       "        [0.394043  , 0.605957  ],\n",
       "        [0.31750095, 0.68249905]], dtype=float32),\n",
       " array([[0.62644285, 0.37355715],\n",
       "        [0.5883761 , 0.4116239 ],\n",
       "        [0.30320227, 0.69679767],\n",
       "        ...,\n",
       "        [0.4075869 , 0.59241307],\n",
       "        [0.33938158, 0.6606184 ],\n",
       "        [0.3015318 , 0.6984682 ]], dtype=float32),\n",
       " array([[0.5925477 , 0.40745226],\n",
       "        [0.5072282 , 0.4927718 ],\n",
       "        [0.25559938, 0.7444006 ],\n",
       "        ...,\n",
       "        [0.45385987, 0.54614013],\n",
       "        [0.28462568, 0.7153743 ],\n",
       "        [0.3185719 , 0.6814281 ]], dtype=float32),\n",
       " array([[0.5725753 , 0.42742476],\n",
       "        [0.4605735 , 0.5394265 ],\n",
       "        [0.25891817, 0.7410818 ],\n",
       "        ...,\n",
       "        [0.4372813 , 0.5627187 ],\n",
       "        [0.31055412, 0.68944585],\n",
       "        [0.33515763, 0.6648423 ]], dtype=float32),\n",
       " array([[0.63662356, 0.36337638],\n",
       "        [0.37664616, 0.6233538 ],\n",
       "        [0.20789944, 0.79210055],\n",
       "        ...,\n",
       "        [0.411564  , 0.588436  ],\n",
       "        [0.37809402, 0.6219059 ],\n",
       "        [0.27332398, 0.726676  ]], dtype=float32),\n",
       " array([[0.5649012 , 0.43509886],\n",
       "        [0.4504875 , 0.5495125 ],\n",
       "        [0.19560194, 0.80439806],\n",
       "        ...,\n",
       "        [0.4355735 , 0.56442654],\n",
       "        [0.43588763, 0.56411237],\n",
       "        [0.26307192, 0.73692805]], dtype=float32),\n",
       " array([[0.60042006, 0.39957994],\n",
       "        [0.4113155 , 0.5886845 ],\n",
       "        [0.18336809, 0.8166319 ],\n",
       "        ...,\n",
       "        [0.40390274, 0.59609723],\n",
       "        [0.3974576 , 0.60254234],\n",
       "        [0.27320385, 0.7267961 ]], dtype=float32),\n",
       " array([[0.59253335, 0.40746665],\n",
       "        [0.46721357, 0.5327864 ],\n",
       "        [0.22761635, 0.7723836 ],\n",
       "        ...,\n",
       "        [0.38576153, 0.61423844],\n",
       "        [0.38322508, 0.6167749 ],\n",
       "        [0.28235543, 0.7176445 ]], dtype=float32),\n",
       " array([[0.64883316, 0.35116684],\n",
       "        [0.5363406 , 0.46365944],\n",
       "        [0.207512  , 0.792488  ],\n",
       "        ...,\n",
       "        [0.3511394 , 0.64886063],\n",
       "        [0.34666574, 0.65333426],\n",
       "        [0.26552576, 0.73447424]], dtype=float32),\n",
       " array([[0.60311335, 0.39688665],\n",
       "        [0.49196878, 0.50803125],\n",
       "        [0.19115953, 0.8088405 ],\n",
       "        ...,\n",
       "        [0.3768267 , 0.6231733 ],\n",
       "        [0.3631048 , 0.63689524],\n",
       "        [0.28346473, 0.71653533]], dtype=float32)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [i for i in boosted_classifier.staged_predict_proba(X_test)]\n",
    "len(a[0])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boosted_classifier.estimator_weights_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1821417 , 0.17475991, 0.14589192, 0.15551748, 0.13755002,\n",
       "       0.17682738, 0.22586276, 0.1431941 , 0.12552032, 0.17962682,\n",
       "       0.18907787, 0.15463544, 0.1608663 , 0.24392606, 0.10908219,\n",
       "       0.06825134, 0.13479391, 0.15155727, 0.12661465, 0.12899345])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boosted_classifier.estimator_errors_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.MyKerasClassifier at 0x7f46dd507910>,\n",
       " <__main__.MyKerasClassifier at 0x7f4773ba2760>,\n",
       " <__main__.MyKerasClassifier at 0x7f4758831640>,\n",
       " <__main__.MyKerasClassifier at 0x7f475b6ee2b0>,\n",
       " <__main__.MyKerasClassifier at 0x7f473e39f100>]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boosted_classifier.estimators_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boosted_classifier.estimator_weights_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.19136202, 0.19596451, 0.16152457, 0.14712343, 0.16450197])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boosted_classifier.estimator_errors_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
